[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Este sitio presenta una breve introducción a técnicas para la organización de datos, generación de gráficos y análisis de datos, que incluye:\nEste sitio integra diferentes técnicas utilizadas por investigadores y posgraduados de la Universidad de Nueva Gales del Sur, Australia, quienes han proporcionado sus conocimientos y habilidades para ayudar a otros a desarrollar habilidades en el procesamiento y análisis de datos.\nLos recursos fueron desarrollados inicialmente para ayudar en la enseñanza de habilidades cuantitativas a estudiantes de pregrado y posgrado en la Escuela de Ciencias Biológicas, Terrestres y Ambientales en la Universidad de Nueva Gales del Sur, con el apoyo de una subvención de Innovación en Enseñanza y Aprendizaje otorgada a Alistair Poore, Will Cornwell, Iain Suthers y Richard Kingsford. El desarrollo de la segunda generación de este sitio fue posible gracias al apoyo del Centro de Investigación en Evolución y Ecología.\nEditores del sitio:\nProfesor Alistair Poore\nProfesor Asociado Will Cornwell\nProfesor Asociado Danial Falster\nDr. Fonti Kar\nAutores de la página: Keryn Bain, Rachel Blakey, Stephanie Brodie, Corey Callaghan, Will Cornwell, Kingsley Griffin, Matt Holland, James Lavender, Andrew Letten, Shinichi Nakagawa, Shaun Nielsen, Alistair Poore, Gordana Popovic, Fiona Robinson y Jakub Stoklosa.\nAño: 2016\nÚltima actualización: r format(Sys.time(), \"%b %Y\")"
  },
  {
    "objectID": "about.html#colaboradores",
    "href": "about.html#colaboradores",
    "title": "About",
    "section": "Colaboradores",
    "text": "Colaboradores\n¡Gracias a ellos  por hacer de los programas de código abierto un lugar mejor!"
  },
  {
    "objectID": "coding-skills/asking-code-questions/index.html",
    "href": "coding-skills/asking-code-questions/index.html",
    "title": "Preguntas sobre código",
    "section": "",
    "text": "Cuando comenzamos a interactuar con R, una de las primeras cosas que probablemente descubriste fue cómo importar tus datos a R. Tuviste que utilizar algún método para leer tus adorables conjuntos de datos de Excel en R para poder lograr lo que querías. Tal vez utilizaste read.csv o read.table, o tal vez utilizaste el paquete reciente de Hadley Wickham, readr.\nEste tutorial tiene la intención de “volver a lo básico” un poco y aprender a “crear” nuestros propios datos."
  },
  {
    "objectID": "coding-skills/asking-code-questions/index.html#dónde-puedes-obtener-más-ayuda",
    "href": "coding-skills/asking-code-questions/index.html#dónde-puedes-obtener-más-ayuda",
    "title": "Preguntas sobre código",
    "section": "¿Dónde puedes obtener más ayuda?",
    "text": "¿Dónde puedes obtener más ayuda?\nToda esta información no es realmente útil a menos que tengas a alguien que responda tu pregunta después de haber creado tu buen ejemplo reproducible. Una alternativa para solicitar ayuda a colegas y amigos es utilizar sitios web en línea, como Stack Overflow. Los consejos de este tutorial te ayudarán a formular una pregunta que no sea eliminada o prohibida. Además, cuando hagas una pregunta, asegúrate de mostrar que has realizado investigaciones previas.\n\n\n\nHay más ayuda en la web para crear ejemplos reproducibles. Por ejemplo, consulta aquí, aquí, aquí o aquí.\nAutor: Corey T. Callaghan\nAño: 2017\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "coding-skills/good-practice/index.html",
    "href": "coding-skills/good-practice/index.html",
    "title": "Buenas prácticas de codificación",
    "section": "",
    "text": "library(formatR)\n\nCuando se emprende cualquier proyecto que involucre análisis de datos en R, es una muy buena idea guardar todo el código necesario para ejecutar cualquier análisis o crear cualquier gráfico en un script de R.\nLos script de R son muy útiles cuando se colabora con otros, ya que puedes compartir tus métodos. También tendemos a reutilizar y adaptar script para proyectos futuros, por lo que es posible que necesites leer un script que escribiste hace meses o incluso años. Es importante formatear tu script para facilitar su transferencia entre computadoras y para una fácil interpretación por parte de otros (y por ti mismo).\nSi nunca has creado un script en R, primero lee Introducción a R.\n\nCrear un directorio de proyecto\nLas buenas prácticas de gestión de código y datos a menudo comienzan antes de abrir RStudio. Para cada proyecto, debes comenzar creando una carpeta especifica para cada uno de los proyectos en tu computadora. En esta carpeta guardarás todos tus datos, script - codigos y resultados (gráficos y tablas) y se referirá como tu directorio de proyecto. La forma en que gestiones tu directorio de proyecto es muy importante para mantener la integridad de tus datos y para facilitar la transferencia de tu trabajo entre computadoras. La siguinte muestra una estructura basica para un directorio de proyecto.\n\nIntegridad de los datos. Una vez que se haya completado la entrada de datos y se hayan guardado en tu directorio de proyecto (consulta Entrada de datos), esta debería ser la última vez que veas los datos sin procesar. Cualquier manipulación, eliminación de valores atípicos, cambio de nombres de variables, etc., se debe realizar con el uso de código de R. Esto mantiene la integridad de tus datos originales y proporciona un registro en tu script de R de los cambios realizados en tu conjunto de datos, permitiendo garantizar la reproducibilidad.\n\n\nFormato general del script\nLas siguientes notas describen el diseño general y el orden que debes seguir al escribir tus script. Si todos utilizan el mismo formato general, será mucho más fácil leer y entender los script propios y de los demás.\nEn primer lugar, todos los script deben comenzar con un título, detalles del autor, una breve descripción del propósito del script y los datos que se están utilizando.\nOtros puntos posibles de considerar a futuro son derechos de autor y cuestiones legales. Por ejemplo:\n\n# título: Análisis de series temporales\n# Detalles del autor: Autor: John Smith,\n# Detalles de contacto: John.Smith@unsw.edu.au\n# Información del script y los datos: Este script contiene código para desarrollar # un análisis de series temporales en datos de conteo.\n# Los datos consisten en recuentos de especies de aves.\n# Los datos fueron recolectados en la región de Hunter Valley entre 1990 y 1991.\n# Declaración de derechos de autor: Este script es producto de UNSW, etc.\n\nTodos los comentarios deben comenzar con # para distinguirlos del código ejecutable, para generar una linea de comentario; si deseas generar un bloque de comentarios, debes utilizar #'.\nLuego, debes incluir algún código que importe tus datos. Idealmente, deberás trabajar desde una carpeta en tu computador el cual sera tu directorio de proyecto.\n\nmy.data &lt;- read.csv(\"my_data.csv\", sep = \",\", header = T, check.names = FALSE)\n\nmy_data &lt;- readr::read_csv(\"my_data.csv\")\n\nPara ahorrar tiempo más adelante y evitar mensajes de error molestos, asegúrate de cargar en R todos los paquetes y funciones necesarios para tus análisis. Esto incluye bibliotecas o cualquier script de funciones que hayas escrito tú mismo. Cada paquete se carga con la ayuda del comando library().\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nsource(\"R_scripts/myfunctions.R\")\n\nFinalmente, antes de comenzar a ejecutar cualquier análisis de datos, es posible que debas realizar algunas tareas de limpieza en tus datos (verificar la estructura del conjunto de datos, buscar valores faltantes, cambiar tipos de variables, etc.). Consulta Estructura de datos y Importación de datos para obtener ayuda con estos problemas.\nAl juntar todo esto, el comienzo de un script se verá algo como lo siguiente.\n\n# título: Análisis de series temporales\n# Detalles del autor: Autor: John Smith,\n# Detalles de contacto: John.Smith@unsw.edu.au\n# Información del script y los datos: Este script contiene código para desarrollar # un análisis de series temporales en datos de conteo.\n# Los datos consisten en recuentos de especies de aves.\n# Los datos fueron recolectados en la región de Hunter Valley entre 1990 y 1991.\n# Declaración de derechos de autor: Este script es producto de UNSW, etc.\n\nmy.data &lt;- readr::read_csv(\"mydata.csv\")\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nsource(\"R_scripts/myfunctions.R\")\n\n# Checking data structure\n\nsummary(my.data)\nstr(my.data)\nmy.data[which(is.na(my.data)), ]\nlevels(my.data$variable1)\n\n# Data cleaning\nmy.data &lt;- my.data |&gt; \n  dplyr::mutate(variable1 = as.numeric(variable1)) |&gt; # Coercion de variable1 a numérico.\n  dplyr::mutate_all(~tidyr::replace_na(., 0)) # reemplazar NA con ceros.\n\n\n\nGuía de estilo\n“Escribir buen código es como usar una puntuación correcta. Puedes arreglártelas sin ella, pero ciertamente facilita la lectura.” - Hadley Wickham.\nExisten muchos estilos diferentes de codificación (ninguno de los cuales es mejor o peor). El objetivo de las guías de estilo es tener un vocabulario común. Cuando se trabaja con otras personas, es una buena idea acordar un estilo común antes de que el proyecto avance demasiado.\nLa siguiente guía se basa en la guía de estilo de R de Google y en la Guía de estilo de Hadley Wickham. Si aún no has adoptado un estilo de codificación consistente, estos son buenos lugares para comenzar.\n\n\nNotación y nombres\nNombres de archivos y carpetas\nCuando nombres tus carpetas de proyectos, archivos de datos, archivos de script o cualquier otro archivo, hay varias cosas que deben tenerse en cuenta. Los archivos pueden copiarse o transferirse entre diferentes sistemas operativos (por ejemplo, Windows, Mac o UNIX) y necesitamos nombrar nuestros archivos para que sean transferibles. Además, los nombres de archivo deben ser únicos e indicativos de lo que contiene el archivo. Considera las siguientes reglas al nombrar tus propias carpetas y archivos.\nEn primer lugar, evita los caracteres “especiales”. Los caracteres especiales incluyen cosas como separadores de archivos (por ejemplo, dos puntos, barra diagonal y barra invertida), símbolos no alfabéticos y no numéricos (por ejemplo, ?, $), signos de puntuación (por ejemplo, puntos, comas, paréntesis, comillas y operadores) y, el error más común, evita los caracteres de espacio en blanco (espacios, tabulaciones, saltos de línea y retornos incrustados).\nDale a tus archivos nombres significativos; evita nombres de archivos como “proyecto1” y “proyecto2” o “datos1.csv” y “datos2.csv”, en su lugar usa cosas como “movimiento_aves.csv”, “alimentación_caracol.csv” o “datos_movimiento_diurno.csv” y “datos_movimiento_anual.csv”. Para scripts de R, los nombres de archivo deben terminar en .R (es decir, “predecir_movimientos_diurnos.R”).\nAunque los sistemas de archivos actuales permiten límites de 255 caracteres, es una buena práctica acortar los nombres de los archivos. Intenta mantener los nombres de archivo entre 1 y 3 palabras de longitud. Si agregas fechas a un nombre de archivo, recuerda evitar el uso de caracteres especiales, considera usar guiones bajos o guiones para separar días-meses-años.\nEs absolutamente crucial que los nombres de archivo sean únicos, especialmente si trabajas en un entorno colaborativo y, especialmente, si copias archivos con frecuencia a un servidor. Si no tienes un sistema para mantener los nombres de archivo únicos, corres el riesgo de sobrescribirlos y perder todos tus datos.\nNombres de objetos en R\nLos nombres de las variables deben estar en minúsculas y separar las palabras con puntos, mientras que los nombres de las funciones deben tener letras iniciales en mayúsculas y no usar puntos. En general, los nombres de las variables deben ser sustantivos y los nombres de las funciones deben ser verbos. Para facilitar la escritura, está bien acortar palabras y usar abreviaturas siempre y cuando sigan identificando y describiendo el objeto al que hacen referencia. Procura que los nombres sean concisos y significativos, evita utilizar nombres de variables de una sola letra y, siempre que sea posible, evita usar nombres de funciones y variables existentes.\n Good examples\n\n# variables \nbird.mvment &lt;- readr::read_csv(\"bird_movement.csv\")\nbird.mvment.mdl &lt;- lm(counts ~ location, data = bird.mvmnt)\nbird.mvment &lt;- bird.mvment |&gt; \n  dplyr::mutate(log.counts = log(bird.mvmnt$counts))\n\n# functions \ncalc_standard_error &lt;- function(x){\n  sd(x)/sqrt(length(x))\n}\n\n Bad examples\n\n# variables\ndata &lt;- readr::read_csv(\"bird_movement.csv\") # Crea un objeto cuyo nombre mantenga relación con los datos que contiene.\n\nBird.Mvment_Mdl &lt;- lm(counts ~ location, data = bird.mvmnt) # Evita usar nombres inconsistentes.\nbird.mvment$log &lt;- log(bird.mvmnt$counts) # los nombres de las variables deben mantener relación los datos que estos almacenan.\n\n\n# Funciones\nS &lt;- function (x){ # S es un nombre poco informativo. \n  sd(x)/sqrt(length(x))\n}\n\n\n\nDirectrices para agregar comentarios\nAgregar comentarios a tu script\nCuando vuelvas y edites o trabajes en proyectos en el futuro, es sorprendente cuánto olvidarás. Por lo tanto, es esencial comentar de manera precisa tu código tanto para proyectos en solitario como en equipo. Sin embargo, se pueden tener demasiados comentarios. Nombres descriptivos e informativos y un código expresivo pueden eliminar la necesidad de muchos comentarios, y sobrecomentar puede hacer que los scripts sean desordenados y difíciles de leer. Esta es una habilidad que se desarrolla con el tiempo y la práctica. A medida que mejore tu habilidad para programar, te encontrarás comentando cada vez menos: “El código no miente, pero los comentarios sí”.\nEn general, los comentarios NO deben indicar lo obvio, deben ser coherentes con lo que describen, debe quedar claro a qué línea o bloque de código se refieren y deben ser legibles para cualquier persona que los maneje en el futuro.\nLas líneas comentadas completas deben comenzar con # y un espacio; los comentarios cortos se pueden colocar después del código precedido por dos espacios, # y luego un espacio.\nConsejo: Usa líneas comentadas de # —— para dividir tu script en secciones legibles.\n Good examples\n\nbird.count &lt;- 10\n\n# Histograma.\nhist(bird_movement$counts,\n     breaks = \"scott\",  # método para elegir el número de contenedores.\n     main   = \"Histogram: bird counts\")\n\n Bad examples\n\nx &lt;- 10  # Bird counts - unneccesary, simply name the variable 'bird.count'\n\nhist(bird_movement$counts,\n     breaks = \"scott\",### method for choosing number of buckets - looks messy. \n     main   = \"Histogram: bird counts\")\n# Creates histogram of frequency of bird counts. - place comment before code. \n\nAgregando comentarios a las funciones\nLos comentarios de las funciones deben contener una breve descripción de la función (una oración), una lista de argumentos de la función con una descripción de cada uno (incluyendo el tipo de datos) y una descripción del valor de retorno. Los comentarios de las funciones deben escribirse inmediatamente debajo de la línea de definición de la función.\nConsulta Escritura de funciones para obtener ayuda sobre cómo crear funciones en R.\n Good example\n\nCalculateStandardError &lt;- function (x){\n # Calcula el error estándar de la muestra\n  #\n  # Argumentos:\n  #  x: Vector del cual se calculará el error estándar. x debe tener una longitud mayor que uno,\n  #     y no debe contener valores faltantes.\n  #\n  # Retorno:\n  #  El error estándar de x\n  se&lt;-sd(x)/sqrt(length(x))\n  return(se)\n}\n\n\n\nSintaxis\nAsignación\nSiempre utiliza &lt;- al asignar nombres a objetos y evita usar = para la asignación. Aunque esta distinción no importa la mayoría de las veces, es una buena práctica utilizar &lt;-, ya que se puede usar en cualquier lugar, mientras que el operador = solo está permitido en el nivel superior. Además, = se asemeja mucho a ==, que es el operador lógico para igual a.\n Good example\n\nbird.count &lt;- bird.mvments$counts\n\n Bad example\n\nbird.count = bird.mvments$counts\n\nLongitud de línea\nLa longitud máxima de una línea debe ser de 80 caracteres. Esto se ajusta cómodamente en una página impresa con una fuente de tamaño razonable. Si te encuentras quedándote sin espacio, es posible que necesites condensar parte del trabajo en una función separada.\n\nThis is how long 80 characters is. Try not to type more than 80 on a single line.\n\nEspaciado\nDebes colocar espacios alrededor de todos los operadores binarios (=, +, -, &lt;-, ==, !=), con la excepción de los dos puntos (:) y las comas (,). Al igual que en inglés, siempre debes poner un espacio después de una coma y nunca antes.\n Good examples\n\nbird.mvments[which(bird.mvments == max(bird.mvments)), ]\n\nbird.var &lt;- bird.mvments[, 4:10]\n\n Bad examples\n\nbird.mvments[which(bird.mvments == max(bird.mvments)), ]  # Espacios necesarios entre operadores y después de la coma.\n\nbird.var &lt;- bird.mvments[, 4:10]  # El espacio va después de la coma, no antes. Eliminar el espacio alrededor de :.\n\nbird.var &lt;- bird.mvments[, 4:10]  # Espacio necesario alrededor de &lt;-.\n\nColoca un espacio antes de los paréntesis, excepto en una llamada de función. No coloques espacios alrededor del código dentro de paréntesis o corchetes, excepto después de una coma.\n Good examples\n\nfor (i in 1:20) {\n   bird.means[[i]] &lt;- mean(bird.mvments$bird.count[[i]])\n}\n\nmean(bird.mvments$bird.count)\n\nbird.mvments[2, ]\n\n Bad examples\n\nfor(i in 1:20) {  # space needed betwen for and (i in 1:20).\n   bird.means[[i]] &lt;- mean (bird.mvments$bird.count[[i]])  # remove space after mean. \n}\n\nmean( bird.mvments$bird.count )  # remove space around code.\n\nbird.mvments[2, ]  # needs a space after comma. \n\nLlaves de llave {}\nLas llaves de llave se utilizan en bucles y para establecer condiciones lógicas. Una llave de apertura nunca debe ir en su propia línea y siempre debe ir seguida de una nueva línea. Una llave de cierre siempre debe ir en su propia línea, a menos que sea seguida por else, que debe estar contenido dentro de llaves de llave orientadas hacia afuera &gt;}else{. Siempre indenta el código dentro de las llaves de llave.\n Good examples\n\nfor (i in 1:20) {\n  bird.means[[i]] &lt;- mean(bird.mvments$bird.count[[i]])\n}\n\nif (y == 0) {\n    log(x)\n  } else {\n    y ^ x\n}\n\n Bad examples\n\nfor (i in 1:20) { bird.means[[i]] &lt;- mean(bird.mvments$bird.count[[i]])  # opening curly followed by new line\n} \n\nfor (i in 1:20) { \n  bird.means[[i]] &lt;- mean(bird.mvments$bird.count[[i]])}  # closing curly needs new line. \n\nif (y == 0) {\n    log(x)\n  } \n  else {  # inclose else within }{. \n    y ^ x\n}\n\nIndentación\nNunca uses tabulaciones ni mezcles tabulaciones y espacios al indentar tu código. Al indentar, utiliza dos espacios, excepto cuando uses paréntesis, donde alineas una nueva línea con el primer carácter dentro del paréntesis o corchetes cuadrados.\n Good examples\n\nCalcStandardError &lt;- function (x){\n  se&lt;-sd(x)/sqrt(length(x))\n  return(se)\n}\n\nbird.mvments[which(bird.mvments$counts == max(bird.mvments$counts)), \n             10:ncols(bird.mvments)]\n\n Bad examples\n\nCalcStandardError &lt;- function (x){\nse&lt;-sd(x)/sqrt(length(x))  # indent two spaces.\nreturn(se)\n}\n\nbird.mvments[which(bird.mvments$counts == max(bird.mvments$counts)), \n  10:ncols(bird.mvments)]  # align with the square brackets. \n\n\n\nMás ayuda\nPara obtener más información, consulta Hadley Wickham’s style guide, que se basa en la Google style guide.\nAhora es posible que estés pensando en todos los scripts que has creado y que necesitan ser reformateados. Como es común en R, alguien ha creado un paquete para ayudar con esto. El paquete formatR de Yihui Xie tiene una pequeña y útil función llamada tidy_source(). Esto no soluciona todos los problemas, pero puede hacer que los scripts horribles sean legibles. Consulta Una introducción a formatR o escribe ?tidy_source() para obtener detalles sobre cómo utilizar este paquete.\nAutor: Keryn F Bain\nAño: 2017\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "coding-skills/index.html",
    "href": "coding-skills/index.html",
    "title": "Introducción a la programación",
    "section": "",
    "text": "Podrías pensar que tener habilidades en programación es solo para desarrolladores de sitios web y juegos, pero las habilidades en programación son cada vez más valiosas para los científicos ambientales. Ejecutar tareas de análisis de datos y gráficos usando código significa que los métodos de investigación están explícitamente guardados, son reproducibles y se pueden compartir fácilmente con otros investigadores.\nEn estas páginas, ofrecemos algunos consejos para escribir código efectivo y algunas de las habilidades de programación muy útiles que te ahorrarán mucho tiempo.\n\nBuenas prácticas para escribir scripts\nEscribir funciones simples\nUsar bucles\nControl de versiones\n\n Autor: Alistair Poore y Will Cornwell\nAño: 2016\nÚltima actualización: r format(Sys.time(), \"%b %Y\")"
  },
  {
    "objectID": "coding-skills/loops/index.html",
    "href": "coding-skills/loops/index.html",
    "title": "Usando bucles",
    "section": "",
    "text": "¿Te encuentras cortando y pegando código de R con frecuencia?\nEsto generalmente creará problemas para ti más adelante. Uno de los principios de una buena codificación es tratar de reducir al mínimo la repetición. Hay dos enfoques para organizar tu código y ahorrarte trabajo. El primero es usar funciones y el segundo, cubierto aquí, es usar bucles.\nA menudo, queremos realizar tareas repetitivas en las ciencias ambientales. Por ejemplo, es posible que deseemos recorrer una lista de archivos y hacer lo mismo una y otra vez. Hay muchos paquetes en R con funciones que harán todo el trabajo duro por ti (por ejemplo, echa un vistazo a dplyr, tidyr y reshape2 cubiertos aquí). El enfoque de dplyr funciona bien si tus datos son “ordenados” y están en un marco de datos. Si tus datos están en muchos archivos diferentes, entonces un bucle puede ser una solución más rápida.\n\nSintaxis básica de los bucles\nLa sintaxis de los bucles es relativamente simple: los componentes esenciales son for(){} donde la parte for() indica cuántas veces se realizan las operaciones dentro de {}.\nConsidera el siguiente bucle. La primera vez que recorremos el bucle, el valor de i será igual a 1 y este valor se mostrará con la función print. Luego se repetirá con i = 2, hasta i = 10, realizando la tarea que se encuentra dentro de {} cada vez.\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nPodemos cambiar el rango de números (1:10) a cualquier cosa que deseemos, no tienen que ser una secuencia de enteros o incluso números. También puedes cambiar i a cualquier valor que desees.\n\nnums &lt;- c(3.2, 890, 0.0001, 400)\n\nfor (bat in nums) {\n  print(bat)\n}\n\n[1] 3.2\n[1] 890\n[1] 1e-04\n[1] 400\n\nchars &lt;- c(\"a\", \"o\", \"u\", \"z\")\n\nfor (bat in chars) {\n  print(bat)\n}\n\n[1] \"a\"\n[1] \"o\"\n[1] \"u\"\n[1] \"z\"\n\n\nDe mayor interés para nosotros es cambiar lo que está dentro de {} o la operación que estamos realizando en nuestros datos. Podemos insertar cualquier cosa que deseemos aquí. Aquí hay un bucle que imprimirá el cuadrado y la raíz cuadrada de los números del 1 al 10.\n\nfor (i in 1:10) {\n  print(i^2)\n  print(sqrt(i))\n}\n\nA menudo, querremos mantener los resultados que obtenemos de nuestro bucle. La primera opción es crear un vector o marco de datos vacío y agregar los resultados a él. Esto tarda más en ejecutarse, pero no importa realmente con bucles simples, aunque puede aumentar los tiempos de espera para estructuras de bucle más largas y complicadas.\nAquí hay un código que almacenará el cuadrado de los números del 1 al 10 en un nuevo vector llamado x.\n\nx &lt;- vector() # makes a blank vector\n\nfor (i in 1:10) {\n  y &lt;- i^2 # performs an operation\n  x &lt;- append(x, y) # overwrites 'x' with y appended to it\n}\n\nAquí hay un código que almacenará tanto el cuadrado como la raíz cuadrada de los números del 1 al 10 en dos columnas de un nuevo marco de datos llamado x2.\n\nx2 &lt;- data.frame(col1 = vector(), col2 = vector()) # makes a blank data frame with two columns\n\nfor (i in 1:10) {\n  col1 &lt;- i^2 # performs first operation\n  col2 &lt;- sqrt(i) # performs second operation\n  x2 &lt;- rbind(x2, cbind(col1, col2)) # overwrites 'x2' values including the new row\n}\n\nLa segunda opción es crear un vector o marco de datos vacío de dimensiones conocidas y luego colocar los resultados directamente en él. Por ejemplo, si tuviéramos un bucle con 10 elementos, podríamos almacenar los resultados de cada operación en un vector con una longitud 10.\n\nx &lt;- vector(length = 10) # makes a blank vector with a length of 10\n\nfor (i in 1:10) {\n  y &lt;- i^2\n  x[i] &lt;- y # places the output in position i in the vector x\n}\n\nAlternativamente, guarda los resultados de múltiples operaciones en un nuevo data frame.\n\nx2 &lt;- data.frame(col1 = vector(length = 10), col2 = vector(length = 10)) # makes a blank data frame with two columns and 10 rows\n\nfor (i in 1:10) {\n  col1 &lt;- i^2 # performs first operation\n  col2 &lt;- sqrt(i) # performs second operation\n  x2[i, 1] &lt;- col1 # places the first result into row i, column 1\n  x2[i, 2] &lt;- col2 # places the second result into row i, column 2\n}\n\n\n\nUn ejemplo ecológico\n\nAhora podemos utilizar tus nuevas habilidades de bucle en un contexto ecológico. Al igual que en el tutorial de Subconjuntos de datos, utilizaremos un conjunto de datos donde se muestrearon murciélagos en un bosque en regeneración en el sureste de Australia, que ha sido adelgazado para reducir la densidad de árboles.\n\nBats &lt;- read.csv(file = \"Bats_data.csv\", header = T, stringsAsFactors = F)\nstr(Bats)\n\n'data.frame':   173 obs. of  10 variables:\n $ Site                 : chr  \"CC02A1\" \"CC02A1\" \"CC02A1\" \"CC02A2\" ...\n $ Activity             : int  299 276 530 356 571 631 144 124 220 468 ...\n $ Foraging             : int  0 6 14 5 3 17 3 0 7 8 ...\n $ Date                 : chr  \"9/01/2013\" \"8/01/2013\" \"7/01/2013\" \"8/01/2013\" ...\n $ Treatment.thinned    : chr  \"medium-term\" \"medium-term\" \"medium-term\" \"medium-term\" ...\n $ Area.thinned         : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Time.since.thinned   : int  7 7 7 7 7 7 7 7 7 7 ...\n $ Exclusion.thinned    : num  25.1 25.1 25.1 25.1 25.1 ...\n $ Distance.murray.water: num  190 190 190 216 216 ...\n $ Distance.creek.water : num  444 444 444 885 885 ...\n\n\nAl analizar la estructura de estos datos, tenemos dos variables de respuesta: actividad (número de llamadas de murciélagos registradas en una noche) y forrajeo (número de llamadas de alimentación de murciélagos registradas en una noche). Estos datos se recolectaron durante un total de 173 noches de estudio y en 47 sitios diferentes. Hay ocho posibles variables predictoras en el marco de datos, una de las cuales es un factor (Tratamiento.adelgazado), y siete de las cuales son variables continuas (Área.adelgazada, Tiempo.desde.adelgazamiento, Exclusión.adelgazada, Distancia.agua.murray, Distancia.agua.arroyo, Promedio.T, Promedio.H).\nDigamos que estamos explorando nuestros datos y nos gustaría saber qué tan bien se correlaciona la actividad de los murciélagos con nuestras covariables continuas. Nos gustaría calcular el coeficiente de correlación de Pearson para la actividad y cada una de las covariables por separado. El coeficiente de correlación de Pearson, calculado con la función cor, varía de -1 (correlación negativa perfecta) a 1 (correlación positiva perfecta), siendo 0 ninguna correlación. Almacenaremos todas nuestras correlaciones en un nuevo data frame llamado Correlations.\nPrimero, usa select de dplyr para hacer una subconjunto de los datos con la variable de respuesta (actividad) y las 5 variables predictoras.\n\nlibrary(dplyr)\n\nBats_subset &lt;- select(Bats, Activity, Area.thinned:Distance.creek.water)\n\nA continuación, crea un data frame vacío con dos columnas (el nombre de la variable y la correlación) y el número de filas necesario para almacenar todas las correlaciones.\n\nrows &lt;- ncol(Bats_subset) - 1 # the number of rows needed in our output dataframe\n\nCorrelations &lt;- data.frame(\n  variable = character(length = rows),\n  correlation = numeric(length = rows),\n  stringsAsFactors = F\n)\n\nFinalmente, podemos utilizar un bucle para calcular cada una de las correlaciones y almacenar la salida en nuestro nuevo data frame.\n\nfor (i in 1:rows) {\n  temp1 &lt;- colnames(Bats_subset[i + 1]) # retrieves the name of predictor variable\n  temp2 &lt;- cor(Bats_subset[, 1], Bats_subset[, i + 1], method = \"pearson\")\n  # calculates the correlation between activity and predictor variable\n  Correlations[i, 1] &lt;- temp1 # places the variable name into row i, column 1\n  Correlations[i, 2] &lt;- temp2 # places the correlation into row i, column 2\n}\n\n\n\n               variable correlation\n1          Area.thinned -0.40890389\n2    Time.since.thinned -0.02135752\n3     Exclusion.thinned  0.17562438\n4 Distance.murray.water -0.18071570\n5  Distance.creek.water -0.09130258\n\n\nAhora podemos ver de un vistazo que la actividad está más fuertemente correlacionada (negativamente) con el área adelgazada y que no está correlacionada en absoluto con el tiempo desde el adelgazamiento o la temperatura media. Es posible que luego deseemos investigar más a fondo algunas de estas relaciones con modelos y pruebas estadísticas apropiadas.\n\n\nAyuda adicional\nTutorial de DataCamp sobre bucles\nPuedes encontrar más ejemplos buenos de bucles, listas y declaraciones if/else en el sitio GitHub del grupo de usuarios de R de BEES Bucles y listas por Mitch.\nAutor: Rachel V. Blakey\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "coding-skills/version-control/index.html",
    "href": "coding-skills/version-control/index.html",
    "title": "Control de versiones",
    "section": "",
    "text": "Ventajas, desventajas y soluciones\nFundamental para la organización de proyectos es mantener un conjunto consistente, uniforme y simple de archivos de trabajo que puedan ser comprendidos por tu futuro yo, colaboradores y personas interesadas que deseen reproducir tus análisis o reutilizar tu código o datos. Sin embargo, ¡para proyectos complejos con muchos colaboradores, mantener la limpieza y hacer un seguimiento de los cambios en los archivos puede ser muy desafiante!\nSi bien los servicios de intercambio de archivos como Dropbox son fáciles de usar y pueden mantener un conjunto consistente de archivos en tu(s) computadora(s) y/o entre colaboradores, tienen algunas desventajas graves. Específicamente, no pueden decirte qué ha cambiado dentro de los archivos. Tampoco pueden manejar conflictos entre archivos cuando tú y tus colaboradores están trabajando al mismo tiempo en las mismas cosas. Esto a menudo lleva a acumular archivos que nadie parece saber para qué son o en qué se diferencian. Su capacidad limitada para rastrear archivos eliminados también lleva a una acumulación de archivos “demasiado temerosos para eliminar”. A medida que los proyectos se desarrollan, estos se convierten en documentos y pasan por innumerables rondas de revisiones. Un poco de organización puede evitar que el caos se agrave y mantener la reproducibilidad (tanto para ti como para otros) a través de todos los cambios.\nIdealmente, sería bueno poder ver fácilmente el conjunto actual (y esperemos que el mejor) de archivos, pero también poder ver quién los ha cambiado y qué partes específicas de los archivos se han modificado. Mantener un registro de la evolución de un proyecto en particular también es importante para poder resolver rápidamente los problemas cuando (no si) surjan. Configurar tus proyectos bajo un sistema de control de versiones te permite hacer todo lo anterior y más.\nVentajas. Las principales ventajas de usar control de versiones en lugar de simplemente usar Dropbox incluyen:\n\nHace un seguimiento de todos los archivos relevantes para un proyecto específico y/o que son necesarios para su reproducibilidad al asignarles un identificador único.\nDocumenta la evolución de cada archivo en un proyecto, incluidos datos, manuscritos y código, al permitir al usuario asignar una etiqueta de confirmación que describe cómo se modificó el archivo.\nIndica automáticamente quién, cuándo y dónde se modificó un archivo específico, identificando las ubicaciones dentro del texto que han cambiado.\nMantiene un historial registrado de todos los archivos modificados, lo que te permite retroceder en el tiempo para ver versiones antiguas de archivos o el estado del proyecto en un momento anterior.\nFusiona automáticamente archivos en los que tú y tus colaboradores están trabajando al mismo tiempo e introduce marcadores de conflicto en el texto del archivo si se han editado las mismas líneas. Esto te obliga a lidiar y corregir los conflictos de inmediato.\nFacilita la ejecución de análisis y scripts en servidores.\nPromueve una colaboración más fácil entre los miembros del laboratorio y los coautores al mantener los archivos sincronizados y anotados en diversas plataformas.\nFacilita mucho al final de un proyecto compartir y almacenar rápidamente los datos y el código relevantes para reproducir el proyecto (análisis, artículo, etc.), lo cual es a menudo un requisito previo necesario para su publicación.\nFacilita corregir errores dentro de un proyecto al permitirte localizar al colega relevante (o viceversa) cuando surgen problemas después de actualizaciones o cambios.\nTe obliga a pensar en las partes relevantes del proyecto que son esenciales para su reproducibilidad y te permite personalizar lo que se rastrea.\n\nDesventajas. Aunque lo anterior es definitivamente atractivo, el control de versiones y git tienen algunas desventajas. Primero, en algún momento necesitarás dedicar un poco de tiempo a programar antes y después del trabajo diario. Aunque la pequeña inversión de tiempo para hacer esto vale mucho la pena a largo plazo. Segundo, la programación adicional puede dificultar que los colaboradores se familiaricen y comiencen a utilizar un sistema de control de versiones. No todos estarán tan familiarizados con la terminal o la línea de comandos, lo que puede crear barreras de entrada.\nSoluciones. Afortunadamente, existen soluciones muy fáciles y simples para muchos de los problemas mencionados anteriormente. Por ejemplo, ahora hay interfaces de usuario gráficas (GUI) muy útiles y fáciles de usar, como SourceTree, que eliminan la necesidad de utilizar la línea de comandos por completo, facilitan y simplifican la configuración de repositorios y son multiplataforma (es decir, Windows, Mac, etc.).\n\n\nCómo funciona\nDado que no todos querrán usar la línea de comandos, mostraremos algunos conceptos básicos utilizando tanto la línea de comandos como una GUI (SourceTree). Haremos esto porque creemos que los beneficios de utilizar el control de versiones superan con creces los costos de no utilizarlo, por lo que esto debería permitir que la mayoría de las personas comiencen de inmediato.\nTambién demostraremos cómo se relaciona la línea de comandos con la GUI. No cubriremos temas importantes como configurar claves SSH, configurar un archivo gitconfig, resolver conflictos, los cuales se explican detalladamente en otros lugares. En cambio, proporcionaremos una descripción general básica de cómo configurar un repositorio y el funcionamiento diario del seguimiento de archivos del proyecto.\nConfiguración de un repositorio.\nPara comenzar a rastrear archivos de un proyecto, necesitamos configurar un repositorio de proyecto que mantenga todos los archivos relevantes (es decir, datos, metadatos, código, funciones, manuscritos, etc.) en un solo lugar. El directorio del proyecto será la base de un proyecto reproducible (consultar gestión de proyectos para más detalles).\nPara aprovechar al máximo el control de versiones, queremos tener tanto una copia local (en tu computadora) como una copia remota (en la nube) de nuestro repositorio. Hay varios sitios de alojamiento que te permitirán crear un repositorio en la nube, incluyendo GitHub y BitBucket. Bitbucket te permite crear repositorios privados de forma gratuita, mientras que todo en GitHub es público (aunque puedes pagar por repositorios privados).\nPara configurar un servidor remoto, primero deberás crear una cuenta en uno de estos sitios de alojamiento. Una vez que hayas terminado, también deberás descargar e instalar Git (si usas la línea de comandos), que es el sistema de control de versiones que utilizaremos. Crear un repositorio en línea es fácil, inicia sesión en tu cuenta y ve a Repositories -&gt; Create new repository. Dale un nombre y eso es todo, si estás en Bitbucket, literalmente te dirá qué hacer a continuación en la línea de comandos (Figura 1 y 2).\n\n\n\nFigura 1 - Configuración de un repositorio para un proyecto en Bitbucket\n\n\n\n\n\nFigura 2 - Explicación de Bitbucket sobre cómo vincular el repositorio en el servidor a un repositorio local. Ver también la Figura 4 a continuación\n\n\nSi estás usando la línea de comandos, simplemente navega hasta el lugar donde deseas que se encuentre la carpeta y escribe los comandos de la Figura 2 anterior. También te guiará a través de algunos pasos importantes sobre cómo funciona Git, pero abordaremos eso más adelante. El componente clave de este repositorio que se necesita para configurarlo y conectarlo a una carpeta en tu computadora es la ruta de acceso ssh o http:\nhttps://UNSW_EnvComp@bitbucket.org/UNSW_EnvComp/unsw_envcomp.git.\nEste nombre de ruta de acceso establecerá la conexión entre una carpeta de proyecto local (es decir, una carpeta en tu computadora) y la existente en el servidor (es decir, Bitbucket). También te permitirá clonar el repositorio completo en tu computadora si es necesario. Si deseas clonar el repositorio en tu computadora, usa la línea de comandos para navegar hasta donde deseas que se encuentre la carpeta y escribe lo siguiente:\ngit clone https://UNSW_EnvComp@bitbucket.org/UNSW_EnvComp/unsw_envcomp.git\nEsto copiará efectivamente desde el servidor todo tu repositorio en la ubicación que desees y es casi idéntico a los comandos sugeridos en la Figura 2. También creará automáticamente una conexión entre tu directorio local y el servidor. También puedes proporcionar esta ruta a tus colaboradores, y siempre que se les dé acceso de lectura y escritura al repositorio (se puede cambiar en la configuración del repositorio en línea), también podrán obtener todo desde el servidor de la misma manera descrita anteriormente.\nCrear un repositorio en SourceTree también es bastante fácil. Una vez que tengas la ruta para tu repositorio, ve a SourceTree, haz clic en \"New Repository\" -&gt; \"Clone from URL\" (Figura 3). Agrega la ruta ssh o http://, decide dónde deseas que viva el repositorio en tu computadora y luego haz clic en \"clone\". Esto descargará los contenidos del repositorio (si no está vacío) y configurará esta conexión en SourceTree para que puedas comenzar a rastrear y enviar al servidor. Si el repositorio ya existe en tu computadora y deseas agregarlo a SourceTree, simplemente haz clic en \"Add existing local repository\" en lugar de \"Clone from URL\".\n\n\n\nFigura 3 - Configurando un repositorio en tu computadora en SourceTree\n\n\n\n\nRastreo de archivos y envío a la nube\nAhora que estás configurado y has clonado o iniciado un repositorio en una carpeta local, el truco está en hacer un seguimiento de los archivos importantes, como datos, código R y documentos (es decir, .Rmd, .md, .txt, etc.). ¿Cómo hace esto Git? Bueno, básicamente hay un conjunto de comandos clave que se necesitan y que se implementan continuamente para realizar un seguimiento de los archivos nuevos o modificados. Utilizas estos comandos al final de tu día de trabajo o cuando hayas realizado un gran avance y quieras enviar los cambios al servidor lo antes posible. Los comandos clave y su orden de ejecución son los siguientes:\n\nConsigue una taza grande de café (o té).\nEste comando obtiene los nuevos archivos del servidor.\n\n\ngit pull\n\n\nRealiza una serie de acciones en los archivos del proyecto y luego verifica qué ha cambiado.\n\n\ngit status\n\n\nAgrega un archivo modificado o no rastreado al área de preparación. Reemplaza \"nombre de archivo\" con la ruta y el nombre del archivo (por ejemplo, \"/R/function.R\"). También puedes agregar muchos archivos a la vez usando \"--all\" en lugar de \"nombre de archivo\".\n\n\ngit add nombre_de_archivo\n\n\nUna vez agregado, deja un mensaje sobre qué hace el archivo, cómo ha cambiado, etc.\n\n\ngit commit -m \"agrega un mensaje sobre los cambios en el archivo\"\n\n\nEnvía los cambios al servidor.\n\n\ngit push\n\n\n¡Ve a casa y relájate! Tu proyecto está bajo control de versiones.\n\nLos procedimientos paso a paso mencionados anteriormente se utilizan cuando deseas realizar un seguimiento de los cambios, y estos comandos se escriben literalmente en la línea de comandos en el orden presentado (ver Figura 4). El Paso 2 solo es necesario al comenzar a trabajar en el directorio y recuperará cualquier cambio que los colaboradores hayan realizado y que aún no conozcas. Si eres el único que trabaja en el proyecto y solo usas una computadora, entonces el Paso 2 es opcional porque tu directorio local siempre tendrá los archivos más actualizados.\n\n\n\nFigura 4 - Ejemplo de secuencia de pasos en la ventana de la Terminal. Los comandos están resaltados en azul y muestran cómo inicializar un nuevo repositorio, hacer seguimiento de archivos no rastreados, agregar esos archivos y confirmarlos y enviarlos al servidor.\n\n\nSourceTree hace todo esto mucho más fácil al proporcionarte una serie de casillas de selección y botones que puedes utilizar para hacer todo esto muy rápidamente. Para agregar un archivo para realizar su seguimiento, simplemente haz clic en las casillas de los archivos relevantes (o en todas) en el área de “Archivos sin preparar” (Figura 5). Una vez que hayas seleccionado todas las casillas, los archivos estarán en el área de “Archivos preparados”. Esto es equivalente a git add en la línea de comandos. Ahora están listos para ser confirmados, y puedes dejar un mensaje de confirmación en la ventana en la parte inferior de la pantalla y hacer clic en el botón Commit en la esquina derecha. Esto es equivalente a git commit -m \"mensaje\" en la línea de comandos.\n\n\n\nFigura 5 - Ejemplo de cómo agregar, confirmar y enviar en SourceTree\n\n\nUna vez que tus archivos hayan sido confirmados, los archivos listos para ser enviados aparecerán en el botón “Push” en la parte superior de la pantalla (Figura 5). Haz clic en esto y serán movidos al servidor. A medida que edites y agregues archivos, nuevos archivos aparecerán en el área de preparación y simplemente puedes repetir la secuencia anterior para agregar, confirmar y enviar estos al servidor. El botón “Pull” corresponde al paso dos en la secuencia de Git mencionada anteriormente y siempre debe ser utilizado al inicio del día si estás trabajando con colaboradores.\nIgnorar archivos que no deseas rastrear\nEn muchos casos, habrá archivos dentro de un proyecto que no deseas rastrear. Esto puede ser porque no son necesarios para la reproducibilidad del proyecto, o son archivos solo para ti y no necesarios para tus colaboradores. Estos podrían incluir notas útiles relevantes solo para ti, documentos en los que estás interesado o incluso archivos de salida grandes. Estos archivos no necesitan ser rastreados, por ejemplo, puede que no haya necesidad de rastrear figuras y tablas, ya que en muchos casos se pueden regenerar a partir del código del proyecto y ocupan mucho espacio en el servidor. Ignorar estos archivos es fácil usando lo que se llama un archivo .gitignore. El “.” al inicio del archivo significa que está oculto y no se mostrará en un directorio de trabajo normal. Afortunadamente, Git y SourceTree reconocerán estos archivos. El .gitignore es un archivo que se mantiene en el directorio principal de la carpeta de tu proyecto y generalmente se rastrea. Puedes crear un archivo .gitignore fácilmente en la línea de comandos utilizando algunas líneas de código (Figura 6).\n\n\n\nFigura 6 - Ejemplo de cómo crear un archivo .gitignore e ignorar la carpeta de fotos/ del repositorio imaginario de Git\n\n\nEn la Figura 6, he ignorado una carpeta “photos/”. Podemos ver que ahora desaparece (comparado con la Figura 4) de nuestra área de preparación en la ventana de Terminal. Simplemente podemos agregar cualquier archivo o carpeta que no queremos rastrear aquí antes de agregarlo para que Git los ignore. Si queremos comenzar a rastrear un archivo que hemos ignorado, simplemente abre el archivo .gitignore en un editor de texto y elimina el archivo que deseas rastrear. Guarda el archivo .gitignore y luego agrega, commit y push como se menciona anteriormente.\n\n\nAyuda adicional\nLos sistemas de control de versiones, y Git en particular, pueden hacer mucho más de lo que hemos cubierto anteriormente. Hemos proporcionado una breve introducción sobre cómo comenzar y usar Git para rastrear los archivos de tu proyecto. Si deseas saber más sobre cómo crear ramas, manejar conflictos y muchas otras características útiles, puedes visitar la página web de Git, donde encontrarás toda la información relevante para realizar tareas más avanzadas.\nAutor: Daniel Noble\nAño: 2016\nÚltima actualización: r format(Sys.time(), \"%b %Y\")"
  },
  {
    "objectID": "coding-skills/writing-functions/index.html",
    "href": "coding-skills/writing-functions/index.html",
    "title": "Como escribir funciones",
    "section": "",
    "text": "Una función es un fragmento de código independiente que realiza una tarea específica. Piensa en ellas como secuencias de código puntuales que se escriben por de forma que complementan la secuencia de código principal.\nEl código bien escrito utiliza muchas funciones, esto incluye:\nEs difícil hacer algo en  sin utilizar algunas de las funciones incorporadas, pero ¿has escrito tus propias funciones? Si no es así, es hora de empezar.\nA continuación, dedicaremos algo de tiempo a describir los dos principales tipos de función, por qué usar funciones y luego cómo se construyen."
  },
  {
    "objectID": "coding-skills/writing-functions/index.html#tipos-de-función",
    "href": "coding-skills/writing-functions/index.html#tipos-de-función",
    "title": "Como escribir funciones",
    "section": "Tipos de función ",
    "text": "Tipos de función \nEn términos generales, existen dos tipos principales de función:\nEn primer lugar, están las funciones que hacen algo y devuelven un objeto. Estas funciones toman algunos valores de entrada especificados, realizan algunas manipulaciones/operaciones y luego te devuelven un objeto. Ejemplos incluyen mean() (calcula la media de un vector), lm() (ajusta un modelo lineal) o read.csv (carga una tabla de datos).\nEn segundo lugar, están las funciones que tienen algún efecto externo en tu computadora o entorno de trabajo. Estas funciones hacen algo pero no devuelven ningún objeto. Ejemplos incluyen funciones como write.csv() (escribe un archivo en el disco), plot() (genera un gráfico), library() (carga un paquete).\nPara el primer tipo, a menudo guardarás el resultado en una variable y lo manipularás aún más. Por ejemplo, supongamos que queremos calcular el promedio de la variable altura de las muestras en los datos de algae. Podemos usar la función mean:\n\nmean_height &lt;- mean(algae$height)\n\nEste código calcula el promedio de algae$altura y lo guarda en la variable mean_height. Podemos consultar la respuesta ejecutando el nombre de la variable:\n\nmean_height\n\n[1] 0.4590399\n\n\nTambién podemos ejecutar la función sin asignar la salida a una variable. La salida aún se devuelve, esta vez en la consola, se imprime y se pierde.\n\nmean(algae$height)\n\n[1] 0.4590399\n\n\nEn cambio, la salida del segundo tipo de función no necesita ser asignada a una variable. Además, la función no imprime nada en la pantalla. Por ejemplo:\n\nwrite.csv(Algae, \"data.csv\")"
  },
  {
    "objectID": "coding-skills/writing-functions/index.html#por-qué-usar-funciones",
    "href": "coding-skills/writing-functions/index.html#por-qué-usar-funciones",
    "title": "Como escribir funciones",
    "section": "¿Por qué usar funciones? ",
    "text": "¿Por qué usar funciones? \nEntonces, ¿por qué es tan útil dividir tu script en muchas funciones separadas pero que colaboran entre sí? ¿Por qué no escribir un solo script grande y largo? Hay varias formas en las que escribir funciones puede mejorar tu código.\n\nEl código con funciones es más fácil de leer\nEscribir funciones es una buena forma de organizar tus métodos analíticos en fragmentos autocontenidos. Generalmente, el código escrito de esta manera es mucho más fácil de leer.\nConsidera algunas de las funciones que ya has utilizado en R. Por ejemplo, mean().\nEsta función ya está predefinida dentro del paquete base de R, lo que significa que no tuviste que decirle al ordenador cómo calcular la media, y debido a que ese trabajo de programación ya se ha realizado, simplemente puedes usar la función en tu propio script. Imagina si cada vez que necesitaras calcular una media tuvieras que escribir lo siguiente:\n\nsum(x) / length(x)\n\nIncluso esta línea de código utiliza dos funciones: la función sum y la función length. Si estas no estuvieran disponibles, tendrías que escribir el método completo cada vez que necesitaras calcular un promedio.\n\n(x[1] + x[2] + x[3] + x[4] + x[5]) / 5\n\nEn cambio, simplemente usamos mean sin pensarlo dos veces.\nEs importante destacar que es mucho más fácil entender qué hace mean(x) que la línea anterior. Al leer el código, sabes exactamente qué está sucediendo. Usando la fórmula completa, sería menos obvio qué estaba sucediendo cada vez que quisieras calcular el promedio de una variable diferente.\nLo cual plantea un punto importante: las funciones deben tener un nombre claro e informativo que indique qué hace la función.\nLas funciones aumentan rápidamente la facilidad con la que puedes leer e interpretar el código.\nNo es obvio qué hace el código sqrt(var(algae$height)/length(algae$height)), mientras que es inmediatamente obvio qué hace el código standard_error(x).\n\n\nOrganiza tu flujo de trabajo\nBasándonos en la idea de hacer que el código sea más fácil de leer, las funciones pueden ayudar a organizar todo tu flujo de trabajo y facilitar su seguimiento. A menudo, las personas tienen un largo script de análisis que es difícil de interpretar. Cuando usas funciones, tu script de análisis puede verse mucho más simple:\n\ndata &lt;- read_csv(\"Algal_traits.csv\")\n\nstats_species &lt;- fit_model_species(data)\n\nstats_spatial &lt;- fit_model_spatial(data)\n\nmake_plot_species(stats_species)\n\nmake_plot_spatial(stats_spatial)\n\nsave_output(stats_species)\n\nAquí, todas las funciones como fit_model_species son aquellas que has escrito tú mismo.\n¡Qué fácil es interactuar con eso en comparación con un largo script con cientos de líneas!\n\n\n\nReutiliza código (también conocido como “No te repitas”)\nNo solo usar la función mean es más informativo (es más fácil entender qué hace tu línea de código), sino que también es reutilizable. Una vez que se define una función, se puede usar una y otra vez, no solo dentro del mismo script, sino también en otros scripts.\nPara resaltar aún más esto, vamos a través de un ejemplo de cómo escribir nuestra propia función para calcular el error estándar de un grupo de variables. R tiene funciones incorporadas para el promedio de un vector (mean(x)) y la desviación estándar (sd(x)), pero no para el error estándar. Para calcular el error estándar,\n\\[SE_\\bar{x}= \\sqrt{\\frac{var}{n}}\\]\nnecesitamos la varianza y el tamaño de la muestra, n. Estos son relativamente fáciles de calcular usando otras funciones básicas en R. var calculará la varianza y length proporcionará la longitud del vector y, por lo tanto, el tamaño de la muestra (n).\nDigamos que primero queríamos el promedio y el error estándar de la altura. Esto se obtiene mediante\n\nsqrt(var(algae$height) / length(algae$height))\n\n[1] 0.04067788\n\n\nImagine ahora que deseas calcular las mismas estadísticas en una variable diferente (por ejemplo, peso seco). Cuando te enfrentas a la necesidad de usar este código dos veces, es posible que te tientes a copiar y pegarlo en un nuevo lugar, lo que resultaría en dos copias del fragmento anterior en nuestro código. Sin embargo, un enfoque mucho más elegante (y beneficioso a largo plazo) es convertirlo en una función y llamar a esa función dos veces.\nSi primero definimos una función para el error estándar:\n\nstandard_error &lt;- function(x) {\n  sqrt(var(x) / length(x))\n}\n\nsimplemente usamos standard_error como lo haríamos con cualquier otra función.\n\nstandard_error(algae$height)\n\n[1] 0.04067788\n\nstandard_error(algae$dryweight)\n\n[1] 0.02190001\n\n\n\n\nReduce chance of errors\nWrapping code into functions reduces the chance of making inadvertent errors. Such errors may not cause your code to crash, but may cause the results to be wrong. These types of mistakes are the hardest to find and can render our results meaningless.\nThere are at least two ways functions reduce the chance of errors.\nFirst, copy and paste leads to errors. Without a function, you may copy and past code all over the place. For example, if I wanted to calcualte the standard error of a bunch of variables (without using our new standard_error function)\n\n\nReducir la posibilidad de errores\nEnvolviendo el código en funciones se reduce la posibilidad de cometer errores inadvertidos. Estos errores pueden no hacer que tu código falle, pero pueden causar que los resultados sean incorrectos. Este tipo de errores son los más difíciles de encontrar y pueden hacer que nuestros resultados carezcan de sentido.\nHay al menos dos formas en las que las funciones reducen la posibilidad de errores.\nPrimero, copiar y pegar conduce a errores. Sin una función, es posible que copies y pegues código por todas partes. Por ejemplo, si quisiera calcular el error estándar de varias variables (sin usar nuestra nueva función standard_error):\n\nsqrt(var(algae$height) / length(algae$height))\n\n[1] 0.04067788\n\nsqrt(var(algae$dryweight) / length(algae$dryweight))\n\n[1] 0.02190001\n\nsqrt(var(algae$length) / length(algae$dryweight))\n\n[1] 0.1824489\n\n\n¿Notaste el error? ¡Olvidé cambiar la segunda variable en la tercera línea! El código se ejecutará pero dará resultados incorrectos. Esto es menos probable si escribimos:\n\nstandard_error(algae$height)\n\n[1] 0.04067788\n\nstandard_error(algae$dryweight)\n\n[1] 0.02190001\n\nstandard_error(algae$length)\n\n[1] 0.1824489\n\n\nSegundo, las funciones limitan el alcance de las variables y aplican una limpieza. Al calcular algo, es común crear nuevas variables. Por ejemplo, digamos que calculamos el error estándar de la siguiente manera:\n\nvar_x &lt;- var(algae$height)\nn &lt;- length(algae$height)\nsqrt(var_x / n)\n\n[1] 0.04067788\n\n\nTen en cuenta que ahora tienes dos nuevos objetos en tu entorno: var_x y n:\n\nvar_x\n\n[1] 0.0992814\n\nn\n\n[1] 60\n\n\nPuedes deshacerte de ellos ejecutando:\n\nrm(var_x, n)\n\n(la función rm() “remueve”, es decir, elimina, objetos del entorno).\nPero, ¿qué pasa si olvidas hacerlo? Existe un verdadero peligro de que más adelante reutilices accidentalmente la variable n o var_x, pensando que son algo que no son. Y si tienen nombres no específicos como n, el riesgo de que esto ocurra es alto.\nEn cambio, si colocas el código anterior dentro de una función, como se muestra a continuación, este peligro desaparece.\n\nstandard_error &lt;- function(x) {\n  var_x &lt;- var(algae$height)\n  n &lt;- length(algae$height)\n  sqrt(var_x / n)\n}\n\nCuando ejecutas:\n\nstandard_error(algae$height)\n\n[1] 0.04067788\n\n\nEl resultado se devuelve, pero las variables var_x y n no se ven en ningún lugar. Esto se debe a que se eliminaron automáticamente cuando la función terminó.\nCualquier variable creada dentro de una función se elimina automáticamente al finalizar la función. Por lo tanto, el uso de funciones nos deja con un espacio de trabajo limpio y ordenado. Además, el entorno dentro de la función es mucho más seguro que el entorno global, porque es menos probable que obtengamos variables aleatorias de otro lugar.\n\n\nAyuda a tu cerebro a resolver problemas grandes\nLa mejor manera de resolver problemas grandes y complejos es dividirlos en una serie de problemas más pequeños. Es bien sabido que nuestros cerebros no pueden lidiar con más de aproximadamente 5-10 bits de información a la vez.\nEscribir funciones nos permite identificar una serie de problemas más pequeños y resolverlos uno por uno, utilizando todo nuestro poder cognitivo.\nCuando observo la función standard_error definida anteriormente, puedo pensar en las operaciones que se realizan (suma, división, raíz cuadrada) de forma aislada del problema más amplio que estoy resolviendo (estudiar algas).\nComo regla general, una buena función hace una cosa bien. Si esa una cosa es complicada, puede estar compuesta por un conjunto de funciones más pequeñas (es decir, pasos), cada una haciendo una cosa bien."
  },
  {
    "objectID": "coding-skills/writing-functions/index.html#escribiendo-tus-propias-funciones",
    "href": "coding-skills/writing-functions/index.html#escribiendo-tus-propias-funciones",
    "title": "Como escribir funciones",
    "section": "Escribiendo tus propias funciones ",
    "text": "Escribiendo tus propias funciones \nAhora veamos más de cerca la mecánica de escribir una función.\n\nLa sintaxis de una función\nUna definición de función tiene la siguiente forma:\n\nfunction_name &lt;- function(arg1, arg2, ...) {\n  statements # do useful stuff\n  object # return something\n}\n\nfunction_name: El nombre de la función. Puede ser cualquier texto válido sin espacios, pero debes evitar usar nombres que ya estén en uso en R. Verifica si tu nombre ya está siendo utilizado como una palabra clave preguntando por la página de ayuda ?function_name (no es una garantía del 100%, pero es una buena verificación). Además, trata de utilizar nombres que describan lo que hace la función. Un nombre largo como calcular_error_estandar es mucho mejor que algo corto e intuitivo como f.\narg1, arg2, …: Los argumentos de la función. Puedes escribir una función con cualquier número de argumentos, los cuales pueden ser cualquier objeto de R (numérico, cadenas de texto, caracteres, data.frames, matrices, otras funciones).\ncuerpo de la función: El código entre las {} es el cuerpo de la función y se ejecuta cada vez que se llama a la función. Este es el código que realiza todas las tareas útiles y se denomina cuerpo de la función.\nvalor de retorno: La última línea de código es el objeto que se debe devolver. A veces verás a las personas escribir return(objeto), aunque es suficiente con escribir objeto.\nUtilizando este formato, una función para calcular el error estándar de los valores en el objeto x sería:\n\nstandard_error &lt;- function(x) {\n  sqrt(var(x) / length(x))\n}\n\nPara poder usar la función, necesitas ejecutar ese código en tu consola. Una vez definida, podemos llamar a la función como lo haríamos con cualquier otra función.\n\nstandard_error(algae$height)\n\n[1] 0.04067788\n\n\n\n\nArgumentos predeterminados\nEchemos un vistazo más de cerca a la función mean. Escribir ?mean en la consola muestra los detalles relevantes de “help”. Observa la estructura:\n\nmean(x, trim = 0, na.rm = FALSE, ...)\n\nEl primer argumento x es nuestro vector de números. Para usar la función, necesitamos especificar algo para x, por ejemplo:\n\nmean(x = algae$height)\n\no simplemente\n\nmean(algae$height)\n\nLa primera versión deja explícito que los valores en algas$altura fuera de la función son pasados a la variable x dentro de la función. La segunda versión hace lo mismo, pero de manera menos explícita. Funciona porque R toma los valores de altura y los asigna al primer argumento sin nombre en nuestra llamada de función, que se asigna al primer argumento sin nombre en la definición de la función. Entonces, lo siguiente también funcionará:\n\nmean(na.rm = TRUE, x = algae$height)\nmean(na.rm = TRUE, algae$height)\n\nBut what are those are other arguments in the function definition: trim and na.rm? These are optional arguments, with default values set as specified. The function needs a value to run but unless you specify it, it will use the default.\nTry running the mean() function on the strength variable.\nPero, ¿qué son esos otros argumentos en la definición de la función: trim y na.rm?. Estos son argumentos opcionales, con valores predeterminados especificados. La función necesita un valor para ejecutarse, pero a menos que lo especifiques, utilizará el valor predeterminado.\nIntenta ejecutar la función mean() en la variable strength.\n\nmean(algae$strength)\n\nWarning in mean.default(algae$strength): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\nObserva que obtenemos NA, esto se debe a que por defecto la función no sabe cómo tratar los valores faltantes (NA es un valor faltante) y hay uno en esa columna de datos. Cómo tratar los valores faltantes depende en gran medida de lo que estés tratando de calcular (consulta el módulo de ayuda sobre importación de datos), pero en este caso, nos gustaría eliminar los NA antes de calcular la media. Esto se puede lograr estableciendo el argumento na.rm en TRUE:\n\nmean(algae$strength, na.rm = TRUE)\n\nWarning in mean.default(algae$strength, na.rm = TRUE): argument is not numeric\nor logical: returning NA\n\n\n[1] NA\n\n\nLas funciones mean, var, sd, sum se comportan de manera similar. Sin especificar el argumento, las funciones utilizan su valor predeterminado, que en este caso es na.rm=FALSE. Por lo tanto, estos dan el mismo resultado.\n\nmean(algae$strength)\n\nWarning in mean.default(algae$strength): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\nmean(algae$strength, na.rm = FALSE)\n\nWarning in mean.default(algae$strength, na.rm = FALSE): argument is not numeric\nor logical: returning NA\n\n\n[1] NA\n\n\nSin embargo, podemos anular esto si es lo que deseamos:\n\nmean(algae$strength, na.rm = TRUE)\n\nWarning in mean.default(algae$strength, na.rm = TRUE): argument is not numeric\nor logical: returning NA\n\n\n[1] NA\n\n\nNotarás que muchas funciones tienen argumentos con valores predeterminados.\nVolviendo a nuestra nueva función standard_error, agreguemos un nuevo argumento na.rm para que se comporte como mean y las otras funciones mencionadas anteriormente:\n\nstandard_error &lt;- function(x, na.rm = FALSE) {\n  sqrt(var(x, na.rm = na.rm) / sum(!is.na(x)))\n}\n\nAl igual que las otras funciones, hemos establecido el comportamiento predeterminado de na.rm en FALSE.\nAhora, probemos nuestra nueva función en la variable de fuerza con datos faltantes, alternando entre na.rm = TRUE y na.rm = FALSE.\n\nstandard_error(algae$strength)\n\nWarning in var(x, na.rm = na.rm): NAs introducidos por coerción\n\n\n[1] NA\n\nstandard_error(algae$strength, na.rm = FALSE)\n\nWarning in var(x, na.rm = na.rm): NAs introducidos por coerción\n\n\n[1] NA\n\nstandard_error(algae$strength, na.rm = TRUE)\n\nWarning in var(x, na.rm = na.rm): NAs introducidos por coerción\n\n\n[1] 0.03870419\n\n\nDentro de la función, el valor para na.rm que se recibe se pasa a la función var. La función var ya tiene un argumento na.rm incorporado en ella (ver archivo de ayuda ?var), pero length no lo tiene. Podemos usar la función sum(!is.na(x)) para calcular n. La función is.na probará cada valor del vector x para ver si falta. Si no falta (el ! significa NO), entonces devuelve un TRUE para esa posición, y al contar los valores devueltos como TRUE con sum, estamos contando efectivamente solo los valores que no faltan.\n\n\nFunciones que extienden funciones\nDigamos que tienes un script donde constantemente deseas establecer na.rm=TRUE y te cansaste de escribirlo en todas partes:\n\nstandard_error(algae$height, na.rm = TRUE)\nstandard_error(algae$strength, na.rm = TRUE)\n...\n\n(Además, nos estamos repitiendo mucho y aumentando el riesgo de errores, ¿qué sucede si olvidamos?)\nUn enfoque aquí es definir una nueva función que se base en nuestra función anterior pero con el comportamiento deseado. Por ejemplo,\n\nstandard_error_narm &lt;- function(x) {\n  standard_error(x, na.rm = TRUE)\n}\n\nPodemos llamar ahora a la nueva función y obtener el mismo resultado que se muestra arriba especificando na.rm=TRUE\n\nstandard_error_narm(algae$strength)\n\nWarning in var(x, na.rm = na.rm): NAs introducidos por coerción\n\n\n[1] 0.03870419\n\n\nSi bien el ejemplo con standard_error tal vez sea un poco trivial, puedes aplicar este enfoque en muchos casos. Por ejemplo, una función que crea un tipo de gráfico con los valores predeterminados configurados exactamente como te gustan.\n\n\n¿Para qué sirve el argumento ...?\n¿Has notado el argumento ... en la definición de la función mean anterior? ¿De qué se trata eso? El elemento ..., o puntos suspensivos, en la definición de una función permite que se pasen otros argumentos a la función y se los transmita a otra función dentro de la función que se llama, sin tener que escribirlos todos por nombre. Por ejemplo, en la definición de la función standard_error_narm podríamos escribir en su lugar:\n\nstandard_error_narm &lt;- function(...) {\n  standard_error(..., na.rm = TRUE)\n}\n\nCuando llamas a standard_error_narm definida de esta manera, cualquier argumento que no sea na.rm se pasará directamente a la siguiente función. Esto evita tener que repetir los argumentos de una función al definir otra.\nUn ejemplo menos trivial es el uso de plot. Podría escribir una función que modifique algunos de los valores predeterminados de plot, de modo que no tenga que repetirlos una y otra vez.\n\nmy_plot &lt;- function(...) {\n  plot(..., pch = 16, las = 1, log = \"xy\")\n}\n\n\n\nAgregar comentarios a tu función\nAntes de terminar, hay una última cosa que debes hacer. Es una buena idea agregar comentarios a tu función, ya que esto te ahorrará muchos problemas cuando tengas que corregir algo más adelante. Los comentarios de una función deben contener una breve descripción de la función (una oración), una lista de los argumentos de la función con una descripción de cada uno (incluido el tipo de dato) y una descripción del valor de retorno. Los comentarios de la función deben escribirse inmediatamente arriba o abajo de la línea de definición de la función.\n\nstandard_error &lt;- function(x, na.rm) {\n  # Computes the sample standard error\n  #\n  # Args:\n  #  x: Vector whose standard error is to be calculated. x must have length greater than one.\n  #  y: na.rm can either be T or F. T removes missing values before calculating standard error.\n  #\n  # Return:\n  #  The standard error of x\n  sqrt(var(x, na.rm = na.rm) / sum(!is.na(x)))\n}\n\nOtra forma común de anotar funciones es utilizando la sintaxis de roxygen2.\n\n\nAlmacenar y utilizar funciones\nUna vez que adquieras el hábito de escribir funciones, es una buena idea mantenerlas en un archivo separado que contenga todas tus funciones juntas. ¿Por qué? Porque de lo contrario, tendrás estos archivos grandes y pesados que entorpecen tu script. Si ya has resuelto un problema y sabes cómo hacer algo, ¿por qué no guardarlo en algún lugar donde puedas acceder a ello solo si es necesario?\nPara mantener tus funciones fuera del camino, te recomendamos mantener todas las funciones de cada proyecto juntas en una carpeta llamada R dentro del directorio de tu proyecto. (Para obtener más información sobre la configuración de proyectos, consulta nuestra publicación sobre gestión de proyectos.)\nPara hacer que estas funciones sean accesibles en tu flujo de trabajo, puedes usar la función source para cargar los archivos de funciones en la memoria, por ejemplo:\n\nsource(\"R/stats.R\")\n\nA menudo, puedes tener una serie de archivos\n\nsource(\"R/data_cleaning.R\")\nsource(\"R/stats.R\")\nsource(\"R/plots.R\")\n\nEs cuestión de preferencia si utilizas un solo archivo o varios archivos.\n\n\nEscribir funciones para trabajar con el operador de pipes %&gt;% - |&gt;\nPara muchos de nosotros, los pipes se han convertido en una parte esencial de nuestro flujo de trabajo. (Si esto te resulta desconocido, consulta nuestra publicación sobre el uso de pipes en manipulación de datos).\nImportante, puedes escribir funciones que funcionen con el operador de pipes. Lo único que necesitas hacer es configurar tu función de modo que el primer argumento sea el objeto que se está pasando al pipe. De hecho, nuestra función standard_error ya funciona con pipes, asumiendo que estás pasando x:\n\nalgae$height %&gt;% standard_error()\n\n[1] 0.04067788\n\n\n\n\nDevolviendo múltiples argumentos\nLos ejemplos anteriores devuelven un solo elemento. ¿Qué sucede si quiero devolver varios elementos desde una función? La respuesta es devolver un objeto list. Las listas son útiles porque puedes agrupar muchos elementos diferentes.\nPor ejemplo, podríamos escribir una función que devuelva varias estadísticas de una variable:\n\nsummary_stats &lt;- function(x, na.rm = TRUE) {\n  list(\n    mean = mean(x, na.rm = na.rm),\n    var = var(x, na.rm = na.rm),\n    n = sum(!is.na(x))\n  )\n}\n\nSi ejecutamos esta función, recibimos un objeto que tiene elementos nombrados:\n\nheight_stats &lt;- summary_stats(algae$height)\n\nnames(height_stats)\n\n[1] \"mean\" \"var\"  \"n\"   \n\nheight_stats$mean\n\n[1] 0.4590399\n\nheight_stats$var\n\n[1] 0.0992814\n\nheight_stats$n\n\n[1] 60\n\n\nDe hecho, muchas funciones hacen esto, por ejemplo, lm() (para ajustar un modelo lineal). Al ajustar un modelo, podemos verificar que es una lista, luego pedir los nombres de los elementos devueltos y comenzar a llamarlos por nombre:\n\nfit &lt;- lm(algae$height ~ algae$dryweight)\n\nis.list(fit)\n\n[1] TRUE\n\nnames(fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nfit$coefficients\n\n    (Intercept) algae$dryweight \n      0.4054402       0.1276447"
  },
  {
    "objectID": "coding-skills/writing-functions/index.html#qué-hace-que-una-buena-función",
    "href": "coding-skills/writing-functions/index.html#qué-hace-que-una-buena-función",
    "title": "Como escribir funciones",
    "section": "Qué hace que una buena función",
    "text": "Qué hace que una buena función\nFinalmente, repasemos algunos puntos sobre qué hace que una buena función.\nEs corta \nIdealmente, cada función hace una sola cosa bien. A menudo, esto significa muchas funciones cortas. Las funciones cortas son extremadamente útiles. Incluso si el código en el cuerpo de la función es más complejo, idealmente aún hace una sola cosa bien.\nHace una sola cosa bien  Tiene un nombre intuitivo"
  },
  {
    "objectID": "coding-skills/writing-functions/index.html#ayuda-adicional",
    "href": "coding-skills/writing-functions/index.html#ayuda-adicional",
    "title": "Como escribir funciones",
    "section": "Ayuda adicional",
    "text": "Ayuda adicional\nPuedes encontrar más ayuda sobre funciones en:\n\nTutorial de DataCamp sobre funciones\nInformación de Hadley Wickham sobre funciones para usuarios intermedios y avanzados.\nEl material introductorio oficial de R sobre cómo escribir tus propias funciones\n\nAutor: Original de Keryn F Bain; revisado por Daniel S Falster\nAño: 2018\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "data-manipulation/combining-datasets/index.html",
    "href": "data-manipulation/combining-datasets/index.html",
    "title": "Combinando conjuntos de datos.",
    "section": "",
    "text": "Combinar conjuntos de datos es una tarea esencial para muchos proyectos. Por ejemplo, podemos tener datos sobre la abundancia de especies, pero también un conjunto de datos de fuentes externas sobre las condiciones ambientales durante nuestras observaciones (por ejemplo, temperatura, precipitación, elevación, tipo de vegetación).\n\nUtilizaremos el paquete dplyr, el cual tiene muchas funciones convenientes para combinar conjuntos de datos. Primero, carga el paquete:\n\nlibrary(dplyr)\n\nAl igual que en la página de ayuda para Subconjunto de datos, utilizaremos un conjunto de datos donde se muestrearon murciélagos en un bosque de regeneración en el sureste de Australia que ha sido adelgazado para reducir la densidad de árboles.\nTambién leeremos un conjunto de datos que proporciona las ubicaciones geográficas de cada sitio (por ejemplo, descargado de un GPS) y los datos meteorológicos nocturnos (en este caso, descargados y resumidos de los datos en línea de la Oficina de Meteorología).\nDescarga los tres conjuntos de datos de muestra (Bats_data.csv, Geo_data.csv y Weather_vars.csv) e impórtalos en R.\n\nBats &lt;- read.csv(file = \"Bats_data.csv\")\nGeo &lt;- read.csv(file = \"Geo_data.csv\")\nWeather &lt;- read.csv(file = \"Weather_vars.csv\")\n\n\nUnir datos\ndplyr tiene una función para agregar simplemente todas las columnas de un conjunto de datos (z) a otro (y):\n\nbind_cols(y, z)\n\nEsto solo es útil si los dos conjuntos de datos tienen el mismo número de filas y las filas están ordenadas de la misma manera (simplemente empareja las filas por su posición).\nEn este caso, tenemos dos conjuntos de datos bastante diferentes que queremos unir a nuestro conjunto de datos principal sobre la abundancia de murciélagos. El conjunto de datos de ubicaciones geográficas se ha medido a nivel de sitio, por lo que cada sitio separado tiene una medición separada de latitud y longitud.\nEl segundo conjunto de datos con información meteorológica ha utilizado la misma estación meteorológica para todos los sitios, pero se ha medido para cada noche de encuesta por separado. Por lo tanto, utilizaremos la columna “Sitio” para unir el conjunto de datos “Geo” y la columna “Fecha” para unir el conjunto de datos “Weather”.\nLa función left_join agregará filas coincidentes de un segundo conjunto de datos al primero, especificando qué variable en el primero se utiliza para hacer la coincidencia.\nPara agregar las ubicaciones geográficas al conjunto de datos de Murciélagos, utilizando la columna “Sitio” para coincidir las filas, utilizaríamos:\n\nBats_withGeo &lt;- left_join(Bats, Geo, by = \"Site\")\n\nLa parte by=\"Site\" en realidad es opcional, y si la omites, left_join buscará columnas con el mismo nombre para usar como columna de coincidencia, y recibirás un mensaje al respecto de la función. Esto es equivalente a la línea de arriba:\n\nBats_withGeo &lt;- left_join(Bats, Geo)\n\nJoining with `by = join_by(Site)`\n\n\nSin embargo, recomendamos especificar la columna de coincidencia para ser más explícitos acerca del comportamiento deseado de la coincidencia. Para agregar ahora los datos climáticos a ese nuevo conjunto de datos, utilizando la variable Fecha para coincidir filas, usaríamos:\n\nBats_withGeoWeather &lt;- left_join(Bats_withGeo, Weather, by = \"Date\")\n\nPuedes verificar lo que ha sucedido al ver el conjunto de datos utilizando la función dim, para encontrar las dimensiones de nuestros conjuntos de datos, o colnames para ver una lista de nombres de columnas.\n\ndim(Bats)\ndim(Bats_withGeoWeather)\ncolnames(Bats_withGeo)\ncolnames(geo.weather.join)\n\nVerás que el nuevo conjunto de datos, Bats_withGeoWeather, tiene el mismo número de filas que el original, Bats, pero con cuatro columnas adicionales: “Latitud”, “Longitud”, “Temperatura_media” y “Humedad_media”.\n\n\nTratando con conjuntos de datos desordenados\nEse fue un ejemplo agradable y ordenado, donde todos los códigos en nuestro conjunto de datos principal (Bats) coincidían con los códigos en los conjuntos de datos en los que queríamos hacer la unión. También existen una serie de funciones que ayudan en situaciones de datos desordenados.\nVamos a simular un par de conjuntos de datos desordenados. Imagina que tienes datos de actividad de murciélagos para cinco sitios y datos de densidad de árboles para cinco sitios, pero solo dos de esos sitios (D y E) contienen mediciones para ambas variables. Esto puede ocurrir cuando utilizas conjuntos de datos recopilados por diferentes personas o con diferentes propósitos en el mismo estudio.\n\nBat_sim &lt;- as.data.frame(cbind(\n  Site = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  Activity = c(62, 29, 30, 23, 24)\n))\n\nTree_sim &lt;- as.data.frame(cbind(\n  Site = c(\"D\", \"E\", \"F\", \"G\", \"H\"),\n  Tree_density = c(525, 390, 477, 778, 817)\n))\n\n\n\n  Site Activity\n1    A       62\n2    B       29\n3    C       30\n4    D       23\n5    E       24\n\n\n  Site Tree_density\n1    D          525\n2    E          390\n3    F          477\n4    G          778\n5    H          817\n\n\nSi queremos combinar conjuntos de datos solo para las filas donde tengamos medidas tanto de actividad de murciélagos como de densidad de árboles y no nos importa el resto de los datos, podemos usar inner_join.\n\nBat_withTree_inn.join &lt;- inner_join(Bat_sim, Tree_sim, by = \"Site\")\nprint(Bat_withTree_inn.join)\n\n  Site Activity Tree_density\n1    D       23          525\n2    E       24          390\n\n\nSi estamos principalmente interesados en el conjunto de datos de murciélagos, podemos usar left_join() como se mencionó anteriormente para mantener todas las mediciones de murciélagos y agregar densidades de árboles donde las tengamos. Los datos faltantes de densidad de árboles serán NA.\n\nBat_withTree_left.join &lt;- left_join(Bat_sim, Tree_sim, by = \"Site\")\n\n\n\n  Site Activity Tree_density\n1    A       62         &lt;NA&gt;\n2    B       29         &lt;NA&gt;\n3    C       30         &lt;NA&gt;\n4    D       23          525\n5    E       24          390\n\n\nPor otro lado, si estamos principalmente interesados en las densidades de árboles, podemos usar right_join para mantener todos los datos de densidad de árboles e incluir la actividad de murciélagos donde la hemos medido. Los datos faltantes de murciélagos serán NA.\n\nBat_withTree_right.join &lt;- right_join(Bat_sim, Tree_sim, by = \"Site\")\n\n\n\n  Site Activity Tree_density\n1    D       23          525\n2    E       24          390\n3    F     &lt;NA&gt;          477\n4    G     &lt;NA&gt;          778\n5    H     &lt;NA&gt;          817\n\n\nAlternativamente, si queremos mantener TODOS los datos y decidir más adelante qué excluir, podemos usar full_join.\n\nBat_withTree_full.join &lt;- full_join(Bat_sim, Tree_sim, by = \"Site\")\n\n\n\n  Site Activity Tree_density\n1    A       62         &lt;NA&gt;\n2    B       29         &lt;NA&gt;\n3    C       30         &lt;NA&gt;\n4    D       23          525\n5    E       24          390\n6    F     &lt;NA&gt;          477\n7    G     &lt;NA&gt;          778\n8    H     &lt;NA&gt;          817\n\n\nFinalmente, podemos consultar qué filas de nuestros datos tienen o no tienen coincidencias en otra tabla. Por ejemplo, podemos usar semi_join para imprimir solo las filas de las mediciones de murciélagos que tienen un sitio coincidente donde se midieron árboles, o usar anti_join para encontrar lo contrario, donde no se midieron árboles.\n\nBat_inTree &lt;- semi_join(Bat_sim, Tree_sim, by = \"Site\")\n\n\n\n  Site Activity\n1    D       23\n2    E       24\n\n\n\nBat_notinTree &lt;- anti_join(Bat_sim, Tree_sim, by = \"Site\")\n\n\n\n  Site Activity\n1    A       62\n2    B       29\n3    C       30\n\n\n\n\nMás ayuda\nEste tutorial se basó en la excelente Hoja de referencia de manipulación de datos con dplyr y tidyr producida por Rstudio. Las imágenes fueron obtenidas del mismo documento. Puedes utilizar ?dplyr para obtener ayuda con este paquete.\nAutor: Rachel V. Blakey\nAño: 2016, actualizado en 2022 por Will Cornwell\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "data-manipulation/index.html",
    "href": "data-manipulation/index.html",
    "title": "Manipulación de Datos",
    "section": "",
    "text": "Tener habilidades en la organización y resumen de datos es crucial para estudiantes e investigadores en todas las ciencias. En estas páginas, brindamos algunas orientaciones para manipular los datos de manera que se pueda ahorrar mucho tiempo en el análisis de los mismos y para crear figuras efectivas.\n\nManipulación de datos utilizando los paquetes dplyr, tidyr, reshape2.\n\nCreación de nuevas variables\nSubconjunto de datos - selección de filas y columnas\nResumen de datos - extracción de estadísticas resumidas\nCombinación de conjuntos de datos - unir conjuntos de datos enteros o partes mediante la coincidencia de filas\nReorganización de datos entre formatos amplio y largo.\n\n\nAyuda adicional\nLa Guía de Gestión de Datos en Ecología y Evolución de la Sociedad Ecológica Británica.\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: r format(Sys.time(), \"%b %Y\")"
  },
  {
    "objectID": "data-manipulation/making-new-variables/index.html",
    "href": "data-manipulation/making-new-variables/index.html",
    "title": "Creación de nuevas variables",
    "section": "",
    "text": "Crear nuevas variables a partir de las que ya están en tu conjunto de datos es una habilidad esencial para la manipulación de datos antes de realizar gráficos o análisis. Estas nuevas variables pueden ser una variable transformada que deseas analizar, una nueva variable que es una función de las existentes, o un nuevo conjunto de etiquetas para tus muestras.\nPara demostrar algunas de las funciones comúnmente utilizadas en R para hacer esto, consideremos un conjunto de datos sobre la especificidad alimentaria de herbívoros marinos en cinco especies de macroalgas. Se recolectaron veinte individuos replicados de cada una de las siete especies de macroalgas en el puerto de Sydney, y se registró la abundancia de siete especies de crustáceos herbívoros en cada muestra replicada (datos de Poore et al. 2000)).\n\nDescarga el conjunto de datos, Herbivore_specialisation.csv, y cárgalo en R.\n\nHerbivores &lt;- read.csv(file = \"Herbivore_specialisation.csv\", header = TRUE)\n\nLas dos primeras columnas son variables categóricas que etiquetan las muestras como provenientes de cada uno de los cinco hábitats o como recolectadas durante el día o la noche. La tercera columna es el número de réplica por combinación de hábitat y día/noche. La cuarta columna es la biomasa del hábitat muestreado y el resto de las columnas son las cuentas de cada especie de herbívoro en cada muestra.\n\nAgregar una nueva variable\n\nAgregar una nueva variable a un marco de datos existente se puede hacer asignando el resultado de una función determinada a un nuevo nombre de variable de la siguiente manera.\n\nHerbivores$log_Mass &lt;- log(Herbivores$Mass)\n\nHerbivores$Ampithoe &lt;- Herbivores$Ampithoe_caddi + Herbivores$Ampithoe_kava + Herbivores$Ampithoe_ngana\n\nLa primera línea crea una nueva variable llamada log_Mass, que es el logaritmo de la variable Mass del data frame Herbivores.\nLa segunda línea crea una nueva variable llamada Ampithoe, que es la suma de las abundancias de cada una de las tres especies de Ampithoe en el conjunto de datos.\nTener que hacer referencia tanto al marco de datos como al nombre de la variable en estas expresiones puede volverse bastante confuso, por lo que recomendamos utilizar funciones del paquete dplyr que nos permiten utilizar solo los nombres de las variables. Primero, carga el paquete:\n\nlibrary(dplyr)\n\nLa función mutate se utiliza para crear nuevas variables. Para obtener el mismo resultado que el código anterior, usaríamos:\n\nHerbivores &lt;- mutate(Herbivores, log_Mass = log(Mass))\n\nHerbivores &lt;- mutate(Herbivores, Ampithoe = Ampithoe_caddi + Ampithoe_kava + Ampithoe_ngana)\n\nIncluso mejor es ejecutar varias cosas a la vez. Podríamos crear tanto esas nuevas variables como muchas otras con un solo bloque de código. Por ejemplo:\n\nHerbivores &lt;- mutate(Herbivores,\n  log_Mass = log(Mass), # logaritmo de la Masa\n  Ampithoe = Ampithoe_caddi + Ampithoe_kava + Ampithoe_ngana, # suma de tres columnas\n  Total_abundance = rowSums(Herbivores[, 5:12]), # suma de las columnas 5 a 12 con todos los datos de abundancia\n  Total_abundance_perGram = Total_abundance / Mass # abundancia en números por gramo de hábitat\n)\n\n]Los argumentos de mutate son simplemente el nombre del data frame seguido de cualquier número de expresiones que creen nuevas variables.\nEn los ejemplos anteriores, ten en cuenta que las nuevas variables se han agregado al data frame existente y se han mantenido todas las variables antiguas. Puedes usar transmute si deseas eliminar las variables originales.\nEstas funciones se vuelven especialmente poderosas cuando se combinan con algunas de las otras en dplyr. Consulta nuestras páginas sobre subsetting y summarising data.\n\n\nRenombrar una variable\ndplyr ofrece una función sencilla, rename, para cambiar el nombre de cualquier variable. Por ejemplo, para cambiar “Mass” a “Biomass”, simplemente utilizamos:\n\nHerbivores &lt;- rename(Herbivores, Biomass = Mass)\n\n\n\nUnir varias columnas en una sola\n\nCombinar el contenido de varias columnas en una sola columna puede ser útil para proporcionar un conjunto diferente de etiquetas para las filas de tu conjunto de datos, o nuevos niveles de una variable categórica que puedas querer usar en gráficos. La función unite en el paquete tidyr nos permite hacer esto de manera muy fácil. Primero, instala y carga este paquete en R.\n\nlibrary(tidyr)\n\nSi quisiéramos crear una nueva variable categórica donde cada nivel fuera la combinación única de hábitat y día/noche, usaríamos:\n\nHerbivores &lt;- unite(Herbivores, \"Habitat_DayNight\", c(Habitat, DayNight), sep = \"_\")\n\nLos argumentos de unite son: * el data frame que se utilizará (en este caso Herbivores) * el nombre de la nueva variable (en este caso Habitat_DayNight) * las columnas a unir, dentro de c() * el carácter utilizado para separar los valores en cada columna que se une (en este caso **“_“**)\nVisualiza nuevamente el data frame y notarás la nueva variable, y el hecho de que las antiguas hayan sido eliminadas. Es mejor idea mantenerlas, agregando remove=FALSE.\n\nHerbivores &lt;- unite(Herbivores, \"Habitat_DayNight\", c(Habitat, DayNight), sep = \"_\", remove = FALSE)\n\n\n\nSeparar una columna en varias\n\nSeparar el contenido de una columna en varias variables separadas también es muy útil si los niveles de las variables categóricas en el conjunto de datos original son en realidad combinaciones de más de una variable. La función separate en tidyr hace esto.\nPor ejemplo, si quisiéramos contrastar la abundancia de herbívoros entre los géneros de algas utilizados como hábitat (en lugar de especies individuales), necesitaríamos crear una nueva variable que contenga solo los nombres de los géneros. Podemos usar separate para crear dos nuevas columnas, una para el género y otra para la especie, a partir de los valores en la variable Habitat.\n\nHerbivores &lt;- separate(Herbivores, Habitat, c(\"Genus\", \"species\"), sep = \"_\", remove = FALSE)\n\nLos argumentos de separate son: * El data frame que se utilizará (en este caso Herbivores). * El nombre de la nueva variable a separar (en este caso Habitat). * Los nombres de las nuevas variables, dentro de c() (en este caso Genus y species). * El carácter utilizado para separar los valores en la columna que se está separando (en este caso “_“). * El remove=FALSE significa que mantenemos la variable que se está separando en la nueva versión del data frame.\nTen en cuenta que esto solo fue posible porque había un carácter que separaba las dos variables en el texto de aquella que se iba a separar (por ejemplo, no podríamos hacer esto si los nombres de las especies en la variable Habitat fueran originalmente GenusSpecies en lugar de Genus_species).\n\n\nMás ayuda\nEscribe ?mutate, ?unite y ?separate en la consola de R para obtener ayuda sobre estas funciones.\nCheat sheet de manipulación de datos con dplyr y tidyr producida por Rstudio. Algunas imágenes anteriores fueron obtenidas de este documento.\nIntroducing tidyr\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "data-manipulation/reshaping-data/index.html",
    "href": "data-manipulation/reshaping-data/index.html",
    "title": "Reorganización de datos",
    "section": "",
    "text": "Aprender cómo formatear datos es una habilidad esencial que te permitirá producir fácilmente gráficos y ejecutar análisis. Siempre debes buscar que los datos estén formateados de manera que cada observación sea una fila y cada variable sea una columna (consulta la ayuda sobre Entrada de datos). Sin embargo, para algunos tipos de variables, hay decisiones que debes tomar sobre cómo ingresar los datos.\nPor ejemplo, es muy común en las ciencias biológicas que registremos la abundancia de muchas especies de organismos en cada una de nuestras observaciones de replicación (transectos, cuadrantes, encuestas, etc.). Luego, tenemos dos opciones para ingresar esos datos: 1. Una columna separada para cada especie que registre su abundancia. 2. Dos columnas: una que identifique la especie y otra que registre la abundancia.\n\nConsidera un conjunto de datos que registra la abundancia de peces en cada uno de tres transectos que se establecieron en dos arrecifes en dos meses diferentes. Primero, importa este conjunto de datos de muestra, ReefFish.csv, para ver cómo está formateado.\n\nReefFish &lt;- read.csv(file = \"ReefFish.csv\", header = T)\n\n\n\n   Site   Month Transect   Species Abundance\n1 Reef1 January        1   RedFish         4\n2 Reef1 January        1  BlueFish         5\n3 Reef1 January        1 BlackFish        10\n4 Reef1 January        2   RedFish        42\n5 Reef1 January        2  BlueFish        13\n6 Reef1 January        3   RedFish         3\n\n\nEste marco de datos está organizado en un formato largo con una variable que identifica la especie de pez y otra variable que contiene los datos de abundancia. Las otras tres variables identifican cada transecto (Sitio, Mes y Transecto).\nEste formato es eficiente para la entrada de datos, ya que solo necesitas tener filas para las especies que estuvieron presentes en ese transecto en particular, y no tienes que seguir agregando columnas cada vez que se registre una nueva especie. Necesitarías los datos en este formato si quisieras utilizar un gráfico para contrastar la abundancia de peces entre las tres especies.\nSin embargo, si deseas contrastar la abundancia de una de las especies o crear una matriz de especies por muestra que se requiere en los diversos análisis multivariados que contrastan la composición de especies, los datos deberán manipularse de alguna manera.\nAfortunadamente, hay paquetes muy útiles en R que hacen esto posible. Si alguna vez has utilizado tablas dinámicas en Excel, hacen cosas similares.\n\nTransformar de formato largo a formato ancho\nEn este ejemplo, utilizaremos el paquete tidyr para convertir este marco de datos a un formato ancho que permitirá una exploración adicional de los datos. Primero, instala y carga el paquete.\n\nlibrary(tidyr)\n\nPara convertir este conjunto de datos en un formato amplio con una columna separada para cada especie de pez, utilizamos la función pivot_wider().\n\nReefFish.wide &lt;- pivot_wider(ReefFish,\n                             names_from = Species,\n                             values_from =  Abundance,\n                             values_fill = 0)\n\nLos argumentos de pivot_wider son: * El data frame que deseas convertir (en este caso, ReefFish) * La variable cuyos niveles se convertirán en nuevas columnas (en este caso, Species) * La variable que contiene los valores que llenarán las nuevas columnas (en este caso, Abundance) * values_fill=0 indica a pivot_wider que complete con cero cuando falta una especie en un transecto determinado.\n\n\n# A tibble: 6 × 6\n  Site  Month   Transect RedFish BlueFish BlackFish\n  &lt;chr&gt; &lt;chr&gt;      &lt;int&gt;   &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n1 Reef1 January        1       4        5        10\n2 Reef1 January        2      42       13         0\n3 Reef1 January        3       3        0         8\n4 Reef2 January        1       5        0        72\n5 Reef2 January        2       0        9         0\n6 Reef2 January        3      24      101        65\n\n\nTen en cuenta que el formato amplio de estos datos ahora tiene una columna para cada especie con todos los valores de abundancia. Obtendrías tantas columnas como niveles únicos en la columna Species. Obtendrías tantas filas como combinaciones únicas de las variables que no se están dividiendo (Sitio, Mes y Transecto en este ejemplo).\nAhora puedes graficar o analizar cualquier especie individual en relación con posibles variables predictoras como Sitio o Mes. Los análisis multivariables de la composición de especies en relación con posibles variables predictoras también requieren que cada especie esté en columnas separadas. Puedes seleccionarlas de este data frame con la función select de dplyr (ver Subsetting data).\n*Ten en cuenta que si tuvieras un transecto sin observaciones de peces, necesitarías agregar una fila al conjunto de datos original, tal vez con un código de especie de “ninguna”. De lo contrario, esa observación de replicación faltaría en el formato amplio, lo cual es necesario si deseas comparar la abundancia entre arrecifes, etc.\n\n\nTransformación de un formato amplio a un formato largo\nLa función pivot_longer convertirá los datos del formato amplio a un formato largo.\nAquí, podemos utilizar este código para obtener nuestro conjunto de datos original a partir del conjunto de datos en formato amplio que acabamos de crear.\n\nReefFish.long &lt;- pivot_longer(ReefFish.wide, \n                             cols = 4:6,\n                              names_to = \"Species\", \n                              values_to = \"Abundance\")\n\n\n\n# A tibble: 6 × 5\n  Site  Month   Transect Species   Abundance\n  &lt;chr&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;         &lt;int&gt;\n1 Reef1 January        1 RedFish           4\n2 Reef1 January        1 BlueFish          5\n3 Reef1 January        1 BlackFish        10\n4 Reef1 January        2 RedFish          42\n5 Reef1 January        2 BlueFish         13\n6 Reef1 January        2 BlackFish         0\n\n\nLos argumentos de pivot_longer() son: * El data frame que deseamos convertir * Los nombres de las columnas a convertir (por nombre o por número de columna) * names_to = - el nombre de la nueva variable que contendrá los nombres de las variables (en este caso, Species) * values_to = - el nombre de la nueva variable que contendrá los valores de las variables (en este caso, Abundance)\nEn ese código, elegimos las tres variables con los datos de especies por sus números de columna (4 a 6). También podríamos usar sus nombres o la primera y última columna en una secuencia de columnas. Por ejemplo,\n\nReefFish.long &lt;- pivot_longer(ReefFish.wide, \n                              cols = c(\"BlackFish\", \"BlueFish\", \"RedFish\",\n                              names_to = \"Species\", \n                              values_to = \"Abundance\")\n\nReefFish.long &lt;- pivot_longer(ReefFish.wide,\n                              cols = BlackFish:RedFish,\n                              names_to = \"Species\",\n                              values_to = \"Abundance\")\n\nHemos recreado nuestro conjunto de datos original. La única diferencia es que las filas se han ordenado y las especies que estaban ausentes de un transecto dado tienen su propia fila con un valor de abundancia de cero.\n\n\nReorganizando con reshape2\nEl paquete reshape2 también nos permite reorganizar datos y tiene algunas capacidades adicionales que no están presentes en tidyr.\n\nlibrary(reshape2)\n\nEn lugar de pivot_wider, se utiliza dcast para convertir de un formato largo a ancho. Este código hará lo mismo que vimos anteriormente.\n\nReefFish.wide &lt;- dcast(ReefFish,\n  Site + Month + Transect ~ Species,\n  value.var = \"Abundance\", fill = 0\n)\n\nLos argumentos de dcast son: * El marco de datos que deseas convertir (en este caso, ReefFish) * Las variable(s) que deseas incluir sin cambios como columnas en el nuevo marco de datos están a la izquierda de ~ (Site, Month y Transect) * Las variable(s) que se están convirtiendo en nuevas columnas están a la derecha de ~ (en este caso, Species) * La variable que contiene los valores que se llenarán en las nuevas columnas (especificada por value.var, en este caso, Abundance) * fill=0 indica a dcast que rellene con ceros cuando una especie esté ausente de un transecto dado.\nEn lugar de pivot_longer, se utiliza melt para convertir de un formato ancho a largo.\n\nReefFish.long &lt;- melt(ReefFish.wide,\n  id.vars = c(\"Site\", \"Month\", \"Transect\"),\n  measure.vars = c(\"RedFish\", \"BlueFish\", \"BlackFish\"),\n  variable.name = \"Species\", value.name = \"Abundance\"\n)\n\nLos argumentos de melt son: * El marco de datos que deseamos convertir * id.vars especifica las columnas que permanecen sin cambios (aquí las variables predictoras que etiquetan cada observación repetida) * measure.vars especifica qué variables contienen los datos que irán en la nueva columna * variable.name y value.name proporcionan los nombres de la nueva columna.\nLo que reshape2 puede hacer y tidyr no puede es la capacidad de resumir datos mientras reorganizas de formato largo a ancho.\nEn el ejemplo anterior, solo había una fila que pertenecía a cada combinación de Site, Month y Transect. Si hay filas duplicadas para cada combinación de las variables que deseas mantener en el nuevo marco de datos (las que están a la izquierda de ~), debes indicarle a dcast cómo quieres tratar los duplicados (por ejemplo, sumarlos o calcular su media).\nPor ejemplo, si quisiéramos combinar los transectos de cada estudio, podríamos eliminar Transect de la lista de variables a incluir en el nuevo marco de datos y agregar un argumento (fun.aggregate = sum) para indicarle a dcast que queremos sumar los valores de los tres transectos en cada combinación de Site/Month.\n\nReefFish.wide_sum &lt;- dcast(ReefFish,\n  Site + Month ~ Species,\n  value.var = \"Abundance\",\n  fun.aggregate = sum, fill = 0\n)\n\n\n\n   Site    Month BlackFish BlueFish RedFish\n1 Reef1 February       116       15      60\n2 Reef1  January        18       18      49\n3 Reef2 February        42      106      18\n4 Reef2  January       137      110      29\n\n\nSi queremos la media de los tres transectos, podemos usar fun.aggregate = mean.\n\nReefFish.wide_mean &lt;- dcast(ReefFish,\n  Site + Month ~ Species,\n  value.var = \"Abundance\", fun.aggregate = mean, fill = 0\n)\n\nTambién puedes incluir expresiones más complejas en la fórmula de reshape para crear nuevas variables que sean combinaciones de las antiguas. Por ejemplo, podrías crear una nueva columna para cada combinación de Especie y Mes agregando ambas variables a la derecha del ~.\n\nReefFish.wide_combined &lt;- dcast(ReefFish,\n  Site + Transect ~ Species + Month,\n  value.var = \"Abundance\", fill = 0\n)\n\n\n\n   Site Transect BlackFish_January BlueFish_February BlueFish_January\n1 Reef1        1                10                12                5\n2 Reef1        2                 0                 3               13\n3 Reef1        3                 8                 0                0\n4 Reef2        1                72                 0                0\n5 Reef2        2                 0                22                9\n6 Reef2        3                65                84              101\n  RedFish_February RedFish_January\n1               52               4\n2                0              42\n3                8               3\n4                0               5\n5                3               0\n6               15              24\n\n\n\n\nAyuda adicional\nEscribe ?pivot_wider y ?pivot_longer para obtener ayuda en R sobre estas funciones de tidyr.\nEscribe ?dcast y ?melt para obtener ayuda en R sobre estas funciones de reshape2.\nCheat sheet de manipulación de datos con dplyr y tidyr producido por Rstudio.\nData wrangling with dplyr and tidyr\nAn introduction to reshape2\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "data-manipulation/subsetting-data/index.html",
    "href": "data-manipulation/subsetting-data/index.html",
    "title": "Subconjuntos de datos",
    "section": "",
    "text": "A menudo queremos subconjuntar nuestros datos, ya sea para examinar filas o columnas particulares de nuestro conjunto de datos, o para extraer observaciones con propiedades específicas. Podemos realizar subconjuntos en la etapa de exploración de datos para investigar si nuestra variable de respuesta difiere en su relación con una variable predictora específica entre categorías (por ejemplo, para buscar una interacción). También podemos querer extraer elementos de nuestro conjunto de datos para análisis separados.\n\nEn este ejercicio, cargaremos un conjunto de datos donde se muestrearon murciélagos en un bosque de regeneración en el sureste de Australia que ha sido adelgazado para reducir la densidad de árboles.\nEl conjunto de datos incluye mediciones del número total de llamadas de murciélagos (“Actividad”) y el número de llamadas de murciélagos que indican comportamiento de forrajeo (“Forrajeo”) registradas durante una noche en 47 sitios y un total de 173 noches de muestreo. Se han recopilado variables relacionadas con el adelgazamiento del bosque y se enumeran como columnas que terminan en “adelgazado”. Como el bosque estudiado estaba en una llanura de inundación, también se recopilaron covariables relacionadas con la disponibilidad de agua y se enumeran en columnas que terminan en “agua”.\nPrimero, descarga el conjunto de datos, Bats_data.csv, y cárgalo en R.\n\nBats &lt;- read.csv(file = \"Bats_data.csv\", header = T, stringsAsFactors = F)\n\n\nFundamentos basicos para subconjuntos en R\nVerifica la estructura después de haber cargado los datos con la función str.\nAhora que tenemos una idea del conjunto de datos con el que estamos trabajando, podemos usar los operadores [] y $ para seleccionar filas y columnas en un marco de datos. Es bastante simple: colocamos el nombre del marco de datos del cual queremos seleccionar antes de los corchetes y dentro de los corchetes colocamos una coma. Los números a la izquierda de la coma representan las filas que deseamos seleccionar y los números a la derecha de la coma representan las columnas que deseamos seleccionar.\nPor ejemplo, si quisiéramos seleccionar solo datos de las primeras tres filas y todas las columnas, usaríamos:\n\nBats[1:3, ]\n\nSi quisiéramos solo las últimas cuatro columnas y todas las filas, usaríamos:\n\nBats[, 7:10]\n\nEn combinación, esto seleccionaría datos solo de las primeras tres filas de las últimas cuatro columnas:\n\nBats[1:3, 7:10]\n\nSi los números de fila y columna que deseas no forman una secuencia, podemos usar la función &gt;c() para concatenar índices de fila o columna. Por ejemplo, agreguemos la sexta fila y la segunda columna a nuestra selección anterior:\n\nBats[c(6, 1:3), c(2, 7:10)]\n\nEn lugar de averiguar qué variable está en qué columna numerada, a menudo es más fácil usar los nombres de las variables en un marco de datos y seleccionar la variable usando el operador $. Por ejemplo, para elegir solo la variable “Site”:\n\nBats$Site\n\n\n\nSubconjunto de filas en dplyr\n\nEl paquete dplyr tiene muchas funciones convenientes para el subconjunto de datos que pueden ser más intuitivas y rápidas para ti. Primero, instala y carga el paquete:\n\nlibrary(dplyr)\n\nSubconjunto por números de fila\nSi conoces los números de fila en los que estás interesado en hacer el subconjunto, puedes seleccionar fácilmente estas filas usando corchetes como se discutió anteriormente o la función slice en dplyr.\nPor ejemplo, para seleccionar solo las filas 10-12 en el marco de datos, usarías:\n\nBats.slice &lt;- slice(Bats, 10:12)\n\nSeleccionar filas que cumplan ciertos criterios\nPodemos hacer un subconjunto de estas filas usando la función filter. Por ejemplo, si solo queremos las filas donde se registró actividad de forrajeo, podríamos seleccionar las filas donde el recuento de llamadas de forrajeo es mayor que cero.\n\nBats.foraging &lt;- filter(Bats, Foraging &gt; 0)\n\nPuedes experimentar con una variedad de operadores lógicos al usar la función filter\n&lt; menor que\n&gt; mayor que\n== igual a\n&lt;= menor o igual a\n&gt;= mayor o igual a\n!= no igual a\nin.NA= es NA\n!is.na= no es NA\ny más.\nPuedes usar los comandos ?base::Logic y ?Comparison para obtener más información sobre estos operadores.\nNotarás que al usar las funciones anteriores, principalmente las hemos asignado a nuevos objetos, por ejemplo, Bats.foraging &lt;-.... Esto nos da la opción de usar los datos recién subconjuntados para cálculos adicionales.\nTomar una selección aleatoria de filas\nEs posible que deseemos seleccionar aleatoriamente un número o fracción de filas en nuestro conjunto de datos para validar nuestros modelos. Por ejemplo, podríamos tomar el 50% de los datos para construir el modelo y luego el 50% del modelo para probarlo con datos observados. La selección aleatoria de filas es importante si vamos a dividir los datos de esta manera, porque no queremos sesgar nuestros datos hacia ninguna propiedad o categoría en particular. Aquí creamos un nuevo marco de datos “Bats.50p” que tiene la mitad del número de filas del conjunto de datos inicial:\n\nBats.50p &lt;- sample_frac(Bats, size = 0.5, replace = FALSE)\n\nDe manera similar, podríamos solicitar que se muestre un cierto número de filas seleccionadas al azar del conjunto de datos. Por ejemplo, para seleccionar al azar 100 filas, usaríamos:\n\nBats.100r &lt;- sample_n(Bats, 100, replace = FALSE)\n\nSeleccionar filas con los valores más altos\ndplyr tiene una función muy útil para seleccionar n filas con los valores más altos de cualquier columna dada: top_n. A continuación, identificamos las tres filas de datos que contienen la mayor actividad total de murciélagos por noche:\n\nBats.top &lt;- top_n(Bats, 3, Activity)\nprint(Bats.top)\n\n    Site Activity Foraging      Date Treatment.thinned Area.thinned\n1 CC04A1      802        9 7/01/2013        short-term            0\n2 PC32A2     1070       66 7/01/2013       medium-term            0\n3 PC32A2      944       52 8/01/2013       medium-term            0\n  Time.since.thinned Exclusion.thinned Distance.murray.water\n1                  0         11.932831              143.9868\n2                  8          7.150972              429.2099\n3                  8          7.150972              429.2099\n  Distance.creek.water\n1             102.5009\n2             694.7085\n3             694.7085\n\n\n¡Wow, más de 1000 llamadas de murciélagos en una noche!\nEliminar filas duplicadas\nOtra función útil es eliminar filas duplicadas, por ejemplo, si hemos ingresado datos dos veces por accidente.\n\nBats.distinct &lt;- distinct(Bats)\n\nPodemos comparar el número de filas entre nuestro conjunto de datos original y nuestro nuevo conjunto de datos sin duplicados y nuestro conjunto de datos anterior utilizando &lt;nrow:\n\nnrow(Bats)\n\n[1] 173\n\nnrow(Bats.distinct)\n\n[1] 173\n\n\nEn este caso, los dataframes son idénticos porque nuestros datos no tienen filas duplicadas.\n\n\nSelección de columnas en dplyr\n\nComo se mencionó anteriormente, puedes seleccionar una columna $ o una o más mediante indexación con []. dplyr tiene la función select que te permite seleccionar columnas por nombre o mediante el uso de funciones auxiliares útiles.\nSeleccionar columnas por nombre\nPor ejemplo, para seleccionar solo la columna “Site” del marco de datos, o tanto las columnas “Site” como “Date”:\n\nBats_subset1 &lt;- select(Bats, Site)\nBats_subset2 &lt;- select(Bats, Site, Date)\n\nSeleccionar varias columnas por su posición\nPara seleccionar un grupo de columnas adyacentes entre sí, utiliza los nombres de la primera y última columna separados por :. Por ejemplo, esto selecciona todas las columnas entre “Site” y “Date”.\n\nBats_subset3 &lt;- select(Bats, Site:Date)\n\nSeleccionar columnas por sus propiedades\nLas funciones auxiliares incorporadas en select te permiten seleccionar columnas particulares según sus propiedades.\nPor ejemplo, podríamos seleccionar las dos columnas (Distance.murray.water y Distance.creek.water) por el inicio de los nombres de columna o por el final de los nombres de columna:\n\nBats_subset4 &lt;- select(Bats, starts_with(\"Distance\"))\nBats_subset4 &lt;- select(Bats, ends_with(\"water\"))\n\nSi estás familiarizado con las expresiones regulares, puedes usar la función auxiliar match. Por ejemplo, para seleccionar esas mismas dos columnas, podríamos usar ^ para indicar que el nombre de la columna comienza con los siguientes caracteres y $ para indicar que el nombre de la columna termina con los caracteres precedentes.\n\nBats_subset4 &lt;- select(bats, matches(\"^Distance\"))\nBats_subset4 &lt;- select(bats, matches(\"water$\"))\n\nPodemos usar contains cuando queremos seleccionar columnas que contienen ciertos caracteres o palabras en su nombre.\n\nBats_subset4 &lt;- select(Bats, contains(\"water\"))\n\nLa función auxiliar one_of seleccionará columnas que pertenezcan a una lista de nombres de columnas, recordando usar c() para concatenar la lista de nombres.\n\nBats_subset4 &lt;- select(bats, one_of(c(\"Distance.murray.water\", \"Distance.creek.water\")))\n\nTen en cuenta que los últimos 6 usos de select hicieron exactamente lo mismo: extrajeron las dos columnas, Distance.murray.water y Distance.creek.water.\nTambién puedes nombrar columnas específicas que deseas excluir de la selección usando un signo menos delante del nombre de la columna a excluir. Por ejemplo, para crear un marco de datos que ya no tenga la variable Foraging:\n\nBats_subset5 &lt;- select(Bats, -Foragaing)\n\nPor último, si tienes columnas numeradas, podemos usar la función auxiliar numrange para seleccionar columnas específicas. Por ejemplo, este código seleccionaría las columnas llamadas var1, var2 y var3.\n\nselect(data, num_range(\"var\", 1:3))\n\n\n\nAyuda adicional\nEste tutorial se basó en la excelente hoja de referencia para el manejo de datos con dplyr y tidyr producida por Rstudio. Las imágenes se obtuvieron del mismo documento.\nPuedes escribir ?dplyr para obtener ayuda con este paquete.\nIntroducción a dplyr\nAutor: Rachel V. Blakey\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "data-manipulation/summarising-data/index.html",
    "href": "data-manipulation/summarising-data/index.html",
    "title": "Resumiendo datos",
    "section": "",
    "text": "Resumir nuestros datos suele ser el primer paso en la exploración de datos y es necesario para comprender los patrones en la magnitud y variabilidad de nuestras mediciones.\n\nUtilizaremos el paquete dplyr, que tiene muchas funciones convenientes para resumir datos, así que comencemos cargando el paquete.\n\nlibrary(dplyr)\n\nAl igual que en la página de ayuda sobre Subconjunto de datos, utilizaremos un conjunto de datos en el que se muestrearon murciélagos en un bosque en regeneración del sureste de Australia que se había adelgazado para reducir la densidad de árboles. Descarga el conjunto de datos, Bats_data.csv, e impórtalo en R.\n\nBats &lt;- read.csv(file = \"Bats_data.csv\", header = T, stringsAsFactors = F)\n\n\nResumir datos con dplyr\nObtener medidas resumidas de una sola variable\nPodemos usar la función summarise con una variedad de funciones resumen incorporadas de R para obtener estadísticas resumidas de nuestros datos.\nPor ejemplo, para obtener la actividad media de los murciélagos en todas las mediciones nocturnas del estudio, usaría la función de resumen mean dentro de la función summarise de la siguiente manera, especificando el marco de datos (Bats), la variable de la que quiero obtener la media (Activity) y un nombre para la nueva variable (Mean.Activity):\n\nsummarise(Bats, Mean.Activity = mean(Activity))\n\n  Mean.Activity\n1      316.0405\n\n\nPodemos agregar tantas otras medidas como deseemos, incluyendo una amplia gama de funciones resumen (descritas con #).\n\nsummarise(Bats,\n  mean.activity = mean(Activity), # media\n  min.Activity = min(Activity), # mínimo\n  max.Activity = max(Activity), # máximo\n  med.Activity = median(Activity), # mediana\n  sd.Activity = sd(Activity), # desviación estándar\n  var.Activity = var(Activity), # varianza\n  n.Activity = n(), # tamaño de la muestra\n  se.Activity = sd.Activity / sqrt(n.Activity), # error estándar\n  IQR.Activity = IQR(Activity) # rango intercuartílico\n) \n\n  mean.activity min.Activity max.Activity med.Activity sd.Activity var.Activity\n1      316.0405            9         1070          282    203.1081     41252.89\n  n.Activity se.Activity IQR.Activity\n1        173    15.44202          292\n\n\nSi estamos analizando factores, especialmente si están ordenados de alguna manera, es posible que algunas de las otras funciones resumen de dplyr sean útiles. Por ejemplo:\n\nsummarise(Bats,\n  first.site = first(Site), # primer valor en la variable Site\n  last.Site = last(Site), # último valor en la variable Site\n  third.Site = nth(Site, 3), # valor n-ésimo de Site\n  n.Sites = n_distinct(Site) # número de sitios distintos\n) \n\n  first.site last.Site third.Site n.Sites\n1     CC02A1    KC33A2     CC02A1      47\n\n\nObteniendo medidas resumidas de grupos de filas\nMuy a menudo estamos interesados en mediciones de valores promedio y variabilidad en diferentes categorías, por lo que necesitamos calcular medidas resumidas para variables dentro de cada categoría. Por ejemplo, en este conjunto de datos, es posible que deseemos comparar la actividad de murciélagos en diferentes bosques que varían en su historial de adelgazamiento. Los sitios pertenecen a cuatro categorías de historial de adelgazamiento: sitios de crecimiento denso que fueron adelgazados recientemente (“a corto plazo”) y a medio plazo (“a medio plazo”), sitios que nunca fueron adelgazados (“sin adelgazar”) y bosques abiertos maduros (“referencia”).\nPara resumir cualquier variable para cada una de estas categorías, utilizamos la función group_by en dplyr.\n\nBats_by_Treatment &lt;- group_by(Bats, Treatment.thinned)\n\nEn orden de conservar nuestro conjunto de datos original tal como está, he utilizado la función para crear un nuevo conjunto de datos llamado “Bats_by_Treatment”. Ahora puedo utilizar exactamente el mismo código que usamos anteriormente para resumir los datos para cada uno de los grupos.\n\nTreatment.summary &lt;- summarise(Bats_by_Treatment,\n  mean.Activity = mean(Activity), # media\n  min.Activity = min(Activity), # mínimo\n  max.Activity = max(Activity), # máximo\n  med.Activity = median(Activity), # mediana\n  sd.Activity = sd(Activity), # desviación estándar\n  var.Activity = var(Activity), # varianza\n  n.Activity = n(), # tamaño de la muestra\n  se.Activity = sd.Activity / sqrt(n.Activity), # error estándar\n  IQR.Activity = IQR(Activity) # rango intercuartílico\n)\n\nTen en cuenta que el marco de datos de entrada ahora es “Bats_by_Treatment”, en lugar de “Bats”.\nLos nuevos datos resumidos se han colocado en un nuevo objeto (Treatment.summary), que pertenece a la clase “tbl” específica de dplyr. Para convertir esto a la clase de marco de datos más ampliamente utilizada, podemos usar as.data.frame.\n\nTreatment.summary &lt;- as.data.frame(Treatment.summary)\n\nVisualiza este nuevo marco de datos para ver las estadísticas resumidas para cada una de las cuatro categorías de bosque.\n\nView(Treatment.summary)\n\nTambién puedes combinar el agrupamiento y la sumarización en un código más ordenado utilizando el operador de tubería %&gt;%. Por ejemplo, el código anterior podría reemplazarse por:\n\nTreatment.summary &lt;- Bats %&gt;%\n  group_by(Treatment.thinned) %&gt;%\n  summarise(\n    mean.Activity = mean(Activity), # media\n    min.Activity = min(Activity), # mínimo\n    max.Activity = max(Activity), # máximo\n    med.Activity = median(Activity), # mediana\n    sd.Activity = sd(Activity), # desviación estándar\n    var.Activity = var(Activity), # varianza\n    n.Activity = n(), # tamaño de la muestra\n    se.Activity = sd.Activity / sqrt(n.Activity), # error estándar\n    IQR.Activity = IQR(Activity) # rango intercuartílico\n  ) \n\nProblemas con datos faltantes\nLas cosas pueden salir mal en el campo y no siempre recopilamos todos los datos que necesitamos en cada sitio.\nPara mostrarte cómo esto afecta a la función summarise, podemos crear una nueva variable (Activity2), que es una copia de Activity pero con algunos de los datos de actividad (las primeras cuatro filas) ahora faltantes.\n\nBats$Activity2 &lt;- Bats$Activity\nBats$Activity2[1:4] &lt;- rep(NA, 4)\n\nA continuación, intentemos resumir los datos:\n\nsummarise(Bats, mean.Activity = mean(Activity2))\n\n  mean.Activity\n1            NA\n\n\nVerás que obtenemos un NA como resultado. Para obtener la media de todos los valores que están presentes, podemos agregar un argumento, na.rm=TRUE, para eliminar las filas que son NA.\n\nsummarise(Bats, mean.Activity = mean(Activity2, na.rm = TRUE))\n\n  mean.Activity\n1      314.8757\n\n\nJusto recuerda que esto disminuirá el tamaño de tu muestra. Esto funcionará para las funciones de resumen, excepto para la función n que cuenta el número de valores en un vector. Para contar los datos no faltantes, puedes usar este fragmento de código (un poco más complicado) para obtener el nuevo tamaño de muestra.\n\nlength(Bats$Activity2[!is.na(Bats$Activity2)])\n\n[1] 169\n\n\nEsto calcula el número de valores, length, del vector de valores de actividad de murciélagos, Bats$Activity2, donde no son NA, !is.na. Revisar Subsetting data puede ayudarte a entender esta afirmación.\n\n\nComunicando los resultados\nEscrito Si estuviéramos escribiendo un artículo sobre la actividad de murciélagos en diferentes tratamientos de adelgazamiento forestal, podríamos usar nuestros datos resumidos para hacer algunas observaciones generales al comienzo de nuestra sección de resultados, antes de un análisis más profundo. Por ejemplo: “Los murciélagos fueron dos veces más activos en bosques maduros abiertos (referencia) (365 ? 27) en comparación con el crecimiento regenerado no adelgazado (166 ? 21) (media ? EE). Sin embargo, la actividad de los murciélagos fue similar en los bosques adelgazados a mediano plazo (385 ? 36) y a corto plazo (350 ? 27) y en los bosques de referencia”.\nVisual Presentar medias y errores estándar de datos categóricos nos brinda una forma de comunicar visualmente un efecto del tratamiento (siempre y cuando esté respaldado por un análisis estadístico adecuado). Aquí hemos utilizado el paquete ggplot2 para crear un gráfico de barras simple con medias ? error estándar (barras de error).\n\n\n\n\n\n\n\nAyuda adicional\nEste tutorial se basó en el excelente Data wrangling with dplyr and tidyr cheat sheet producido por Rstudio. Las imágenes fueron obtenidas del mismo documento.\nPuedes escribir ?dplyr para obtener ayuda con este paquete.\nIntroduction to dplyr\nSi deseas aprender más sobre el lenguaje ggplot para graficar, echa un vistazo a nuestras hojas de trabajo sobre gráficos, comenzando con Plotting with ggplot: the basics.\nAutor: Rachel V. Blakey\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "datasets/index.html",
    "href": "datasets/index.html",
    "title": "Datasets",
    "section": "",
    "text": "Esta página contiene los conjuntos de datos o archivos que se utilizan en los tutoriales encontrados en este sitio web. La mayoría de los conjuntos de datos están en formato .csv, a menos que se indique lo contrario. Los conjuntos de datos están organizados alfabéticamente."
  },
  {
    "objectID": "datasets/index.html#a",
    "href": "datasets/index.html#a",
    "title": "Datasets",
    "section": "A",
    "text": "A\nAlgal_traits.csv"
  },
  {
    "objectID": "datasets/index.html#b",
    "href": "datasets/index.html#b",
    "title": "Datasets",
    "section": "B",
    "text": "B\nBac.counts.csv\nBac.factors.csv\nBats_data.csv"
  },
  {
    "objectID": "datasets/index.html#c",
    "href": "datasets/index.html#c",
    "title": "Datasets",
    "section": "C",
    "text": "C\nCairns_Mangroves_30m.tif\nCrabs.csv"
  },
  {
    "objectID": "datasets/index.html#e",
    "href": "datasets/index.html#e",
    "title": "Datasets",
    "section": "E",
    "text": "E\nEstuaries.csv\nEstuary_fish.csv"
  },
  {
    "objectID": "datasets/index.html#g",
    "href": "datasets/index.html#g",
    "title": "Datasets",
    "section": "G",
    "text": "G\nGeo_data.csv\nGreenhouse.csv"
  },
  {
    "objectID": "datasets/index.html#h",
    "href": "datasets/index.html#h",
    "title": "Datasets",
    "section": "H",
    "text": "H\nHarbour_metals.csv\nHerbivore_specialisation.csv"
  },
  {
    "objectID": "datasets/index.html#i",
    "href": "datasets/index.html#i",
    "title": "Datasets",
    "section": "I",
    "text": "I\nInteraction_plots.csv"
  },
  {
    "objectID": "datasets/index.html#l",
    "href": "datasets/index.html#l",
    "title": "Datasets",
    "section": "L",
    "text": "L\nLandsat_TIR.tif\nLeafshape.csv"
  },
  {
    "objectID": "datasets/index.html#m",
    "href": "datasets/index.html#m",
    "title": "Datasets",
    "section": "M",
    "text": "M\nmauna_loa_co2.csv\nMink.csv\nmultiband.tif"
  },
  {
    "objectID": "datasets/index.html#p",
    "href": "datasets/index.html#p",
    "title": "Datasets",
    "section": "P",
    "text": "P\nPilot.csv\nPlant_height.csv\nPrawns_MR.csv"
  },
  {
    "objectID": "datasets/index.html#r",
    "href": "datasets/index.html#r",
    "title": "Datasets",
    "section": "R",
    "text": "R\nReefFish.csv\nRevegetation.csv\nRiver_pH.csv"
  },
  {
    "objectID": "datasets/index.html#s",
    "href": "datasets/index.html#s",
    "title": "Datasets",
    "section": "S",
    "text": "S\nSessile.csv\nSnail_feeding.csv"
  },
  {
    "objectID": "datasets/index.html#t",
    "href": "datasets/index.html#t",
    "title": "Datasets",
    "section": "T",
    "text": "T\ntree_curtis1998.tre\nTurtles.csv"
  },
  {
    "objectID": "datasets/index.html#w",
    "href": "datasets/index.html#w",
    "title": "Datasets",
    "section": "W",
    "text": "W\nWeather_vars.csv\nAño: 2016\nÚltima actualización: r format(Sys.time(), \"%b %Y\")"
  },
  {
    "objectID": "getting-started-with-r/data-entry/index.html",
    "href": "getting-started-with-r/data-entry/index.html",
    "title": "Estructura de una base de datos - Data Entry",
    "section": "",
    "text": "Los datos son la vida de la ciencia. Como parte del proceso científico, invertimos una cantidad enorme de tiempo en recolectar, analizar y presentar datos. Sin embargo, antes de que podamos analizar datos, típicamente necesitamos ponerlos en un formato que pueda ser interpretado por otros, y más importante aún, por el software usado para análisis. Hacer esto bien puede ahorrarte mucho tiempo; hacerlo mal puede resultar en desperdiciar tiempo considerable “limpiando” y estructurando los datos para hacerlos utilizables.\n\n¿Cómo debe verse un conjunto de datos?\nLa mayoría de conjuntos de datos consisten en tablas rectangulares de valores (usualmente números o texto). Cada valor pertenece a una variable y a una observación. Una variable consiste en valores del mismo tipo (p. ej., temperatura, duración o abundancia). Una observación consiste en todos los valores medidos en la misma unidad (p. ej., parcela o individuo). La convención es almacenar las variables en columnas y las observaciones en filas.\n\nAquí hay un conjunto de datos abreviado de muestreo de insectos que muestra el formato deseado. Consiste en cinco variables y datos de las primeras nueve observaciones de réplica. Tenga en cuenta que la primera fila consta de un encabezado que enumera los nombres de cada variable en cada columna. Las variables son:\n\nSitio, con dos valores posibles que reflejan el tipo de hábitat (bosque o selva tropical).\nMétodo, con dos valores posibles que reflejan la técnica de muestreo (hojarasca o trampa de luz).\nInsecto, con los valores posibles que reflejan el tipo de insecto (hormiga, escarabajo, colémbolo, avispa, polilla o termita).\nNúmero, con valores que reflejan la abundancia de cada insecto en cada observación.\nGrupo, con un valor posible que refleja la identidad del recolector (A, B, C, etc.).\n\n\n\n# A tibble: 9 × 5\n  Site       Method     Insect     Number Group\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;\n1 woodland   leaflitter ant            26 A    \n2 woodland   leaflitter beetle          8 A    \n3 woodland   leaflitter springtail      6 A    \n4 woodland   lighttrap  beetle          8 A    \n5 woodland   lighttrap  wasp            1 A    \n6 woodland   lighttrap  moth            1 A    \n7 rainforest leaflitter ant            16 A    \n8 rainforest leaflitter termite         2 A    \n9 rainforest leaflitter springtail     63 A    \n\n\nEsta tabla está estructurada de tal manera que se pueden agregar nuevos datos con facilidad. Por ejemplo, es posible que deseemos combinar los datos de múltiples ‘grupos de recolección’, en cuyo caso es sencillo agregar nuevas filas al conjunto de datos existente. Si un conjunto adicional de muestras tiene nuevas especies de insectos, no es necesario agregar nuevas columnas para cada nueva especie, solo nuevos valores para la variable Insecto. Esto se conoce como formato largo. Algunos análisis (por ejemplo, contrastes de composición de especies) requerirán que cada especie se presente como una columna separada. Consulte nuestra sección de Manipulación de datos para obtener ayuda sobre cómo cambiar entre formatos largos y anchos.\n\n\n6 reglas de oro de la entrada de datos\n1. Cada columna debe contener solo un tipo de información (es decir, texto, números, fechas, booleanos). Por ejemplo, si se inserta texto dentro o debajo del conjunto de datos, R u otro software de análisis intentará interpretar el texto como un valor en la columna correspondiente. De manera similar, si se proporciona texto de resumen sobre los datos (un error común), esto se interpretará como la primera línea o encabezado de los datos reales.\n2. Los metadatos extensos (por ejemplo, descripciones de sitios) deben documentarse por lo general en un archivo separado, pero si son lo suficientemente breves, puede ser útil incluir esta información en su propia columna en la tabla de datos.\n3. Solo se deben utilizar caracteres ASCII (letras mayúsculas y minúsculas del alfabeto inglés, números y signos de puntuación comunes) para los nombres de archivo, los nombres de variables y los valores de datos.\n4. Aunque no afectará los análisis, para ayudar con la visualización de los datos en bruto, es una buena práctica ordenar las variables fijas primero, seguidas de las variables medidas. En la Tabla 1, Sitio y Método son fijos en el sentido de que los conocemos antes de la recopilación de datos, mientras que Insecto y Número son medidas. Sin embargo, esta no es una regla estricta. Por ejemplo, podemos querer ordenar Grupo al final, ya que esta información generalmente se tratará como metadatos en lugar de datos de interés real para el análisis.\n5. No manipule los datos en bruto una vez digitalizados. Idealmente, los datos en bruto deben tratarse como “solo lectura” y cualquier otra transformación o manipulación debe realizarse mediante scripts R guardados (o un lenguaje de programación alternativo). Esto evita insertar errores accidentalmente en los datos en bruto cada vez que se desea ajustar algo.\n6. Por último, siempre almacene los datos como .csv o .txt NO en otros formatos propietarios como .xls, .xlsx u otros, ya que no se pueden leer fácilmente en R o compartir con colaboradores. Los archivos de texto no requieren un software específico para leerlos.\n\n\nAyuda adicional\nUna vez que sus datos estén en formato .csv o .txt, consulte Importación de datos para obtener ayuda sobre cómo importarlos a R.\n\nBorer, ET, EW Seabloom, MB Jones & M Schildhauer. 2009. Algunas pautas simples para la gestión efectiva de datos. Bulletin of the Ecological Society of America, 90: 205-214. enlace\n\n\nWickham, H. 2014. Tidy data. Journal of Statistical Software, 59:(10). enlace\n\nAutor: Andrew Letten\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "getting-started-with-r/data-types-structure/index.html",
    "href": "getting-started-with-r/data-types-structure/index.html",
    "title": "Tipos de datos + Estructura",
    "section": "",
    "text": "Una de las fuentes más comunes de frustración para los principiantes en R es lidiar con diferentes estructuras y tipos de datos. Aquí se presenta una descripción general de las estructuras de datos y tipos más importantes y cómo comprobar y manipularlos.\nLos términos ‘estructura’ y ‘tipo’ a menudo se utilizan indistintamente. Para evitar confusiones, en esta página de ayuda, la estructura de datos se refiere a si los datos son un vector, matriz o marco de datos, etc., y el tipo se refiere a si los datos o variables son enteros, caracteres o numéricos, etc.\n\nDatos unidimensionales (vectores)\nLa estructura de datos más básica en R es un vector, un conjunto unidimensional de números o caracteres. Esta es la estructura de datos con la que trabajarás más a menudo, aunque desde dentro de un marco de datos (ver más abajo). Los vectores pueden ser atómicos o listas, los vectores atómicos difieren de las listas en que todos los elementos dentro de un vector atómico deben ser del mismo tipo (ver más abajo). En su mayor parte, trabajamos con vectores atómicos y el siguiente archivo de ayuda se refiere a este tipo de datos.\n\nTipos comunes de vectores\nLos tipos comunes de vectores atómicos son lógicos, enteros, numéricos (es decir, dobles), caracteres y factores. Puedes crear fácilmente cada uno de estos tipos de datos utilizando c(). En el ejemplo de enteros, la L obliga a R a considerar esos números como enteros en lugar de numéricos.\n\neg_logical &lt;- c(T, T, T, F, F)\neg_integer &lt;- c(1L, 6L, 1L, 5L, 4L)\neg_numeric &lt;- c(0, 2.3, 2.45, 2.99, -1.1)\neg_character &lt;- c(\"things\", \"in\", \"apostrophe\", \"are\", \"characters\")\neg_factor &lt;- factor(c(\"NSW\", \"NSW\", \"ACT\", \"WA\", \"WA\"))\n\n\n\n\nFactores (un tipo de datos especial)\nObserva cómo no pude simplemente usar c() para crear un factor. Aunque los factores se ven (y se comportan en su mayor parte) como caracteres, en realidad son un tipo especial de entero con categorías predefinidas, conocidas como niveles. El factor en este ejemplo tiene tres niveles: NSW, ACT y WA.\nPuedes verificar cuántos niveles tiene cualquier factor usando:\n\nlevels(eg_factor)\n\n[1] \"ACT\" \"NSW\" \"WA\" \n\n\nEsto los hace comportarse de manera diferente a los enteros. Una vez creados, los factores solo pueden contener un conjunto predefinido de niveles. Por ejemplo, si recolecta datos de sitios en toda Australia, podría tener el número fijo de estados como un factor, pero sería mejor tener una variable como sitio como un carácter si planea agregar datos de más sitios más adelante.\nPor defecto, R siempre ordenará los niveles en orden alfabético. Si desea que sus factores estén ordenados (es decir, pequeño, medio, alto), use ordered para definir la secuencia que le gustaría que se presenten los niveles. Esto es particularmente útil para gráficos para presentar las categorías a lo largo de un eje x en un orden más lógico.\n\nsizes &lt;- factor(c(\"small\", \"large\", \"large\", \"small\", \"medium\"))\nsizes\n## [1] small  large  large  small  medium\n## Levels: large medium small\nsizes &lt;- ordered(sizes, levels = c(\"small\", \"medium\", \"large\"))\nsizes\n## [1] small  large  large  small  medium\n## Levels: small &lt; medium &lt; large\n\n\n\nRevición de tipos de datos\nPuedes comprobar el tipo de dato de cualquier vector usando las funciones class o is.\nConsulta aquí para obtener más información sobre factores ordenados.\n\nChecking data types\nYou can check the data type of any vector using the class or is functions.\n\nclass(eg_logical)\n## [1] \"logical\"\nis.integer(eg_integer)\n## [1] TRUE\nis.factor(eg_factor)\n## [1] TRUE\n\n\n\n\nConversión automática\nComo todos los elementos dentro de un vector atómico deben ser del mismo tipo, combinar diferentes tipos hará que los datos se conviertan al tipo más flexible. Los tipos de menor a mayor flexibilidad son: lógico - logical, entero - integer, doble - double y caracter - character. Por ejemplo, combinar enteros y caracteres producirá un vector de caracteres. Esto es algo que debes tener en cuenta al manipular tus propios datos, especialmente cuando se fusionan data frames.\n\neg_coerced &lt;- c(\"tricks\", 1, 2, 3, 4)\nclass(eg_coerced)\n## [1] \"character\"\n\n\nConversión de datos\nSi descubre que sus datos tienen el tipo equivocado, puede utilizar las funciones as para convertir los datos de un tipo a otro. Tenga en cuenta lo que sucede con sus datos después de la conversión. Por ejemplo, la conversión de lógicos a numéricos reemplaza F con 0 y T con 1, y cualquier conversión insensata (como tratar de convertir el carácter “NSW” en un número) resultará en NAs.\n\nas.numeric(eg_logical)\n## [1] 1 1 1 0 0\nas.numeric(eg_character)\n## Warning: NAs introducidos por coerción\n## [1] NA NA NA NA NA\n\n\n\n\nDatos bidimensionales (matrices y marcos de datos)\nEn su mayor parte, tendemos a trabajar con datos bidimensionales que contienen tanto columnas como filas. Al igual que los vectores unidimensionales, vienen en dos formas: matrices, donde todos los vectores deben ser del mismo tipo de datos, y marcos de datos, que pueden estar compuestos por vectores que contienen diferentes tipos de datos.\n\nMatrices\nLas matrices se construyen fácilmente en R y se puede verificar si es una matriz utilizando la función class. Por ejemplo, para hacer una matriz con 3 filas y 2 columnas con 6 valores:\n\neg_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2)\n\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nPiensa en matrices como vectores atómicos con dimensiones; el número de filas y columnas. Al igual que las matrices atómicas, puedes verificar el tipo de datos con is y convertirlo usando las funciones as.\n\nis.logical(eg_matrix)\n## [1] FALSE\nas.numeric(eg_logical)\n## [1] 1 1 1 0 0\n\n\n\nData frames\nLa estructura de datos más común con la que trabajamos es el marco de datos (data frame en inglés). Los marcos de datos son simplemente una colección de vectores atómicos de igual longitud juntos. Son diferentes a las matrices ya que pueden contener vectores de diferentes tipos.\nPara crear un marco de datos simple que combine tres de los vectores que hicimos anteriormente, podríamos usar:\n\neg_data_frame &lt;- data.frame(eg_character, eg_factor, eg_numeric)\n\n\n\n  eg_character eg_factor eg_numeric\n1       things       NSW       0.00\n2           in       NSW       2.30\n3   apostrophe       ACT       2.45\n4          are        WA       2.99\n5   characters        WA      -1.10\n\n\nMás comúnmente, importamos datos ingresados en una hoja de cálculo directamente en un marco de datos usando read.csv (consulte la ayuda Importación de datos).\nPara verificar los tipos de datos dentro de un marco de datos, use la función str. Esto da una lista de salida para cada columna (es decir, variables) y el tipo de datos correspondiente.\n\nstr(eg_data_frame)\n\n'data.frame':   5 obs. of  3 variables:\n $ eg_character: chr  \"things\" \"in\" \"apostrophe\" \"are\" ...\n $ eg_factor   : Factor w/ 3 levels \"ACT\",\"NSW\",\"WA\": 2 2 1 3 3\n $ eg_numeric  : num  0 2.3 2.45 2.99 -1.1\n\n\nTen en cuenta que los tipos de datos pueden cambiar. En este ejemplo, el vector de caracteres ha sido convertido en un factor en el proceso de crear el marco de datos.\nSi deseas verificar el tipo de datos de una sola variable, o cambiar esa variable a otro tipo, utilizamos $ para acceder a esa variable desde dentro del marco de datos. Por ejemplo:\n\nstr(eg_data_frame$eg_character)\n##  chr [1:5] \"things\" \"in\" \"apostrophe\" \"are\" \"characters\"\nlevels(eg_data_frame$eg_factor)\n## [1] \"ACT\" \"NSW\" \"WA\"\nis.numeric(eg_data_frame$numeric)\n## [1] FALSE\n\n\n\n\nAyuda adicional\nPuedes encontrar más información sobre los tipos de datos en R aquí y aquí.\nAutor: Keryn Bain\nAño: 2016\nÚltima actualización: May 2023"
  },
  {
    "objectID": "getting-started-with-r/importing-data/index.html",
    "href": "getting-started-with-r/importing-data/index.html",
    "title": "Importing Data",
    "section": "",
    "text": "Antes de poder ejecutar análisis de datos o crear gráficos en R, debemos importar esos datos a R. Preparar y limpiar los datos para el análisis es esencial y a menudo requiere más tiempo que los propios análisis estadísticos. Es inusual que los datos sin procesar estén en el formato correcto y sin errores. La limpieza de datos es el proceso de identificar y corregir cualquier problema para que los datos puedan ser analizados fácilmente.\nImportar datos es un desafío importante para los principiantes. Este módulo brindará instrucciones para una de las formas más comunes de importar datos, así como algunos desafíos que puedes enfrentar y cómo superarlos. Algunos de estos problemas se pueden evitar siguiendo buenas prácticas de entrada y gestión de datos (lee primero Entrada de datos para obtener ayuda con esto).\n\nImportar datos como un marco de datos\nRecomendamos ingresar los datos en un programa de hoja de cálculo (por ejemplo, Excel) y guardar esos datos como un archivo de valores separados por comas (.csv). Estos se pueden leer fácilmente en R y compartir entre usuarios con diferentes programas de hojas de cálculo.\n\nEn este módulo, vamos a utilizar un conjunto de datos de muestra sobre el comportamiento de alimentación de un caracol marino para demostrar cómo importar los datos y los problemas más comunes que surgen al importar y limpiar datos en R.\nPrimero, guarda el archivo de datos, Snail_feeding.csv, en tu directorio de trabajo. Consulta Introducción a R para obtener ayuda sobre cómo configurar el directorio de trabajo.\nSegundo, importa el archivo de datos a un marco de datos llamado Snail_feeding utilizando la función read.csv.\n\nSnail_feeding &lt;- read.csv(\"Snail_feeding.csv\")\n\n\n\nLimpieza de data frames\nCuando utilizas read.csv, R utiliza varios argumentos por defecto que se pueden modificar para asegurar que tus datos se importen con menos errores. Echa un vistazo al archivo de ayuda dentro de R (escribiendo ?read.csv) para familiarizarte con algunos de estos argumentos.\nLos que son particularmente útiles son:\nheader = T - al especificar esto como T (es decir, TRUE), asegurarás que los valores en la primera fila de tus datos se traten como nombres de variables.\nstrip.white = T - esto eliminará los espacios en blanco al final y al principio de los factores. Este es un error común al escribir los datos (por ejemplo, “males” vs “males_”). Si lo establecemos como TRUE, ambos se convertirán en “males”. De lo contrario, R considerará que hay dos niveles diferentes.\nna.strings = \"\" - esto asegurará que las celdas vacías se reemplacen por NA (la forma en que R registra los datos faltantes). De manera molesta, R importa valores faltantes dentro de caracteres/factores como el valor ““. Usando na.strings = \"\", se insertarán NAs en su lugar. Además, si has codificado valores faltantes como algo distinto a un espacio en blanco, puedes definir ese valor faltante usando este argumento (por ejemplo, na.strings = c(\"\", \"-\", \"*\")).\nAl combinar todos estos en la función read.csv, obtendremos un marco de datos más limpio.\n\nSnail_feeding &lt;- read.csv(\"Snail_feeding.csv\", header = T, strip.white = T, na.strings = \"\")\n\n\n\nVerificación de los datos\nSi algo es un carácter cuando debería ser numérico, es posible que veas mensajes como “‘x’ must be numeric” o “non-numeric argument to binary operator”. De manera similar, si algo es un factor cuando debería ser un carácter, algunas operaciones con caracteres podrían fallar. Para evitar algunos de estos problemas, verifica tus datos utilizando str y summary antes de realizar análisis.\nstr te permite verificar la estructura de tus datos y asegurarte de que tus variables tengan el tipo correcto (es decir, numérico, caracteres, enteros o factores). Consulta Tipos de datos y estructura para obtener explicaciones sobre estos diferentes tipos.\n\nstr(Snail_feeding)\n## 'data.frame':    768 obs. of  12 variables:\n##  $ Snail.ID: int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ Sex     : chr  \"male\" \"male\" \"males\" \"male\" ...\n##  $ Size    : chr  \"small\" \"small\" \"small\" \"small\" ...\n##  $ Feeding : logi  FALSE FALSE FALSE FALSE FALSE TRUE ...\n##  $ Distance: chr  \"0.17\" \"0.87\" \"0.22\" \"0.13\" ...\n##  $ Depth   : num  1.66 1.26 1.43 1.46 1.21 1.56 1.62 162 1.96 1.93 ...\n##  $ Temp    : int  21 21 18 19 21 21 20 20 19 19 ...\n##  $ X       : logi  NA NA NA NA NA NA ...\n##  $ X.1     : logi  NA NA NA NA NA NA ...\n##  $ X.2     : logi  NA NA NA NA NA NA ...\n##  $ X.3     : logi  NA NA NA NA NA NA ...\n##  $ X.4     : logi  NA NA NA NA NA NA ...\n\nsummary te permite ver estadísticas básicas para cada una de tus variables y se puede utilizar para identificar errores obvios (es decir, máximos o mínimos extremos en relación a la media o mediana, o grupos adicionales dentro de un vector categórico).\n\nsummary(Snail_feeding)\n##     Snail.ID         Sex                Size            Feeding       \n##  Min.   : 1.00   Length:768         Length:768         Mode :logical  \n##  1st Qu.: 4.75   Class :character   Class :character   FALSE:502      \n##  Median : 8.50   Mode  :character   Mode  :character   TRUE :266      \n##  Mean   : 8.50                                                        \n##  3rd Qu.:12.25                                                        \n##  Max.   :16.00                                                        \n##                                                                       \n##    Distance             Depth              Temp          X          \n##  Length:768         Min.   :  1.000   Min.   :18.00   Mode:logical  \n##  Class :character   1st Qu.:  1.260   1st Qu.:19.00   NA's:768      \n##  Mode  :character   Median :  1.510   Median :19.00                 \n##                     Mean   :  1.716   Mean   :19.49                 \n##                     3rd Qu.:  1.760   3rd Qu.:20.00                 \n##                     Max.   :162.000   Max.   :21.00                 \n##                                       NA's   :6                     \n##    X.1            X.2            X.3            X.4         \n##  Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n##  NA's:768       NA's:768       NA's:768       NA's:768      \n##                                                             \n##                                                             \n##                                                             \n##                                                             \n## \n\nTen en cuenta que para el factor de sexo, errores en la entrada de datos resultaron en cinco niveles (female, female s, male, Male y males) cuando debería haber solo dos. Ver a continuación la solución.\n\n\nProblemas comunes al importar datos\nNo obtienes el número de columnas o filas que esperas\nSi ves un montón de columnas adicionales (X, X.1, X.2, X.3, etc.) o filas en tu marco de datos llenas de NA, es probable que se haya ingresado un carácter adicional (probablemente un espacio en blanco o una tabulación) en celdas más allá de tus datos reales en Excel. Este problema se puede evitar durante la entrada de datos eliminando todos los colores/formatos y vaciando columnas/filas excepto las necesarias.\nSi el problema persiste, podemos abordarlo en R mediante la indexación dentro de corchetes cuadrados dataframe[fila, columna]. Por ejemplo, el siguiente código reemplazará el marco de datos en este ejemplo con uno que solo incluye las primeras 7 columnas y elimina las 5 no deseadas (X, X.1, X.2, X.3, etc.).\n\nSnail_feeding &lt;- Snail_feeding[, 1:7]\n\nEl paquete dplyr tiene varias funciones muy útiles para subconjuntar filas y columnas. Consulta Subsetting data para obtener ayuda con esto.\nSi tienes muchas columnas y no quieres contarlas, puedes usar lógica (&, or, ==, &gt;, &lt; , !=) para eliminar las filas y columnas no deseadas. No queremos eliminar todos los valores NA, solo las filas y columnas adicionales que estén completamente llenas de NAs. Estas tienen la propiedad de que el número de NAs es igual al número de filas en una columna dada (o número de columnas en una fila dada).\nHacemos esto seleccionando las columnas de nuestro data frame donde la suma de columnas, colSums, de todos los NAs, &gt;is.na, en nuestro data frame no sea igual, !=, al número de filas, nrow, de nuestro data frame. Puedes usar la misma lógica para hacer esto con las filas también.\n\n# Selects rows that are not entirely NA\nSnail_feeding &lt;- Snail_feeding[, colSums(is.na(Snail_feeding)) != nrow(Snail_feeding)]\n# Select columns that are not entirely NA\nSnail_feeding &lt;- Snail_feeding[rowSums(is.na(Snail_feeding)) != ncol(Snail_feeding), ]\n\nLas columnas no son del tipo de datos que esperas\nCaracteres como factores Cuando se importan marcos de datos en R, los caracteres se convierten automáticamente en factores. Esto tiene mucho sentido para el trabajo estadístico; es más probable que queramos trabajar con factores (definidos anteriormente) que con caracteres. Los factores son fantásticos cuando se hace análisis estadístico y se explora realmente los datos. Sin embargo, antes de eso, cuando se leen, limpian, solucionan problemas, fusionan y manipulan los datos en general, los factores son un dolor de cabeza total.\nPuedes usar el argumento stringsAsFactors=FALSE al importar los datos para evitar la conversión automática a factores o puedes usar as.character para convertir vectores individuales en caracteres.\nConsejo: Utiliza factores lo más tarde posible en el proceso de análisis. Configura stringsAsFactors=FALSE al importar tus datos y, cuando necesites factores en algún lugar dentro de tu script, convierte los datos a un factor.\nFactores como enteros En este marco de datos, el ID de caracol es un entero cuando debería ser un factor o un carácter. Esto se debe a que en la hoja de datos original, el ID de caracol se codificó utilizando números. Este es un problema común y es fácil de solucionar. Simplemente usa as.factor() o as.character(), y luego class() para verificar que nuestra conversión haya funcionado. Recuerda usar $ para acceder al vector desde dentro del marco de datos.\n\nSnail_feeding$Snail &lt;- as.factor(Snail_feeding$Snail)\nclass(Snail_feeding$Snail)\n## [1] \"factor\"\n\nNúmeros como caracteres (factores) Debido a la coerción automática en R, cualquier dígito no numérico dentro de una variable numérica resultará en que toda la variable se convierta en un carácter (R lo importa como factores). En este ejemplo, la variable “Distance” se ha importado como un factor con 768 niveles, cuando en realidad debería ser una variable numérica. Esto indica que hay un error tipográfico en alguna parte del vector de distancias. Estos errores tipográficos son comunes (por ejemplo, usar accidentalmente una coma en lugar de un punto decimal al ingresar decimales), pero un poco más difíciles de resolver.\nCon un conjunto de datos pequeño, lo más rápido es regresar a los datos originales y encontrar el error. Una vez corregido, vuelves a importar y la variable debería ser numérica.\nCon un conjunto de datos más grande, surge el problema de tener que encontrar los errores tipográficos. Desafortunadamente, no podemos convertir directamente nuestros datos de un vector de tipo factor a numérico. Esto se debe al atributo “levels” (consultar Tipos de datos y estructura). Si intentas convertir un factor directamente en una variable numérica, los valores se convierten en un número que corresponde al número de niveles (1:n, ordenados alfabéticamente o de forma ascendente) en lugar del valor real.\nPor lo tanto, primero debemos convertirlo a carácter y luego a numérico. Al convertirlo a numérico, obtendrás un mensaje de advertencia “NAs introduced by coercion”. Esto se debe a que los valores no numéricos (es decir, nuestros errores tipográficos) no se pueden convertir a numéricos y se reemplazan por NA.\nPodemos aprovechar esto, utilizando is.na en combinación con which para identificar dónde están los errores tipográficos o los valores faltantes dentro del vector.\n\nSnail_feeding$Distance &lt;- as.character(Snail_feeding$Distance)\nSnail_feeding$Distance &lt;- as.numeric(Snail_feeding$Distance)\n## Warning: NAs introducidos por coerción\n\nwhich(is.na(Snail_feeding$Distance))\n## [1] 682 755\n\nEsto nos indica que algo extraño sucedió en las filas 682 y 755. Ahora que hemos identificado dónde está el problema, es fácil reemplazar los valores en nuestro marco de datos con los valores correctos (vuelve a tu hoja de datos original). Puedes corregir el error en el conjunto de datos y volver a importarlo, o reemplazar los valores en R utilizando índices dentro de corchetes y asignando el nuevo valor con &lt;-. Usa which(is.na()) para verificar si tu corrección funcionó.\n\nSnail_feeding[682, \"Distance\"] &lt;- 0.356452\nSnail_feeding[755, \"Distance\"] &lt;- 0.42336\n\nwhich(is.na(Snail_feeding$Distance))\n## integer(0)\n\nTienes más niveles de variables de los esperados\nUno de los pasos más importantes en cualquier análisis de datos o tarea de procesamiento es verificar que los valores de tus datos sean correctos. Por ejemplo, se esperaría que una variable llamada “Sexo” tenga solo dos niveles. Sin embargo, en nuestro marco de datos tiene cinco niveles (ver str y summary arriba).\nPuedes verificar los niveles de un factor o los valores únicos de caracteres con levels (solo para factores) o unique (para caracteres y factores).\n\nlevels(Snail_feeding$Sex)\n## NULL\nunique(Snail_feeding$Sex)\n## [1] \"male\"     \"males\"    \"Male\"     \"female\"   \"female s\"\n\nHay varios errores tipográficos que podemos corregir utilizando unique y which, así como los operadores lógicos == (igual) y | (o) para identificar y reemplazar los errores tipográficos.\nPara reemplazar cualquier valor que sea “males” o “Male” con “male”, usaríamos:\n\nSnail_feeding$Sex[which(Snail_feeding$Sex == \"males\" | Snail_feeding$Sex == \"Male\")] &lt;- \"male\"\n\nPara reemplazar cualquier valor que sea “female s” con “female”, usaríamos:\n\nSnail_feeding$Sex[which(Snail_feeding$Sex == \"female s\")] &lt;- \"female\"\n\nVerifica que funcione usando unique, pero también observa lo que sucede cuando verificas los levels (niveles).\n\nunique(Snail_feeding$Sex)\n## [1] \"male\"   \"female\"\nlevels(Snail_feeding$Sex)\n## NULL\n\nCuando usamos unique para verificar nuestras categorías, solo aparecen “male” y “female”, sin embargo, cuando observamos los niveles, todavía tenemos los cinco niveles diferentes, incluyendo los errores tipográficos. Esto se debe al comportamiento de los factores. Una vez que se han definido los niveles, seguirán existiendo independientemente de si están incluidos en alguna muestra. Dado que nuestros niveles adicionales eran errores tipográficos y no niveles reales, debemos eliminarlos de los atributos.\nfactor eliminará los niveles adicionales de un vector.\n\nSnail_feeding$Sex &lt;- factor(Snail_feeding$Sex)\nlevels(Snail_feeding$Sex)\n## [1] \"female\" \"male\"\n\nErrores numéricos\nUsar summary anteriormente es una herramienta útil para verificar posibles errores tipográficos en nuestras variables numéricas. Compara el valor máximo o mínimo con la mediana de cada variable numérica. Si alguno de los valores es una orden de magnitud mayor o menor que la mediana, podría ser un error tipográfico. Por ejemplo, observa la profundidad máxima, parece que se ha olvidado el punto decimal.\nNuevamente, podemos utilizar operadores lógicos e indexación para identificar posibles errores numéricos. Dado que todos los valores de nuestra variable de profundidad parecen estar entre 1 y 2 (según los rangos intercuartiles de summary), buscaremos filas con una profundidad mayor a 2.\n\nSnail_feeding[which(Snail_feeding$Depth &gt; 2), ]\n##   Snail.ID  Sex  Size Feeding Distance Depth Temp Snail\n## 8        1 male small    TRUE      0.6   162   20     1\n\nSolo hay 1 fila. Después de confirmar con nuestros datos originales que esto es, de hecho, un error tipográfico, lo reemplazaremos con el valor real. Seleccionando la fila y la columna, reemplazamos el valor 162 con 1.62.\n\nSnail_feeding[which(Snail_feeding$Depth &gt; 2), 6] &lt;- 1.62\n\n\n\n¿Por qué hacer esto en R?\nPuede que te preguntes por qué molestarse en corregir esto en R. ¿Por qué no ir directamente al archivo .csv y corregir todos los problemas? Por lo general, está bien hacer esto si sabes que el problema es un error tipográfico, etc. Sin embargo, para mantener la integridad de los datos, es importante tener un registro de cada cambio que se haya realizado en tu conjunto de datos original. Al realizar todas las manipulaciones y correcciones en R, también estás guardando un registro de todos los cambios que se están produciendo en tus datos.\nAdemás, es posible que desees comenzar a explorar los datos antes de haber recopilado todo el conjunto de datos. Si configuras un script que verifique todas estas cosas antes de crear gráficos y ejecutar análisis, es rápido volver a ejecutar el script en los datos completos una vez que los tengas todos. El script actúa como un recordatorio de todas las cosas que debes verificar y es mejor para detectar errores tipográficos que tú en una hoja de cálculo enorme.\n\n\nLa verificación final\nFinalmente, una vez que hayas verificado y corregido todos los problemas en tus datos, vuelve a ejecutar str y summary. De hecho, es una buena idea ejecutar estas funciones regularmente durante la limpieza de datos para estar al tanto de cualquier cambio que realices.\n\nstr(Snail_feeding)\n## 'data.frame':    768 obs. of  8 variables:\n##  $ Snail.ID: int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ Sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Size    : chr  \"small\" \"small\" \"small\" \"small\" ...\n##  $ Feeding : logi  FALSE FALSE FALSE FALSE FALSE TRUE ...\n##  $ Distance: num  0.17 0.87 0.22 0.13 0.36 0.84 0.69 0.6 0.85 0.59 ...\n##  $ Depth   : num  1.66 1.26 1.43 1.46 1.21 1.56 1.62 1.62 1.96 1.93 ...\n##  $ Temp    : int  21 21 18 19 21 21 20 20 19 19 ...\n##  $ Snail   : Factor w/ 16 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\nsummary(Snail_feeding)\n##     Snail.ID         Sex          Size            Feeding       \n##  Min.   : 1.00   female:384   Length:768         Mode :logical  \n##  1st Qu.: 4.75   male  :384   Class :character   FALSE:502      \n##  Median : 8.50                Mode  :character   TRUE :266      \n##  Mean   : 8.50                                                  \n##  3rd Qu.:12.25                                                  \n##  Max.   :16.00                                                  \n##                                                                 \n##     Distance          Depth            Temp           Snail    \n##  Min.   :0.0000   Min.   :1.000   Min.   :18.00   1      : 48  \n##  1st Qu.:0.2775   1st Qu.:1.260   1st Qu.:19.00   2      : 48  \n##  Median :0.5100   Median :1.510   Median :19.00   3      : 48  \n##  Mean   :0.5120   Mean   :1.507   Mean   :19.49   4      : 48  \n##  3rd Qu.:0.7500   3rd Qu.:1.760   3rd Qu.:20.00   5      : 48  \n##  Max.   :1.0000   Max.   :2.000   Max.   :21.00   6      : 48  \n##                                   NA's   :6       (Other):480\n\n\n\nMás ayuda\nIntroducción a la limpieza de datos en R por de Jonge y van de Loo\nR-bloggers - errores comunes en la importación de tablas\nAutor: Keryn Bain\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "getting-started-with-r/index.html",
    "href": "getting-started-with-r/index.html",
    "title": "Empezando con R",
    "section": "",
    "text": "A lo largo de este sitio, se proporcionan instrucciones para análisis estadísticos, gestión de datos y técnicas gráficas en el lenguaje de software R. R es un lenguaje gratuito y de código abierto que se ha convertido rápidamente en uso estándar en las ciencias biológicas y ambientales. Aparte del costo, el lenguaje tiene la ventaja de poder compartir explícitamente métodos con colegas y una comunidad muy activa de personas que desarrollan paquetes que ejecutarán muchos tipos de análisis y gráficos.\nAntes de comenzar a hacer gráficos y análisis, deberá:\n\nInstalar R y aprender los conceptos básicos de su uso\n[Pensar en la gestión de proyectos para planificar cómo va a organizar sus archivos.\n\n\nEntrada de datos\n\nDespués de recopilar datos, ser capaz de ingresar esos datos e importarlos en varios paquetes de software son habilidades esenciales para estudiantes e investigadores en las ciencias ambientales. Es posible que pienses que solo debes escribir los números en un paquete de hojas de cálculo y abrir ese archivo en otro software, pero en realidad hay bastante que aprender sobre formas ordenadas de ingresar los datos e importarlos sin errores.\nEn estas páginas, brindamos algunas pautas para la entrada de datos que ahorrarán mucho tiempo cuando se trate de analizar datos y crear gráficos efectivos. También describimos los tipos y la estructura de los objetos de datos en R que verás una vez que hayas importado tus datos.\n\nEntrada de datos - organizando los datos al ingresarlos por primera vez en una hoja de cálculo.\nImportación de datos en R.\nTipos y estructura de datos - mejor comprensión de los objetos de datos en R\n\n\n\n\nAyuda adicional\nLa Guía de Gestión de Datos en Ecología y Evolución de la Sociedad Ecológica Británica\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Mayo 2023"
  },
  {
    "objectID": "getting-started-with-r/installing-r-rstudio/index.html",
    "href": "getting-started-with-r/installing-r-rstudio/index.html",
    "title": "Instalando R y RStudio",
    "section": "",
    "text": "Recomendamos usar RStudio como una interfaz fácil de usar para usar R.\nPrimero, instala la última versión de R - descárgala desde aquí. Segundo, instala la última versión de R Studio - descárgala desde aquí.\n\nEstablecer el directorio de trabajo\nUna vez que hayas instalado R y RStudio, necesitas establecer el directorio de trabajo. Este es el lugar en tu computadora donde se pueden encontrar los archivos de datos que se importarán y donde se guardarán los scripts de R (los archivos que guardan tu código).\nEn RStudio, puedes establecer el directorio de trabajo con los menús (Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory) o con una línea de código que indique la ruta de la carpeta en tu computadora:\n\nsetwd(\"Drive:/Folder1/Folder2\")\n\nSi estás trabajando con cualquiera de los archivos de datos de ejemplo en este sitio, primero deberás descargarlos a una carpeta en tu computadora y especificar esa carpeta como el directorio de trabajo.\n\n\nPaneles de RStudio\n\nRStudio tiene cuatro paneles:\n1.- El panel superior izquierdo es el editor (o ventana de script) donde puedes ver tu script de R. Ejecutar código desde aquí es tan simple como presionar Ctrl+Enter cuando el cursor esté en la línea o líneas de código que deseas ejecutar.\n2.- El panel inferior izquierdo es la consola (o ventana de comandos) donde también puedes ejecutar líneas de código (escribir código al lado de &gt; y presionar Enter), pero también es donde aparecerá cualquier texto o resultado numérico.\n3.- El panel superior derecho es la ventana del espacio de trabajo que muestra los diferentes objetos de R que estás utilizando actualmente. Estos pueden ser conjuntos de datos u objetos creados por varios análisis.\n4.- El panel inferior derecho tiene:\n\nListas de los archivos en tu directorio de trabajo\nLos paquetes de R que estás utilizando actualmente\nCualquier salida gráfica en la ventana de gráficos\nArchivos de ayuda (accedidos mediante ? antes de cualquier fragmento de código)\n\nPuedes cambiar el tamaño de estos paneles arrastrando los bordes de las ventanas.\n\n\nGuardar tu código en un script\nDebes guardar todo el código que uses para un análisis o gráfico determinado. Utiliza los menús en RStudio para crear un nuevo script de R (File &gt;&gt; New &gt;&gt; R script) y guárdalo con el ícono de disco o en el menú (File &gt;&gt; Save). Revisa Gestión de proyectos para obtener consejos sobre cómo estructurar estos archivos.\n\n\nInstalando paquetes de R\nSi tus análisis o gráficos requeridos necesitan un paquete que no está incluido en la instalación inicial de R, puedes instalar nuevos paquetes desde los menús (Tools &gt;&gt; Install packages) o desde el panel en la parte inferior derecha. Una vez instalados, se pueden cargar con la función library (recomendado) o marcando la casilla al lado del nombre del paquete (no recomendado). Es mejor práctica utilizar la función library en tu script, ya que te recordará qué paquetes deben cargarse.\nPor ejemplo, este código cargará el paquete maptools si está instalado en tu computadora.\n\nlibrary(maptools)\n\n\n\nAyuda adicional\nNuestros módulos de ayuda sobre Buenas prácticas para escribir scripts y Importación de datos y limpieza de datos.\nAyuda en línea con R Studio.\nFormación en línea en R.\nAutor: Alistair Poore y Will Cornwel.\nAño: 2016.\nÚltima actualización: Nov. 2023."
  },
  {
    "objectID": "getting-started-with-r/project-management/index.html",
    "href": "getting-started-with-r/project-management/index.html",
    "title": "Gestión de proyectos",
    "section": "",
    "text": "Tener tus archivos de datos, scripts de R y salidas organizados es muy útil para llevar un registro de lo que estás trabajando y para compartir datos o scripts con colegas o supervisores. Trata de evitar tener una carpeta en tu computadora que sea una mezcla de hojas de cálculo, imágenes, documentos de procesamiento de texto, etc. Cómo organices tus archivos es tu elección, pero un poco de planificación y organización de proyectos al principio te dará muchos beneficios a largo plazo. Aquí están nuestras sugerencias para mantener tus proyectos organizados de manera ordenada.\n\nConfigurar una carpeta de proyecto\nCrea una carpeta en tu computadora para cada uno de los proyectos que desarrolles. Esto podría ser un curso que estás realizando actualmente, un capítulo de tu tesis o cualquier tipo de proyecto de investigación. Dentro de esa carpeta, incluye lo siguiente:\n\nUna carpeta de datos (incluyendo metadatos).\nUna carpeta de salidas (con subcarpetas “Figuras” y “Tablas” y posiblemente “Material Suplementario”).\nUn script de R que ejecutará la manipulación de datos, análisis y creará las figuras requeridas para el proyecto (intenta mantener este script de R corto y legible).\nUna carpeta para funciones de R (si se utilizan), generalmente llamada “R”.\nUna carpeta para escritura y referencias llamada “Manuscritos”.\n\nLa carpeta de datos debe contener los datos sin procesar (generalmente ingresados en un programa de hojas de cálculo). Después de terminar de ingresar tus datos, debes intentar mantener estos datos intactos; todas las cosas que desees hacer con los datos, como limpiar los datos y trabajos con subconjuntos, resumir los datos en una tabla, realizar análisis estadísticos o crear figuras, se pueden hacer mediante scripts de R. Esto mantiene un registro de todo lo que haces. Piensa en este/estos script(s) como el cuaderno de laboratorio de un científico de datos. Los resultados intermedios son buenos, sepáralos de los datos sin procesar y guárdalos en la carpeta de salidas.\nUno de los objetivos de esta organización de proyectos es que tu procesamiento y análisis de datos sean completamente reproducibles, y como tal, tanto los datos como los métodos exactos pueden compartirse con colegas y supervisores, quienes luego pueden repetir y, esperamos, ampliar tus análisis. Además, aunque en este momento puedes conocer todos los archivos y pasos, piensa en ti mismo/a dentro de un año cuando hayas pasado a otros trabajos o proyectos; intenta ser amable contigo mismo/a en el futuro y tomar buenas notas. En contraste, si utilizas un programa de apuntar y hacer clic o una hoja de cálculo para realizar pasos importantes en tu análisis, deberás recordar todos los pasos tomados para recrear los análisis o figuras. La mayoría de los análisis en algún momento se vuelven demasiado complicados como para que esto sea viable.\n\n\nUso de los archivos de proyecto de RStudio\nUna forma fácil de acceder a todos estos archivos es utilizar un archivo de proyecto. En RStudio, puedes crear un nuevo archivo de proyecto con File, New Project. A continuación, se te preguntará si deseas crear un nuevo directorio o asociarlo con un directorio existente.\n\nElige esta segunda opción para tener un archivo de proyecto asociado con la carpeta que creaste anteriormente.\nUna vez creado, puedes abrir tu proyecto haciendo clic directamente en ese archivo o desde File, Open si ya tienes RStudio abierto. Una vez abierto, verás la estructura de tu directorio en el panel de archivos en la parte inferior derecha de RStudio y todos tus scripts y datos serán fácilmente accesibles.\n\nUn gran beneficio de trabajar con el enfoque de proyecto es que cuando importas datos, no necesitas especificar el directorio de trabajo con setwd. RStudio buscará dentro de la carpeta que contiene el proyecto. Al usar read.csv(\"\"), presiona la tecla tab repetidamente cuando estés dentro de las \"\" para elegir las carpetas y archivos locales:\n\nread.csv(\"/Data/Survey_data.csv\")\n\nEl archivo de datos se encuentra con una ruta relativa, en lugar de las alternativas de una ruta completa en la función read.csv o establecer el directorio de trabajo y luego usar read.csv.\n\nread.csv(\"C:/Work/Data/Survey_data.csv\")\n\nsetwd(\"C:/Work/Data\")\nread.csv(file = \"Survey_data.csv\")\n\nDe manera similar, cuando exportas un archivo (por ejemplo, figuras o tablas), puedes guardarlos en tu carpeta de salidas (lejos de los datos crudos).\n\nwrite.csv(\"/Outputs/Survey_summary_table.csv\")\n\nAhora puedes simplemente copiar toda la carpeta de tu proyecto a cualquier otra computadora (de escritorio a portátil, de estudiante a supervisor, etc.) y tu código siempre funcionará sin tener que cambiar el directorio de trabajo en cada máquina.\nSi tienes varios archivos de proyectos de R, puedes cambiar fácilmente entre ellos haciendo clic en el nombre del proyecto en la parte superior derecha de RStudio.\n\n\nAyuda adicional\nLa ayuda de RStudio sobre el uso de proyectos Nuestra introducción al control de versiones para realizar un seguimiento de las revisiones de los archivos del proyecto.\nAutor: Alistair Poore y Will Cornwell\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "getting-started-with-r/rstudio-notebooks/index.html",
    "href": "getting-started-with-r/rstudio-notebooks/index.html",
    "title": "Utilizando R Notebooks",
    "section": "",
    "text": "Si estás trabajando en el laboratorio o en el campo, es esencial tomar buenas notas sobre lo que estás haciendo para poder recordar tus métodos y asegurarte de que tu trabajo sea reproducible.\nDe manera similar, una vez que estés trabajando con los datos que has recopilado, es fundamental tener buenas notas sobre cómo has procesado esos datos. Te ayudarán a comprender exactamente lo que estás haciendo y serán necesarias para escribir secciones de métodos y comunicar tanto los métodos como los resultados a colaboradores o supervisores.\nEn un script tradicional de R, todo el texto es código y los comentarios se separan del código ejecutable comenzando una línea con #. Por ejemplo,\nplot(y ~ x)\nEsto puede volverse confuso si tienes muchos comentarios, por lo que recomendamos utilizar notebooks de RStudio como una forma muy limpia de escribir tu código con los comentarios correspondientes.\nLos notebooks de RStudio son una forma de documento markdown, un formato ampliamente utilizado para exportar texto y código a varios formatos de informe. Todas las páginas web de este sitio comienzan su vida como archivos markdown con todo el texto y código necesario para generar el archivo html. Los notebooks de RStudio tienen la ventaja adicional de mostrar la salida de tu código directamente en el documento."
  },
  {
    "objectID": "getting-started-with-r/rstudio-notebooks/index.html#control-del-comportamiento-de-los-bloqueschunk-de-código",
    "href": "getting-started-with-r/rstudio-notebooks/index.html#control-del-comportamiento-de-los-bloqueschunk-de-código",
    "title": "Utilizando R Notebooks",
    "section": "Control del comportamiento de los bloques(chunk) de código",
    "text": "Control del comportamiento de los bloques(chunk) de código\nPuedes controlar cómo se muestra el código y su resultado en el documento exportado con una serie de opciones para cada bloque. Hay muchas de estas opciones (detalladas aquí), pero algunas útiles son:\nSi quieres mostrar el código pero no ejecutarlo, utiliza eval=FALSE dentro de {r} al comienzo de tu bloque. Elige ejecutar el código en la salida o no.\n\n# ```{r, eval=FALSE}\n2 + 2\n\nPara ejecutar el código pero ocultar el código en la salida:\n\n# ```{r, echo=FALSE}\n2 + 2\n\nPara ocultar todos los mensajes y advertencias:\n\n# ```{r, warning=FALSE, message=FALSE}\n2 + 2\n\n[1] 4\n\n\nPara cambiar el tamaño de cualquier gráfico:\n\n# ```{r, fig.height=3, fig.width=10}\nggplot(Plant_height, aes(temp, loght)) +\n  geom_point()\n\n\n\n\nEstas opciones también se pueden establecer una vez al comienzo del documento en lugar de individualmente para cada bloque de código (llamadas opciones globales). Por ejemplo, para establecer el tamaño de todas las figuras, usaríamos:\n\nMás ayuda\nRStudio’s introduction to notebooks\nHoja de referencia de R Markdown de RStudio\nPor qué me encantan los cuadernos de R por Nathan Stephen\nR Notebooks por Jonathan McPherson\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/basic-plotting/index.html",
    "href": "graphics/basic-plotting/index.html",
    "title": "Gráficos básicos en R",
    "section": "",
    "text": "R cuenta con una amplia gama de funciones y paquetes para visualizar datos. Aquí tienes ayuda para realizar algunos gráficos muy simples utilizando las funciones básicas en R para datos con:\n\nuna variable continua - histogramas y diagramas de caja\ndos variables continuas - gráficos de dispersión\nuna variable continua vs variables categóricas - diagramas de caja y gráficos de barras"
  },
  {
    "objectID": "graphics/basic-plotting/one-continuous/index.html",
    "href": "graphics/basic-plotting/one-continuous/index.html",
    "title": "Una Variable Continua",
    "section": "",
    "text": "Poder visualizar las propiedades de los datos es un paso crítico en la exploración de datos y necesario para la comunicación efectiva de resultados. En esta página se detallan dos formas comunes en las que se pueden mostrar los datos de una sola variable continua en R: histogramas de frecuencia y diagramas de caja - bigotes.\nLos datos de muestra utilizados a continuación son las longitudes (cm) de 80 peces capturados en un estuario en sitios con diferentes zonas de gestión (protegidas versus no protegidas) y costas (urbanizadas versus saludables).\n\n\nHistogramas de frecuencia\nLos histogramas de frecuencia representan con qué frecuencia caen los valores de una variable continua en ciertos rangos. Son una forma efectiva de visualizar el rango de valores obtenidos y la distribución de tus datos (es decir, ¿es simétrica o asimétrica?).\nEn primer lugar, descarga el conjunto de datos de muestra, Estuary_fish.csv, e impórtalo a R.\n\nFish &lt;- read.csv(file = \"Estuary_fish.csv\")\n\nUn histograma de frecuencia de la variable “Length” del data frame “Fish” se puede producir fácilmente con la función hist.\n\nhist(Fish$Length)\n\n\n\n\nDe inmediato puedes ver que todos los peces tenían menos de 30 cm de longitud y que estos datos de longitud tienen una asimetría positiva: los peces pequeños son más frecuentes y los peces grandes son raros.\nPuedes modificar el número de intervalos o el rango de cada intervalo con el argumento breaks. Esto cambiará el ancho y la forma del histograma. Por ejemplo, si deseas 15 intervalos, usarías:\n\nhist(Fish$Length, breaks = 15)\n\n\n\n\nSi deseas que cada intervalo tenga un rango de 2 cm, puedes usar breaks=seq(0,30,by=2), donde los números entre corchetes son el mínimo, el máximo y el rango para cada intervalo.\n\n\nGráficos de caja y bigotes\nUn gráfico de caja y bigotes, también conocido como boxplot, es otra forma útil de visualizar la distribución de una variable continua. Se pueden crear fácilmente con la función boxplot. El argumento horizontal=TRUE hace que el eje único sea horizontal.\n\nboxplot(Fish$Length, horizontal = TRUE)\n\n\n\n\nEl gráfico muestra la distribución de una variable indicando la mediana, los cuartiles, el máximo y el mínimo de la variable. Los bigotes superior e inferior representan los valores máximo y mínimo (excluyendo cualquier valor atípico que se indica con un círculo). La línea gruesa en negro es la mediana, y las cajas a cada lado de la línea de la mediana son los cuartiles inferior y superior.\nRecuerda que la mediana es el valor que tiene el 50% de los valores mayores y el 50% de los valores menores. De manera similar, los cuartiles representan los valores con el 25% de los valores menores (el cuartil inferior) o el 25% de los valores mayores (el cuartil superior).\nEl gráfico de caja y bigotes indicará la asimetría en tus datos si la mediana no está equidistante de los cuartiles o de los valores máximo y mínimo. En este ejemplo, puedes ver que la mediana está más cerca del valor mínimo que del máximo (lo que indica que los valores bajos son más comunes).\n\n\nFormateo de gráficos\nEstos gráficos sencillos se pueden formatear utilizando las funciones básicas de formateo de R en el paquete de gráficos. El código a continuación muestra algunos de los comandos de formateo más comúnmente utilizados para tener un mayor control sobre tus gráficos.\nAgregar etiquetas de ejes o títulos\nLas etiquetas de los ejes se crean con los argumentos xlab y ylab. Los títulos se proporcionan con el argumento main.\n\nhist(Fish$Length, xlab = \"Fish length (cm)\", main = \"Frequency histogram of fish length\")\n\n\n\nboxplot(Fish$Length, xlab = \"Fish length (cm)\", main = \"Box plot of fish length\", horizontal = TRUE)\n\n\n\n\nUsando el argumento main=NULL, se elimina el título, lo cual suele ser innecesario, ya que los detalles sobre lo que ilustra un gráfico suelen estar escritos en una leyenda debajo del gráfico.\nEditar límites de los ejes\nLos límites de los ejes se establecen mediante los argumentos xlim y ylim, donde se requiere un vector con los límites mínimo y máximo. Por ejemplo, para que cada uno de estos gráficos muestre los datos entre 0 y 40, se usaría:\n\nhist(Fish$Length, xlab = \"Fish length (cm)\", main = \"Frequency histogram of fish length\", xlim = c(0, 40))\n\nboxplot(Fish$Length, xlab = \"Fish length (cm)\", main = \"Box plot of fish length\", ylim = c(0, 40), horizontal = TRUE)\n\nTen en cuenta que ylim es necesario allí para establecer el rango de la variable de respuesta única (longitud del pez), aunque al final acabe en el eje horizontal después de usar el argumento horizontal=TRUE.\nAlineación de los gráficos\nUtiliza el argumento horizontal=TRUE para alinear el diagrama de caja horizontalmente; omítelo si deseas una alineación vertical.\nAñadir color\nSe puede agregar color a cualquier parte de los gráficos (ejes, fuentes, etc.) utilizando el argumento col. Hay más de 600 colores que se pueden representar en el gráfico; escribe colors() para ver los nombres de todo el rango.\nAquí simplemente cambiaremos el color del histograma.\n\nhist(Fish$Length, xlab = \"Fish length (cm)\", main = \"Frequency histogram of fish length\", col = \"red\")\n\n\n\n\n\n\nMás ayuda\nEscribe ?hist y ?boxplot para obtener ayuda sobre estas funciones en R.\nAutor: Stephanie Brodie\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/basic-plotting/one-continuous-one-factor/index.html",
    "href": "graphics/basic-plotting/one-continuous-one-factor/index.html",
    "title": "Una Variable Continua y una Variable Categórica",
    "section": "",
    "text": "Visualizar cómo una variable medida se relaciona con otras variables de interés es esencial para la exploración de datos y la comunicación de los resultados de la investigación científica. Esta página detalla cómo trazar una única variable continua frente a niveles de una variable predictora categórica.\nEste tipo de gráficos se utiliza comúnmente en las ciencias biológicas, de la Tierra y del medio ambiente. Por ejemplo, para ver cómo una variable dada difiere entre un tratamiento experimental y un control, o entre sitios y tiempos de muestreo en el muestreo ambiental.\n\nUtilizaremos datos de muestra de un experimento que contrastó la tasa metabólica de dos especies de camarones e introduciremos dos tipos de gráficos comúnmente utilizados para este propósito: diagramas de caja y diagramas de barras.\nEn primer lugar, descarga el archivo de datos de muestra, Prawns_MR.csv, e impórtalo en R.\n\nPrawns &lt;- read.csv(file = \"Prawns_MR.csv\")\n\n\nDiagramas de Caja\nLos diagramas de caja se realizan fácilmente con la función boxplot. Los diagramas de caja muestran la distribución de una variable indicando la mediana, los cuartiles, el máximo y el mínimo de una variable. Los bigotes superior e inferior son los valores máximo y mínimo (excluyendo los valores atípicos que se indican con un círculo). La línea gruesa negra es la mediana, y las cajas a cada lado de la línea de mediana son los cuartiles inferiores y superiores.\nPara contrastar la tasa metabólica entre las dos especies, utilizaríamos:\n\nboxplot(Metabolic_rate ~ Species, data = Prawns)\n\n\n\n\nLa variable continua se encuentra a la izquierda del símbolo de tilde (~) y la variable categórica se encuentra a la derecha. De inmediato, puedes observar que la especie B tiene una tasa metabólica más alta que la especie A.\n\n\nGráficos de barras\nEste tipo de datos también se visualiza comúnmente mediante un gráfico de barras que muestra las medias de la variable continua para cada nivel de la variable categórica, con barras de error que muestran algunas medidas de variación dentro de cada categoría. Las barras de error pueden ser la desviación estándar, el error estándar o los intervalos de confianza del 95%.\nAunque se utilizan comúnmente, no son tan fáciles de crear con las funciones básicas de R. Hay varias formas de hacerlo, pero una opción es utilizar las funciones summarise y group_by del paquete dplyr para calcular las medias y medidas de variación para cada nivel de tu variable categórica (consulta Resumen de datos).\nAquí tienes un código de ejemplo para visualizar las medias +/- desviaciones estándar. Para tener un mayor control sobre las barras de error, te recomendamos utilizar opciones de trazado más avanzadas en el paquete ggplot2 (consulta gráficos de barras con barras de error).\n\nlibrary(dplyr)\n\nSpecies.summary &lt;- Prawns %&gt;% # the names of the new data frame and the data frame to be summarised\n  group_by(Species) %&gt;% # the grouping variable\n  summarise(\n    mean = mean(Metabolic_rate), # calculates the mean\n    sd = sd(Metabolic_rate), # calculates the standard deviation\n    lower = mean(Metabolic_rate) - sd(Metabolic_rate),\n    upper = mean(Metabolic_rate) + sd(Metabolic_rate)\n  )\n\nEn un nuevo data frame llamado Species.summary, ahora tenemos las medias, desviaciones estándar y los valores inferiores y superiores que establecen el tamaño de las barras de error para cada nivel de la variable de agrupación. Los límites de las barras de error se calcularon sumando (superior) o restando (inferior) la desviación estándar a la media.\nAhora podemos utilizar las funciones barplot y arrows para crear un gráfico con barras de error.\n\nPrawn.plot &lt;- barplot(Species.summary$mean,\n  names.arg = Species.summary$Species,\n  ylab = \"Metabolic rate\", xlab = \"Species\", ylim = c(0, 1)\n)\n\narrows(Prawn.plot, Species.summary$lower, Prawn.plot, Species.summary$upper, angle = 90, code = 3)\n\n\n\n\nTen en cuenta que los gráficos de medias y barras de error pueden ser engañosos, ya que ocultan las distribuciones reales de los datos. Las medias también pueden ser engañosas cuando los datos están muy sesgados y los cálculos para las barras de error utilizando estadísticas t (por ejemplo, intervalos de confianza del 95%) asumen que los datos están distribuidos de manera normal.\n\n\nFormateo de gráficos\nLos diagramas de caja (box plots) y los gráficos de barras se pueden formatear utilizando el formato básico de R en el paquete de gráficos base. El siguiente código detalla algunos de los comandos de formato más utilizados para estos gráficos. Estos comandos se pueden utilizar para cualquier función de visualización en el paquete de graphics.\nAgregar etiquetas de ejes o títulos\nLas etiquetas de los ejes se agregan con los argumentos xlab y ylab. Los títulos se proporcionan con el argumento main. Ten en cuenta que las figuras en publicaciones científicas rara vez tienen un título, pero incluyen información sobre el gráfico en una leyenda:\n\nboxplot(Metabolic_rate ~ Species, data = Prawns, xlab = \"Species\", ylab = \"Metabolic rate\")\n\nEditar límites de los ejes\nLos límites de los ejes se establecen mediante los argumentos xlim y ylim, donde se requiere un vector con los límites mínimos y máximos. Por ejemplo, para establecer el eje Y con un mínimo de cero y un máximo de 1, utiliza:\n\nboxplot(Metabolic_rate ~ Species,\n  data = Prawns, xlab = \"Species\", ylab = \"Metabolic rate\",\n  ylim = c(0, 1)\n)\n\nRenombrar niveles del factor categórico\nSi los niveles de una variable categórico no son ideales para el gráfico, se puede modificar los nombres utilizando el argumento names. Por ejemplo, para poner los nombres reales de las especies:\n\nboxplot(Metabolic_rate ~ Species,\n  data = Prawns, xlab = \"Species\", ylab = \"Metabolic rate\",\n  names = c(\"Penaeus monodon\", \"Fenneropenaeus merguiensis\")\n)\n\n\n\n\nAgregar color\nSe puede agregar color a cualquier parte del gráfico (ejes, fuentes, etc.) utilizando el argumento col. Hay más de 600 colores disponibles, puedes escribir colours() para ver toda la lista de colores disponibles.\nAquí simplemente cambiaremos el color de las barras en el gráfico de barras a rojo.\n\nbarplot(Species.summary$mean,\n  names.arg = Species.summary$Species,\n  ylab = \"Metabolic rate\", xlab = \"Species\", ylim = c(0, 1), col = \"red\"\n)\n\n\n\n\n\n\nMás ayuda\nEscribe ?boxplot y ?barplot para obtener la ayuda de R sobre estas funciones.\nAutores: Stephanie Brodie y Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/basic-plotting/two-continuous/index.html",
    "href": "graphics/basic-plotting/two-continuous/index.html",
    "title": "Dos Variables Continuas",
    "section": "",
    "text": "Visualizar la relación entre dos variables continuas es una de las técnicas gráficas más comúnmente utilizadas en las ciencias. Esta página detalla cómo producir diagramas de dispersión simples para mostrar cómo una variable continua está relacionada con otra.\n\nPara este ejemplo práctico, descarga un conjunto de datos sobre las alturas de las plantas alrededor del mundo, Plant_height.csv, e impórtalo en R.\n\nPlant_height &lt;- read.csv(file = \"Plant_height.csv\", header = TRUE)\n\n\nDiagramas de dispersión\nPara visualizar cómo varía la altura de las plantas con relación a la temperatura, podemos usar un diagrama de dispersión simple, utilizando la función plot. Coloca la variable Y a la izquierda del símbolo tilde (~) y la variable X a la derecha. El argumento data=Plant_height le indica a R que busque esas dos variables en el data frame Plant_height.\n\nplot(height ~ temp, data = Plant_height)\n\n\n\n\nSi las dos variables que deseas visualizar están en diferentes objetos en R, simplemente usarías:\n\nplot(y ~ x)\n\ndonde y y X son dos vectores de igual longitud.\n\n\nFormateo de gráficos\nLos gráficos de dispersión se pueden construir utilizando el formato básico de R en el paquete “graphics”. El código a continuación detalla algunos de los comandos de formato más comúnmente utilizados para gráficos de dispersión simples. Estos comandos se pueden utilizar para cualquier función de visualización en el paquete “graphics”.\nAgregar etiquetas de ejes o títulos\nLas etiquetas de ejes se generan con los argumentos xlab e ylab. Los títulos se proporcionan con el argumento main. Ten en cuenta que las figuras en publicaciones científicas rara vez tienen un título, pero incluyen información sobre el gráfico en una leyenda de la figura presentada debajo del gráfico.\n\nplot(height ~ temp, data = Plant_height, xlab = \"Temperature (°C)\", ylab = \"Plant height (m)\", main = \"Plant height vs temperature\")\n\n\n\n\nEditar límites de ejes\nLos límites de los ejes se establecen mediante los argumentos xlim e ylim, donde se requiere un vector de los límites mínimo y máximo. Por ejemplo, para establecer el eje Y con un mínimo de cero y un máximo de 80 m, y el eje X con un rango entre -20 y 30, utiliza:\n\nplot(height ~ temp, data = Plant_height, xlab = \"Temperature (°C)\", ylab = \"Plant height (m)\", ylim = c(0, 80), xlim = c(-20, 30))\n\nEstilo de símbolos\nLa elección de los símbolos a utilizar en la visualización es amplia y las opciones se acceden mediante el argumento pch en los parámetros gráficos. Escribe ?pch para ver todas las opciones.\n\nLos círculos sólidos pch=16 o pch=19 son a menudo la forma más clara de mostrar datos en un gráfico de dispersión.\n\nplot(height ~ temp, data = Plant_height, xlab = \"Temperature (°C)\", ylab = \"Plant height (m)\", ylim = c(0, 80), pch = 16)\n\n\n\n\nAgregar color\nEl color se puede agregar a cualquier parte de los gráficos (ejes, fuentes, etc.) utilizando el argumento col. Hay más de 600 colores que se pueden representar, escribe colours() para ver todo el rango.\nAquí simplemente cambiaremos el color de los símbolos a azul.\n\nplot(height ~ temp, data = Plant_height, xlab = \"Temperature (°C)\", ylab = \"Plant height (m)\", pch = 16, col = \"blue\")\n\n\n\n\nAgregando una línea de mejor ajuste\nPara explorar aún más la relación entre dos variables, puedes agregar una línea ajuste. Por ejemplo, para agregar la línea de ajuste de una regresión lineal simple, usaríamos la función de modelado lineal, lm, para obtener la pendiente e intersección, y agregar esta línea al diagrama de dispersión a través del parámetro gráfico abline.\nConsulta la página sobre regresión lineal para el análisis de altura de árboles versus temperatura en este conjunto de datos. La variable dependiente analizada fue la transformación logarítmica de la altura de los árboles (loght). Para graficar esto en función de la temperatura con la línea de ajuste del modelo lineal, usaríamos:\n\nplot(loght ~ temp, data = Plant_height, xlab = \"Temperature (°C)\", ylab = \"log(Plant height)\", pch = 16)\nabline(lm(loght ~ temp, data = Plant_height))\n\n\n\n\n\n\nMás ayuda\nEscribe ?plot y ?abline para obtener la ayuda de R sobre estas funciones.\nAutores: Stephanie Brodie y Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/ggplot/ggplot-appearance/index.html",
    "href": "graphics/ggplot/ggplot-appearance/index.html",
    "title": "Personalización de un gráfico en ggplot",
    "section": "",
    "text": "Los resultados de ggplot son casi completamente personalizables. Esto brinda al usuario la libertad de crear un diseño de gráfico que se ajuste perfectamente a los requerimientos para informes, ensayos o artículos.\nDe forma general se puede editar las caracteristicas, modificando el estilo, las líneas de cuadrícula, marcas de los ejes, color del panel, color de la leyenda o contornos.\nAntes de comenzar, lee la página sobre Conceptos básicos para la construcción de gráficos con ggplot e instala el paquete ggplot2.\n\nlibrary(ggplot2)\n\n\nPersonalización de un grafico ggplot2\nUna de las formas más efectivas de personalizar un gráfico es crear un tema personalizado y separarlo del código básico de ggplot. Al separar y guardar el tema personalizado, será mucho más fácil editarlo, modificarlo y reutilizarlo para otros proyectos.\n\nEn estos ejemplos, vamos a utilizar un conjunto de datos disponibles en R, el cual incluye la longitud y el ancho de partes florales para tres especies de iris. Primero, carga el conjunto de datos:\n\ndata(iris)\n\nEl siguiente código genera un gráfico de dispersión de la longitud del pétalo vs longitud del sépalo, con las tres especies codificadas por color. Esta es la base que utilizaremos en todo este tutorial.\n\nIrisPlot &lt;- ggplot2::ggplot(iris, \n                   ggplot2::aes(Sepal.Length, Petal.Length,\n                       colour = Species)) +\n  ggplot2::geom_point()\n\n\n\n\n\n\nPara cambiar el diseño de un elemento en un gráfico: por ejemplo, el código para cambiar el color y el tamaño de la leyenda en el gráfico anterior se vería así:\n\nmytheme &lt;-\n  theme(legend.title = element_text(colour = \"steelblue\", size = rel(2)))\n\nLuego volveríamos a imprimir el gráfico base con este tema agregado:\n\nprint(IrisPlot + mytheme)\n\n\n\n\nLos temas pueden tener muchos elementos diferentes relacionados con leyendas, ejes, títulos, etc. Separa cada elemento dentro del código theme con una coma.\n\n\nFondo del gráfico y de la leyenda\nEl fondo del gráfico y de la leyenda se puede cambiar a cualquier color de los enumerados aquí utilizando el siguiente código:\n\npanel.background = element_rect() modificará del panel detrás del gráfico.\nlegend.key = element_rect() modificará las cajas junto a cada nombre de categoría.\nlegend.background = element_rect() modificará la caja alrededor de los nombres y cajas de la leyenda.\n\nPara cada uno de estos elementos, los formatos que puedes modificar son:\n\nfill = \"color\" para modificar el dolor del panel detrás del gráfico.\ncolour = \"color\" modificará el color del contorno de los ejes del gráfico.\nsize = 6 modificará el grosor de las lineas de contorno.\n\nPara demostrar cómo funciona, aquí hay un tema con los colores elegidos para mostrar las diferencias:\n\nmytheme &lt;-\n  theme(\n    panel.background = element_rect(\n      fill = \"black\",\n      colour = \"yellow\",\n      size = 4\n    ),\n    legend.key = element_rect(fill = \"darkgrey\", colour = \"yellow\"),\n    legend.background = (element_rect(colour = \"yellow\", fill = \"blue\"))\n  )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nprint(IrisPlot + mytheme)\n\n\n\n\n\n\nTamaño, color o presencia de las líneas de la cuadrícula\nLas líneas de cuadrícula se dividen en dos conjuntos: las líneas de cuadrícula mayores y menores. Esto te permite acentuar o eliminar la mitad de las líneas de cuadrícula.\n\npanel.grid.major = element_line() - modifica el formato de las líneas de cuadrícula mayores.\npanel.grid.minor = element_line() - modifica el formato de las líneas de cuadrícula menores.\n\ndonde element_line() se usa para especificar el color y el grosor de las líneas.\nLos formatos incluyen:\n*Color: Por ejemplo, colour = \"steelblue2\" - recuerda incluir “” antes y después del nombre del color.\n\nTamaño: Se especifica ingresando un número, por ejemplo, size = 3.\n\nAl combinar todo esto en un tema, se vería así:\n\nmytheme &lt;-\n  theme(\n    panel.grid.major = element_line(colour = \"black\", size = (1.5)),\n    panel.grid.minor = element_line(\n      size = (0.2), colour =\n        \"grey\"\n    )\n  )\n\nprint(IrisPlot + mytheme)\n\n\n\n\nPara remover las líneas de cuadrícula, puedes usar element_blank()\n\nmytheme &lt;- theme(panel.grid.minor = element_blank(), panel.grid.major = element_blank())\n\nprint(IrisPlot + mytheme)\n\n\n\n\n\n\nLíneas de los ejes x e y\nEl tamaño y el color de las líneas de los ejes se pueden personalizar de la misma manera que las líneas de la cuadrícula (arriba), utilizando axis.line = element_line(). Por ejemplo:\n\nmytheme &lt;-\n  theme(\n    axis.line = element_line(size = 1.5, colour = \"red\"),\n    panel.background = element_rect(fill = \"white\")\n  )\n\nprint(IrisPlot + mytheme)\n\n\n\n\n\n\nMarcas de los ejes\nEl tamaño y el color de las marcas de los ejes se pueden modificar utilizando axis.ticks = element_line( ). Por ejemplo:\n\nmytheme &lt;- theme(axis.ticks = element_line(colour = \"red\", size = (2)))\n\nprint(IrisPlot + mytheme)\n\n\n\n\n\n\nEliminación de cualquier elemento de un tema\nSe utiliza la función = element_blank() si deseas eliminar completamente un elemento del tema seleccionado. Por ejemplo, para eliminar las marcas, las líneas de la cuadrícula y el fondo, utilizarías:\n\nmytheme &lt;-\n  theme(\n    axis.ticks = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.background = element_blank()\n  )\n\nprint(IrisPlot + mytheme)\n\n\n\n\n\n\nEjemplo de gráfico completamente personalizado\nAquí tienes un ejemplo de un gráfico generado con ggplot2 con muchos elementos personalizados. Obviamente, es mucho código, pero recuerda que puedes guardar y reutilizar tu tema favorito tantas veces como quieras. Consulta la página sobre Agregar títulos y nombres de ejes para obtener ayuda con esas partes del código print().\n\nIrisPlot &lt;-\n  ggplot(\n    iris,\n    aes(\n      Sepal.Length,\n      Petal.Length,\n      colour = Species,\n      shape = Species\n    )\n  ) +\n  geom_point()\n\nmytheme3 &lt;-\n  theme(\n    legend.text = element_text(\n      face = \"italic\",\n      colour = \"steelblue4\",\n      family = \"Helvetica\",\n      size = rel(1)\n    ),\n    axis.title = element_text(\n      colour = \"steelblue4\",\n      family = \"Helvetica\",\n      size = rel(1.5)\n    ),\n    axis.text = element_text(\n      family = \"Helvetica\",\n      colour = \"steelblue1\",\n      size = rel(1.5)\n    ),\n    axis.line = element_line(size = 1, colour = \"black\"),\n    axis.ticks = element_line(colour = \"grey\", size = rel(1.4)),\n    panel.grid.major = element_line(colour = \"grey\", size = rel(0.5)),\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"whitesmoke\"),\n    legend.key = element_rect(fill = \"whitesmoke\"),\n    legend.title = element_text(\n      colour = \"steelblue\",\n      size = rel(1.5),\n      family = \"Helvetica\"\n    ),\n    plot.title = element_text(\n      colour = \"steelblue4\",\n      face = \"bold\",\n      size = rel(1.7),\n      family = \"Helvetica\"\n    )\n  )\n\nprint(\n  IrisPlot + mytheme3 + ggtitle(\"Iris species petal and sepal length\")\n    + labs(y = \"Petal length (cm)\", x = \"Sepal length (cm)\", colour = \"Species\")\n    + scale_colour_manual(values = c(\"slateblue\", \"slateblue2\", \"slateblue4\"))\n)\n\n\n\n\n\n\nMás ayuda\nPara personalizar aún más la estética de un gráfico, incluyendo el color y el formato, consulta nuestras otras páginas de ayuda de ggplot:\n\nAgregar títulos y nombres a los ejes\nColores y símbolos.\n\nPuedes encontrar ayuda sobre todas las funciones de ggplot en el sitio web de ggplot.\nPuedes descargar una hoja de referencia útil sobre las funciones más utilizadas aquí.\n\nChang, W (2012) R Graphics cookbook. O’Reilly Media. - una guía sobre ggplot con bastante ayuda en línea aquí\n\nAutor: Fiona Robinson\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/ggplot/ggplot-barplot/index.html",
    "href": "graphics/ggplot/ggplot-barplot/index.html",
    "title": "Gráfico de barras con barras de error",
    "section": "",
    "text": "Los gráficos de barras con barras de error se utilizan con mucha frecuencia en las ciencias ambientales para representar la variación en una variable continua dentro de una o más variables categóricas. No siempre es sencillo crear estos gráficos con las funciones base de R. Esta página te muestra cómo hacer estos gráficos con el paquete ggplot2.\nAntes de empezar, lee la página sobre los conceptos básicos de visualización con ggplot e instala el paquete ggplot2.\n\nlibrary(ggplot2)\n\n\nEn estos ejemplos, utilizaremos un conjunto de datos que ya está en R con la longitud y el ancho de partes florales de tres especies de iris. Primero, carga el conjunto de datos:\n\ndata(iris)\n\nPara contrastar una variable entre especies, primero necesitamos resumir los datos para obtener las medias y una medida de variación para cada una de las tres especies en el conjunto de datos. Hay varias formas de hacer esto en R, pero nos gusta utilizar las funciones summarise y group_by del paquete dplyr. Consulta aquí para obtener más detalles sobre cómo usar dplyr para resumir datos.\nEl siguiente código creará un nuevo marco de datos con los datos resumidos por especie.\n\nlibrary(dplyr)\n\nIris_summary &lt;- iris %&gt;% # the names of the new data frame and the data frame to be summarised\n  group_by(Species) %&gt;% # the grouping variable\n  summarise(\n    mean_PL = mean(Petal.Length), # calculates the mean of each group\n    sd_PL = sd(Petal.Length), # calculates the standard deviation of each group\n    n_PL = n(), # calculates the sample size per group\n    SE_PL = sd(Petal.Length) / sqrt(n())\n  ) # calculates the standard error of each group\n\nAhora podemos hacer un gráfico de barras de medias vs. especies, con desviaciones estándar o errores estándar como barras de error. El siguiente código utiliza las desviaciones estándar.\n\nIrisPlot &lt;- ggplot(Iris_summary, aes(Species, mean_PL)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean_PL - sd_PL, ymax = mean_PL + sd_PL), width = 0.2)\n\nIrisPlot + labs(y = \"Petal length (cm) +/- s.d.\", x = \"Species\") + theme_classic()\n\n\n\n\ngeom_col utiliza el valor de la variable y (mean_PL) como altura de las barras.\nEn el código de geom_errorbar, ymin y ymax son la parte superior e inferior de las barras de error (definidas aquí como media +/- sd), y width define el ancho de las barras de error.\n\nMás ayuda\nPara personalizar aún más la estética del gráfico, incluyendo el color y el formato, consulta nuestras otras páginas de ayuda de ggplot: * Alterar la apariencia general * Añadir títulos y nombres de ejes * Colores y símbolos\nPuedes encontrar ayuda sobre todas las funciones de ggplot en el sitio de ayuda maestro de ggplot.\nPuedes descargar una hoja de referencia útil sobre funciones comúnmente utilizadas aquí.\n\nChang, W (2012) R Graphics cookbook. O’Reilly Media. - una guía de gg"
  },
  {
    "objectID": "graphics/ggplot/ggplot-basics/index.html",
    "href": "graphics/ggplot/ggplot-basics/index.html",
    "title": "Fundamentos de ggplot2",
    "section": "",
    "text": "Creación de un grafico con ggplot\nPrimero, necesitarás instalar el paquete ggplot2 en tu máquina y luego cargar el paquete con la función library como de costumbre.\n\nlibrary(ggplot2)\n\nEl punto de partida para crear un gráfico es utilizar la función ggplot con la siguiente estructura básica:\n\nplot1 &lt;- ggplot(data, aes(x_variable, y_variable)) +\n  geom_graph.type()\n\nprint(plot1)\n\ndata es un data frame que contiene las variables que deseas representar en el gráfico.\naes especifica qué variables se deben representar. Este código suele causar confusión al crear un objeto ggplot. Aunque aes significa estética, en ggplot no se refiere al aspecto visual del gráfico, sino a los datos que deseas ver en el gráfico. Especifica qué se presenta en el gráfico en lugar de cómo se presenta.\n+ geom_ especifica qué tipo de gráfico quieres crear, ggplot no funcionará a menos que agregues esto.\nAlgunos ejemplos comúnmente utilizados incluyen: * scatterplot (gráfico de dispersión) - + geom_point() * boxplot (gráfico de caja) - + geom_boxplot() * histograma - + geom_histogram() * gráfico de barras - + geom_bar()\nAsegúrate de tener un paréntesis abierto y cerrado después del código geom_. Esto le indica a R que haga el gráfico con el formato estándar básico para este tipo de gráfico.\nEl tipo de gráfico que deseas hacer debe coincidir con las clases de las variables de entrada. Por ejemplo, un scatterplot requeriría que ambas variables sean numéricas. O un boxplot requeriría que la variable x sea un factor y la variable y sea numérica. Puedes verificar la clase de cualquier variable con la función class, o todas las variables en un data frame con la función str.\nUna práctica útil al crear objeto con ggplot es asignar el gráfico que has creado a un objeto (por ejemplo, plot1 en el código anterior) y luego imprimirlo por separado. A medida que tu ggplot se vuelve más complicado, esto facilitará mucho las cosas.\n\n\nEjemplos de gráficos\n\nEn estos ejemplos, utilizaremos un conjunto de datos que ya está en R con la longitud y el ancho de partes florales de tres especies de iris. Primero, carga el conjunto de datos:\n\ndata(iris)\n\nPara contrastar las longitudes del pétalo entre las tres especies de iris con un diagrama de caja, usaríamos:\n\nplot1 &lt;- ggplot(iris, aes(Species, Petal.Length)) +\n  geom_boxplot()\nprint(plot1)\n\n\n\n\nObserva cómo aes incluye las variables x y luego y que deseas graficar, y cómo + geom_boxplot() especifica un diagrama de caja.\nPara graficar un histograma de frecuencia de la longitud del pétalo, usaríamos:\n\nplot2 &lt;- ggplot(iris, aes(Petal.Length)) +\n  geom_histogram()\nprint(plot2)\n\n\n\n\nPara generar un gráfico de dispersión y visualizar la relación entre la longitud del pétalo y la longitud del sépalo, usaríamos:\n\nplot3 &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point()\nprint(plot3)\n\n\n\n\nEstos puntos podrían ser coloreados según los niveles de una variable categórica. Para hacer esto, agrega colour=\"variable.categórica\" dentro de los paréntesis de aes. Para colorear por especie, usaríamos:\n\nplot4 &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length, colour = Species)) +\n  geom_point()\nprint(plot4)\n\n\n\n\n\n\nAñadiendo un tema básico\nLa apariencia general del ggplot se puede modificar con diferentes temas predefinidos. Esto se puede hacer agregando + theme_bw() o + theme_classic() al final de tu línea de código. Al igual que con geom(), asegúrate de que haya paréntesis abiertos y cerrados después del nombre del tema.\n\nplot5 &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length, colour = Species)) +\n  geom_point() +\n  theme_bw()\nprint(plot5)\n\n\n\nplot6 &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length, colour = Species)) +\n  geom_point() +\n  theme_classic()\nprint(plot6)\n\n\n\n\n\n\nMás ayuda\nPara personalizar aún más la estética del gráfico, incluyendo el color y el formato, consulta nuestras otras páginas de ayuda de ggplot:\n* Modificar la apariencia general\n* Agregar títulos y nombres de ejes\n* Colores y símbolos.\nLa ayuda de todas las funciones de ggplot se puede encontrar en The master ggplot help site.\nSe puede descargar una útil hoja de referencia con funciones comúnmente utilizadas aquí.\n\nChang, W (2012) R Graphics cookbook. O’Reilly Media. - una guía de ggplot con mucha ayuda en línea aquí\n\nAutor: Fiona Robinson\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/ggplot/ggplot-colour-shapes/index.html",
    "href": "graphics/ggplot/ggplot-colour-shapes/index.html",
    "title": "Colores y formas",
    "section": "",
    "text": "Antes de comenzar, lee la página sobre los conceptos básicos de visualización con ggplot e instala el paquete ggplot2.\n\nlibrary(ggplot2)\n\n\nEn estos ejemplos, utilizaremos un conjunto de datos que ya está en R con la longitud y el ancho de partes florales de tres especies de iris. Primero, carga el conjunto de datos:\n\ndata(iris)\n\nLos siguientes gráficos se utilizarán como código base a lo largo de este tutorial:\n* Un gráfico de dispersión de longitud de pétalo vs. longitud de sépalo\n* Un diagrama de caja de longitud de sépalo vs. especies\n* Un histograma de frecuencia de longitud de sépalo\n\nIrisPlot &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point()\n\nIrisBox &lt;- ggplot(iris, aes(Species, Sepal.Length, fill = Species)) +\n  geom_boxplot()\n\nIrisHist &lt;- ggplot(iris, aes(Sepal.Length)) +\n  geom_histogram()\n\n\n\n\n\n\n\nCambiar el color de todo el gráfico o de su contorno\nPara colorear todo tu gráfico de un solo color, agrega fill = \"color\" o colour = \"color\" dentro de los corchetes siguientes al código geom_... donde especificaste qué tipo de gráfico quieres.\nTen en cuenta que para la mayoría de los gráficos, fill = \"color\" coloreará toda la forma, mientras que colour = \"color\" coloreará el contorno.\nPor ejemplo, para hacer un diagrama de caja azul con un contorno rojo, o un histograma amarillo con un contorno verde:\n\nIrisBox &lt;- ggplot(iris, aes(Species, Sepal.Length)) +\n  geom_boxplot(fill = \"blue\", colour = \"red\")\n\nIrisHist &lt;- ggplot(iris, aes(Sepal.Length)) +\n  geom_histogram(fill = \"yellow\", colour = \"green\")\n\n\n\n\n\n\nPara gráficos de dispersión, colour = \"color\" especificará el color de relleno para la forma del punto.\n\nIrisPlot &lt;- ggplot(iris, aes(Petal.Length, Sepal.Length)) +\n  geom_point(colour = \"red\")\n\n\n\n\n\n\nLa paleta básica de colores puede ser revisada aquí. Recuerda incluir “” antes y después del nombre del color.\n\n\nUsar el color para visualizar variables adicionales\nVariables categóricas adicionales\nSi deseas colorear los puntos de un gráfico de dispersión según una tercera variable categórica, agrega colour = nombre_variable como argumento dela función aes. Esto le indica a ggplot que esta tercera variable coloreará los puntos. Para colorear los puntos según la variable Species:\n\nIrisPlot &lt;- ggplot(iris, aes(Petal.Length, Sepal.Length, colour = Species)) +\n  geom_point()\n\n\n\n\n\n\nPara colorear el diagramas de caja o el gráficos de barras según una variable categórica dada, utiliza fill = nombre_variable en lugar de colour.\n\nIrisBox &lt;- ggplot(iris, aes(Species, Sepal.Length, fill = Species)) +\n  geom_boxplot()\n\n\n\n\n\n\nVariables continuas adicionales\nEl formato básico para colorear una variable continua es muy similar al de una variable categórica. La única diferencia real es que debes usar + scale_colour_gradient(low = \"color1\", high = \"color2\"). Las otras escalas de color no funcionarán, ya que son para variables categóricas. Por ejemplo, aquí tienes un gráfico de longitud del sépalo vs. longitud del pétalo, con los símbolos coloreados según su valor de ancho del sépalo.\n\nIrisPlot.continuous &lt;- ggplot(iris, aes(Petal.Length, Sepal.Length, colour = Sepal.Width)) +\n  geom_point()\n\nprint(IrisPlot.continuous)\n\n\n\n\nPara hacer el degradado más efectivo, especifica dos colores dentro de los corchetes + scale_colour_gradient que representen los extremos del degradado. Por ejemplo:\n\nprint(IrisPlot.continuous + scale_colour_gradient(low = \"black\", high = \"white\"))\n\n\n\nprint(IrisPlot.continuous + scale_colour_gradient(low = \"darkolivegreen1\", high = \"darkolivegreen\"))\n\n\n\n\n\n\nElección de tus propios colores para estas variables\nEsto se puede hacer de varias formas. El formato básico consiste en agregar + scale_colour_...() para gráficos de dispersión o + scale_fill_...() para diagramas de cajas al código donde “imprimes” tu gráfico, donde ...() es una de varias opciones. La sintaxis es:\nprint(tu.grafico.basico + scale_colour_...())\nHay varias opciones para la parte + scale_colour_...().\nSelección individual de colores. Para elegir colores manualmente, puedes usar + scale_colour_manual() o + scale_fill_manual(). Por ejemplo, para elegir tres colores para los gráficos de iris:\n\nprint(IrisPlot + scale_colour_manual(values = c(\"Blue\", \"Red\", \"Green\")))\n\n\n\nprint(IrisBox + scale_fill_manual(values = c(\"Black\", \"Orange\", \"Brown\")))\n\n\n\n\nAsignar tonos en una escala de grises. Utiliza + scale_colour_grey() o + scale_fill_grey()\n\nprint(IrisPlot + scale_colour_grey())\n\n\n\nprint(IrisBox + scale_fill_grey())\n\n\n\n\nAsignación de colores desde una paleta predefinida. Utiliza + scale_colour_brewer() o + scale_fill_brewer(). Para hacer esto, deberás instalar el paquete RColorBrewer y cargarlo en R.\n\nlibrary(RColorBrewer)\n\n\nEsto se puede agregar al final del código de tu gráfico, al igual que los otros + scale_colour_brewer(palette = \"nombre_de_la_paleta\") para gráficos de dispersión y + scale_fill_brewer(palette = \"nombre_de_la_paleta\") para diagramas de caja, donde \"nombre_de_la_paleta\" es una de las paletas disponibles. Por ejemplo,\n\nprint(IrisPlot + scale_colour_brewer(palette = \"Dark2\"))\n\n\n\nprint(IrisBox + scale_fill_brewer(palette = \"Oranges\"))\n\n\n\n\n\n\nCambiar símbolos en un gráfico de dispersión\nEn un gráfico de dispersión simple sin variables de agrupamiento, puedes cambiar la forma del símbolo agregando shape = ? al código de geom_point(), donde ? es uno de los siguientes números para diferentes formas.\n\nPor ejemplo, para usar un triángulo relleno,\n\nIrisPlot &lt;- ggplot(iris, aes(Petal.Length, Sepal.Length)) +\n  geom_point(shape = 17)\n\n\n\n\n\n\nSe pueden usar diferentes símbolos para agrupar datos en un gráfico de dispersión. Esto puede ser muy útil al imprimir en blanco y negro o para distinguir aún más tus categorías.\nPara hacer esto, debes agregar shape = nombre_variable dentro de los corchetes de aes de tu gráfico básico, donde nombre_variable es el nombre de tu variable de agrupamiento. Por ejemplo, para tener diferentes símbolos para cada especie, usaríamos.\n\nIrisPlot.shape &lt;- ggplot(iris, aes(Petal.Length, Sepal.Length, shape = Species)) +\n  geom_point()\n\n\n\n\n\n\nPara establecer los símbolos manualmente, podemos utilizar los códigos de símbolo en scale_shape_manual() agregados a tu función de impresión.\n\nprint(IrisPlot.shape + scale_shape_manual(values = c(0, 16, 3)))\n\n\n\n\nEsto se puede usar junto con el color para distinguir y agrupar aún más tus variables.\n\nIrisPlot.shape &lt;- ggplot(iris, aes(Petal.Length, Sepal.Length, shape = Species, colour = Species)) +\n  geom_point()\n\nprint(IrisPlot.shape + scale_shape_manual(values = c(0, 16, 3)) + scale_colour_manual(values = c(\"chartreuse4\", \"chocolate\", \"slateblue4\")))\n\n\n\n\n\n\nMás ayuda\nPara personalizar aún más la estética del gráfico, incluyendo el color y el formato, consulta nuestras otras páginas de ayuda de ggplot: * Modificando la apariencia general. * Agregando títulos y nombres de ejes.\nPuedes encontrar ayuda sobre todas las funciones de ggplot en el sitio principal de ayuda de ggplot.\nPuedes descargar una hoja de ayuda útil con funciones comúnmente utilizadas aquí.\n\nChang, W (2012) R Graphics cookbook. O’Reilly Media. - una guía sobre ggplot con bastante ayuda en línea aquí\n\nAutor: Fiona Robinson\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/ggplot/ggplot-labels/index.html",
    "href": "graphics/ggplot/ggplot-labels/index.html",
    "title": "Títulos y etiquetas de ejes",
    "section": "",
    "text": "Las gráficas de ggplot son casi completamente personalizables. Esto te brinda la libertad de crear un diseño de gráfico que se ajuste perfectamente a tu informe, ensayo o artículo.\nEsta página proporciona ayuda para agregar títulos, leyendas y etiquetas de ejes.\nAntes de comenzar, lee la página sobre los conceptos básicos de visualización con ggplot e instala el paquete ggplot2.\n\nlibrary(ggplot2)\n\n\nEn estos ejemplos, vamos a utilizar un conjunto de datos que ya está en R con la longitud y el ancho de las partes florales de tres especies de iris. Primero, carga el conjunto de datos:\n\ndata(iris)\n\nEl siguiente código para un gráfico de dispersión de longitud del pétalo vs. longitud del sépalo, con las tres especies codificadas por colores, es la base que utilizaremos en este tutorial:\n\nIrisPlot &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length, colour = Species)) +\n  geom_point()\n\n\n\n\n\n\n\nAgregar un título\nPara agregar un título a tu gráfico, añade el código +ggtitle(\"Tu título aquí\") a tu línea de código básico de ggplot. Asegúrate de utilizar comillas al inicio y al final de tu título.\n\nprint(IrisPlot + ggtitle(\"Petal and sepal length of iris\"))\n\n\n\n\nSi tienes un título especialmente largo que funcionaría mejor en dos líneas, utiliza \\n para hacer un salto de línea. Asegúrate de utilizar la barra diagonal correcta.\n\nprint(IrisPlot + ggtitle(\"Petal and sepal length \\nof three species of iris\"))\n\n\n\n\n\n\nCambiar las etiquetas de los ejes\nPara modificar las etiquetas de los ejes, añade el código +labs(y = \"nombre del eje y\", x = \"nombre del eje x\") a tu línea de código básico de ggplot.\n\nprint(IrisPlot + labs(y = \"Petal length (cm)\", x = \"Sepal length (cm)\"))\n\n\n\n\nNota: También puedes usar +labs(title = \"Título\"), que es equivalente a ggtitle.\nPor ejemplo:\n\nprint(IrisPlot + labs(\n  title = \"Petal and Sepal Length \\nof Iris\",\n  y = \"Petal Length (cm)\", x = \"Sepal Length (cm)\"\n))\n\n\n\nCambiar el título de la leyenda\nDe la misma manera que editaste el título y los nombres de los ejes, puedes modificar el título de la leyenda agregando +labs(colour = \"Título de la leyenda\") al final de tu código básico de trazado.\nNota: Esto solo funcionará si realmente has añadido una variable adicional a tu código aes (en este caso, utilizando color = Species para agrupar los puntos por especie).\n\nIrisPlot &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length, colour = Species)) +\n  geom_point()\n\nprint(IrisPlot + labs(colour = \"Iris species\"))\n\n\n\n\nNota: Si estás utilizando un histograma, diagrama de caja o gráfico de barras, es ligeramente diferente. Debes usar fill en lugar de colour.\n\nIrisBox &lt;- ggplot(iris, aes(Species, Sepal.Length, fill = Species)) +\n  geom_boxplot()\nprint(IrisBox + labs(fill = \"Iris species\"))\n\n\n\nModificando el estilo del texto en la leyenda, ejes o título\nSe pueden modificar la fuente, color, tamaño y énfasis de las etiquetas y texto. Para hacer esto, utiliza el código theme() y personalízalo con element_text() para modificar estas propiedades.\nEl formato básico es: mi_tema &lt;- theme(tipo_de_título = element_text(tus_formatos))\nDonde “tipo_de_título” especifica qué texto en particular deseas editar. Estos pueden ser:\n\ntítulo del gráfico. - plot.title = element_text()\ntítulo del eje. - axis.title = element_text()\ntítulo de la leyenda. - legend.title = element_text()\ncategorías de la leyenda - legend.text = element_text()\napariencia de los valores/números del eje. - axis.text = element_text()\n\nOpciones de formato La fuente, color, tamaño y énfasis de cualquiera de estas etiquetas se pueden modificar con argumentos dentro de element_text(tu_formato).\n\nfamily. - el estilo de fuente. Ejemplos de fuentes incluyen: “Palatino”, “Helvetica”, “Courier”, “Times”. Puedes ver más opciones de fuente aquí. Por ejemplo, family = \"Palatino\"\nface. - el tipo de énfasis, con opciones que incluyen bold (negrita), italic (cursiva) y “bold.italic” (negrita cursiva). Por ejemplo, face = \"bold.italic\"\ncolour. - el color se puede cambiar a cualquiera de los colores listados aquí. Recuerda incluir “” antes y después del nombre del color. Por ejemplo, colour = \"steelblue2\".\nsize. - el tamaño del texto, especificado con un número. Por ejemplo, size = (3).\n\n\n\nCódigo de ejemplo\nAquí tienes un ejemplo de un tema que personaliza el título, la leyenda, las etiquetas de los ejes y especifica la fuente, énfasis, tamaño y color de cada uno de ellos. La figura se traza con este tema y con código adicional que proporciona el contenido del título y las etiquetas de los ejes:\n\nmynamestheme &lt;- theme(\n  plot.title = element_text(family = \"Helvetica\", face = \"bold\", size = (15)),\n  legend.title = element_text(colour = \"steelblue\", face = \"bold.italic\", family = \"Helvetica\"),\n  legend.text = element_text(face = \"italic\", colour = \"steelblue4\", family = \"Helvetica\"),\n  axis.title = element_text(family = \"Helvetica\", size = (10), colour = \"steelblue4\"),\n  axis.text = element_text(family = \"Courier\", colour = \"cornflowerblue\", size = (10))\n)\n\nprint(IrisPlot + mynamestheme + labs(title = \"Petal and sepal \\nlength of iris\", y = \"Petal length (cm)\", x = \"Sepal length (cm)\"))\n\n\n\n\n\n\nEliminación de una etiqueta\nOtra opción es eliminar completamente el texto del gráfico. Para hacer esto, utiliza el código = element_blank(), recordando los paréntesis abiertos y cerrados. El siguiente código eliminaría el título de la leyenda y el texto del eje.\n\nmyblanktheme &lt;- theme(\n  plot.title = element_text(family = \"Helvetica\", face = \"bold\", size = (15)),\n  legend.title = element_blank(),\n  legend.text = element_text(face = \"italic\", colour = \"steelblue4\", family = \"Helvetica\"),\n  axis.title = element_text(family = \"Helvetica\", size = (10), colour = \"steelblue4\"),\n  axis.text = element_blank()\n)\n\nprint(IrisPlot + myblanktheme + labs(title = \"Petal and sepal \\nlength of iris\", y = \"Petal length (cm)\", x = \"Sepal length (cm)\"))\n\n\n\nMás ayuda\nPara personalizar aún más la estética del gráfico, incluyendo el color y el formato, consulta nuestras otras páginas de ayuda de ggplot: * Modificar la apariencia general. * Colores y símbolos.\nPuedes encontrar ayuda sobre todas las funciones de ggplot en el sitio principal de ayuda de ggplot.\nUna hoja de referencia útil sobre las funciones comúnmente utilizadas se puede descargar aquí.\n\nChang, W (2012) R Graphics cookbook. O’Reilly Media. - una guía de ggplot con bastante ayuda en línea aquí\n\nAutor: Fiona Robinson\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/ggplot/index.html",
    "href": "graphics/ggplot/index.html",
    "title": "Gráficos en R con ggplot2",
    "section": "",
    "text": "ggplot2 es un paquete poderoso para la producción de gráficos en R que se puede utilizar para crear gráficos de aspecto profesional para informes, ensayos o trabajos académicos. Puede crear una variedad de gráficos, incluyendo diagramas de caja, gráficos de dispersión e histogramas, y se pueden personalizar ampliamente para adaptarse a tus datos.\nEl paquete lleva el nombre de un libro llamado The Grammar of Graphics que introduce un enfoque sistemático y estructurado para crear gráficos estadísticos.\nComienza con los Conceptos básicos para aprender la sintaxis básica de cómo crear un gráfico.\nLuego, visita nuestras otras páginas para personalizar aún más la estética del gráfico, incluyendo el color y el formato: * Modificar la apariencia general * Agregar títulos y nombres de ejes * Colores y símbolos.\nPuedes encontrar ayuda sobre todas las funciones de ggplot en el Sitio principal de ayuda de ggplot.\nUna hoja de referencia útil sobre las funciones más utilizadas se puede descargar aquí.\n\nChang, W (2012) R Graphics cookbook. O’Reilly Media. - una guía sobre ggplot con bastante ayuda en línea aquí\n\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/index.html",
    "href": "graphics/index.html",
    "title": "Gráficos",
    "section": "",
    "text": "R cuenta con una amplia gama de funciones y paquetes para visualizar datos.\nEstas páginas ofrecen una breve introducción a:"
  },
  {
    "objectID": "graphics/multivariate-vis/cluster-analysis/index.html",
    "href": "graphics/multivariate-vis/cluster-analysis/index.html",
    "title": "Análisis de Clúster",
    "section": "",
    "text": "El análisis de clúster es un método de clasificación que tiene como objetivo agrupar objetos en función de la similitud de sus atributos. Se utiliza comúnmente para agrupar una serie de muestras basadas en múltiples variables que han sido medidas en cada muestra. El procedimiento produce un diagrama en forma de árbol (un dendrograma) que ilustra las relaciones entre todas las muestras basadas en una medida definida de similitud.\n\nExisten muchos métodos disponibles para el agrupamiento (aglomerativo, divisivo, no jerárquico, etc.). Aquí tienes algunas instrucciones para uno de los métodos más comúnmente utilizados, el agrupamiento jerárquico aglomerativo. Este procedimiento involucra una serie de pasos:\n\nCalcular una matriz que contenga todas las similitudes entre pares de objetos.\nUnir los objetos que son más similares.\nRecalcular la matriz de similitud para ese clúster frente a todos los objetos restantes.\nRepetir los dos últimos pasos hasta que todos los objetos estén unidos.\n\n\nEjecutando el análisis\n\nEn este ejemplo, utilizaremos el análisis de clúster para visualizar las diferencias en la composición de contaminantes metálicos en las algas marinas del Puerto de Sídney (datos de (Roberts et al. 2008)). Descarga el conjunto de datos, Harbour_metals.csv, y cárgalo en R.\n\nHarbour_metals &lt;- read.csv(file = \"Harbour_metals.csv\", header = TRUE)\n\nEstos datos contienen las concentraciones de siete metales medidos en 60 muestras, la mitad de la alga marina Padina crassa y la mitad del Sargassum linearifolium.\nLas dos primeras columnas son variables categóricas que agrupan las muestras por sitio y especie de alga marina. La tercera columna tiene etiquetas únicas para cada muestra replicada y las columnas restantes son las concentraciones de metales.\nEl análisis de agrupamiento se realiza solo en las variables de respuesta, por lo que necesitamos crear un marco de datos solo con las concentraciones de metales (columnas 4 a 8).\n\nHarbour_metals2 &lt;- Harbour_metals[, 4:8]\n\nPara ayudar a interpretar el gráfico, podemos agregar las etiquetas de las muestras como nombres de fila en este marco de datos.\n\nrownames(Harbour_metals2) &lt;- Harbour_metals$Rep\n\nPara realizar el análisis de agrupamiento, necesitamos crear una matriz que cuantifique la similitud entre cada par de muestras. Aquí utilizaremos la distancia euclidiana como nuestro coeficiente de similitud, pero hay otros para elegir (ver más abajo).\n\nH_metals.sim &lt;- dist(Harbour_metals2, method = \"euclidean\")\n\nLuego, utilizamos la función hclust con un argumento que especifica el método de enlace (aquí utilizaremos el método de enlace simple).\n\nH_metals.cluster &lt;- hclust(H_metals.sim, method = \"single\")\n\nFinalmente, trazamos el objeto que fue creado por la función hclust.\n\nplot(H_metals.cluster)\n\n\n\n\nPodemos mejorar esto un poco alineando todas las muestras en la parte inferior convirtiéndolo en un objeto dendrograma (la función as.dendrogram) y trazándolo (aunque este gráfico predeterminado aún es un poco feo y necesitaría trabajar en etiquetas y ejes antes de estar listo para su publicación).\n\nplot(as.dendrogram(H_metals.cluster), ylab = \"Euclidean distance\")\n\n\n\n\n\n\nInterpretación de los resultados\nUn dendrograma tiene una rama para cada muestra unida en nodos que se relacionan con el valor del coeficiente de similitud que une los dos objetos. La interpretación de todas las relaciones se realiza mediante el examen de la estructura de ramificación (qué objetos se unen más estrechamente entre sí) y de las similitudes en las que se unen. Los objetos que se unen cerca de las puntas de las ramas son más similares entre sí que aquellos que se unen más cerca de la base del árbol (ten en cuenta que el dendrograma predeterminado en R se parece a un árbol boca abajo, con las ramas en la parte inferior y el tronco en la parte superior).\nSe podría evidenciar una fuerte evidencia de grupos distintos si hubiera grupos donde las muestras dentro de un grupo son mucho más similares entre sí que las muestras en otros grupos.\n\n\nSupuestos a verificar\nAntes de ejecutar un análisis de agrupamiento para crear un dendrograma, debes considerar:\nQué medida de similitud usar. Los dendrogramas se pueden crear a partir de cualquier matriz de similitud. Hay muchas medidas de distancia que se pueden utilizar para describir la similitud entre muestras. La función dist en R tiene opciones como \"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\" o \"minkowski\" (especificadas en el argumento method de dist). Otras medidas están disponibles en otros paquetes (por ejemplo, la medida Bray-Curtis, que se recomienda para análisis de datos de composición de especies, está disponible en el paquete vegan).\nSi los datos necesitan ser transformados o estandarizados. Si las variables se miden en escalas muy diferentes, o si hay valores atípicos, entonces la estructura del dendrograma estará fuertemente influenciada por los valores más grandes en el conjunto de datos. Las variables se pueden transformar o estandarizar para disminuir la influencia de los valores grandes (es decir, tratar todas las variables en igualdad de condiciones).\nQué método se utilizará para crear el dendrograma. La estructura de los dendrogramas también puede ser sensible al algoritmo utilizado para construir el árbol (método de enlace). En el ejemplo anterior, utilizaste el método de enlace simple. La función hclust en R tiene varios disponibles, como \"ward\", \"single\", \"complete\", \"average\", \"mcquitty\", \"median\" y \"centroid\".\n\n\nComunicación de los resultados\nEscrita. La interpretación del dendrograma se describe en el texto de la sección de Resultados (por ejemplo, ¿hay grupos evidentes de muestras? ¿hay muestras que son muy diferentes del resto?). No hay resultados numéricos que informar.\nVisual. Los resultados de los análisis de agrupamiento se comunican visualmente con el dendrograma. Es importante etiquetar los ejes para mostrar qué medida de similitud se utilizó en el análisis.\nSi hay grupos predefinidos de muestras (por ejemplo, muestras agrupadas por especie en el ejemplo anterior), generalmente se etiquetan las muestras o se les proporcionan símbolos codificados por colores para permitir una visualización más fácil de los patrones en el dendrograma. Con conjuntos de datos grandes, las etiquetas individuales para cada muestra (como hicimos aquí) generan gráficos muy completos.\nCon muchas muestras en tu dendrograma, a menudo es necesario etiquetarlas de alguna manera (por ejemplo, por color) para ayudar a visualizar los patrones. El paquete dendextend te permite hacer esto. Aquí tienes un código para producir el mismo dendrograma con las muestras codificadas por color según la ubicación.\n\nlibrary(dendextend)\ndend &lt;- as.dendrogram(H_metals.cluster)\nsample_colours &lt;- as.numeric(Harbour_metals$Location)\nsample_colours &lt;- sample_colours[order.dendrogram(dend)]\nlabels_colors(dend) &lt;- sample_colours\nplot(dend, ylab = \"Euclidean distance\")\n\n\n\n\n\n\nMás ayuda\nPuedes acceder a la ayuda de R para las funciones principales utilizadas aquí escribiendo ?hclust, ?dist o ?as.dendrogram. Hay muchos paquetes de R asociados con los diferentes tipos de análisis de agrupamiento. Consulta una larga lista de paquetes posiblemente útiles aquí.\n\nQuinn, GP y MJ Keough (2002) Diseño experimental y análisis de datos para biólogos. Cambridge University Press. Capítulo 18. Escalamiento multidimensional y análisis de agrupamiento.\n\n\nMcKillup, S (2012) Estadística explicada. Una guía introductoria para científicos de la vida. Cambridge University Press. Capítulo 22. Conceptos introductorios del análisis multivariado.\n\nVisualización de dendrogramas en R\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/multivariate-vis/heatmaps/index.html",
    "href": "graphics/multivariate-vis/heatmaps/index.html",
    "title": "Heatmaps - Mapa de Calor",
    "section": "",
    "text": "Los mapas de calor son un método útil para explorar conjuntos de datos multivariados. Las variables de respuesta (por ejemplo, abundancias) se visualizan utilizando gradientes de color o esquemas de color. Con la transformación adecuada y la agrupación de filas y columnas, se pueden observar patrones interesantes dentro de los datos. También se pueden utilizar para mostrar los resultados después del análisis estadístico, por ejemplo, para mostrar aquellas variables que difieren entre los grupos de tratamiento.\n\nEn este tutorial, utilizaremos mapas de calor para visualizar patrones en las comunidades bacterianas encontradas en hábitats marinos en presencia de dos macrófitas (pasto marino y Caulerpa) en dos densidades (escasa y densa). También hay muestras de sedimento sin vegetación (Otro). Hay tres muestras replicadas en cada grupo.\nUtilizaremos el paquete pheatmap (mapas de calor bonitos) para dibujar nuestros mapas de calor. El paquete base de R también puede dibujar mapas de calor, pero tiene algunas limitaciones. Primero, instala el paquete y cárgalo en R. También necesitaremos el paquete dplyr para seleccionar filas y columnas.\n\nlibrary(pheatmap)\nlibrary(dplyr)\n\n\nLectura de los datos\nCon datos multivariados, a menudo tenemos dos marcos de datos con 1) los recuentos por muestra y 2) los factores que agrupan las muestras. Descarga estos dos archivos de datos, Bac.counts.csv y Bac.factors.csv, e impórtalos en R.\n\nBac.counts &lt;- read.csv(file = \"Bac.counts.csv\", header = TRUE, row.names = 1)\nBac.factors &lt;- read.csv(file = \"Bac.factors.csv\", header = TRUE, row.names = 1)\n\nEl argumento row.names=1 asigna la primera columna de la hoja de cálculo como nombres de fila en el marco de datos. Deberíamos verificar la estructura de los datos de los recuentos utilizando el comando head. Dado que hay muchas columnas, solo revisaremos las primeras 10 utilizando indexación (uso de [,] después del objeto, con números de fila antes de la coma y números de columna después).\n\nhead(Bac.counts[, 1:10])\n\n          DC1  DC2  DC3  DS1  DS2  DS3  SC1  SC2  SC3  SS1\nOtu00002 2906 2619 2200 2959 3205 2455 2815 2761 2275 3519\nOtu00003 1631 1323 1258 1055 1552 1509 1345 1255 1270 1180\nOtu00005 1493 1416 1592  984 1131  879 1430 1448 1296 1431\nOtu00004 1171 1164 1489  936 1514 1174 1271 1310 1207 1278\nOtu00006 1160 1226 1245  764 1134 1271  906  983 1110 1251\nOtu00007 1112 1042 1211 1060 1155 1103 1283 1198 1175 1485\n\n\nPodemos ver que los nombres de las filas de los datos tienen los números de código para cada unidad taxonómica operativa (OTU) de bacterias, y hay recuentos enteros de estas en cada muestra. Ahora, vamos a verificar las dimensiones de los datos (número de filas y columnas).\n\ndim(Bac.counts)\n\n[1] 4299   15\n\n\nHay 4299 unidades taxonómicas operativas (OTUs) de bacterias como filas entre 15 muestras como columnas.\nA continuación, podemos verificar la estructura de la información de factor utilizando la función str. En este experimento, las muestras están categorizadas por un ID de tratamiento (cada combinación de densidad y especie), niveles de densidad y niveles de especie. Las otras columnas son para fines de trazado en otro lugar.\n\nstr(Bac.factors)\n\n'data.frame':   15 obs. of  6 variables:\n $ Treatment_ID: chr  \"DC\" \"DC\" \"DC\" \"DS\" ...\n $ Density     : chr  \"Dense\" \"Dense\" \"Dense\" \"Dense\" ...\n $ Species     : chr  \"Caulerpa\" \"Caulerpa\" \"Caulerpa\" \"Seagrass\" ...\n $ pch1        : int  16 16 16 15 15 15 21 21 21 22 ...\n $ pch2        : int  4 22 21 15 16 NA NA NA NA NA ...\n $ legend      : chr  \"U - Unvegetated\" \"SZ - Sparse Zostera \" \"SC - Sparse Caulerpa\" \"DS - Dense Zostera\" ...\n\n\n\n\nDibujando un mapa de calor\nLa función básica es pheatmap. Vamos a intentarlo sin argumentos especiales, excepto que solo observaremos las primeras 500 OTUs (ya están ordenadas de mayor a menor abundancia total). La función slice en dplyr puede tomar cualquier subconjunto de filas numeradas (ver Subconjunto de datos).\n\nBac.counts500 &lt;- slice(Bac.counts, 1:500)\n\npheatmap(Bac.counts500)\n\n\n\n\n\n\n\n\nEn la figura, las muestras son columnas y las OTUs bacterianas son filas, con el color representando el rango de recuentos de cada OTU en cada muestra. El rojo significa más abundante (~3500 recuentos), el azul menos abundante (0 recuentos) y el amarillo claro está en algún punto intermedio. Ten en cuenta que tanto las filas como las columnas se han reorganizado en función de medidas de similitud entre filas y columnas (ver Análisis de clúster).\nTransformación de datos. Ahora puedes notar que tenemos un problema de escala. Los datos están llenos de bacterias poco comunes (azules) y eso es todo lo que podemos ver en el mapa de calor. Para visualizar esto de manera más efectiva, podemos intentar una transformación logarítmica con una constante de +1 para lidiar con los ceros.\n\nBac.Log10.counts500 &lt;- log10(Bac.counts500 + 1)\npheatmap(Bac.Log10.counts500)\n\n\n\n\n\n\n\n\nLa vista de los datos ha cambiado realmente con la transformación, al igual que el agrupamiento de filas y columnas. Parece haber algunas OTUs muy abundantes (rojo/amarillo), algunas de abundancia media (blanco/baja) y otras de baja abundancia (azul).\nEsto no se habría visto tan claramente si no hubiéramos agrupado las filas y columnas, y si las hubiéramos representado “tal cual” desde la tabla de datos (aunque aquí ya se había hecho un ordenamiento de filas). Puedes ver esto si dibujamos el mapa de calor nuevamente sin agrupar las filas y columnas.\n\npheatmap(Bac.Log10.counts500, cluster_rows = FALSE, cluster_cols = FALSE)\n\n\n\n\n\n\n\n\n\n\nColoreando grupos de muestras\nAntes de analizar más a fondo cómo afecta el agrupamiento a los patrones observados, debemos agregar algunos colores asociados a los grupos de tratamiento. El método más simple es utilizar el data frame Bac.factors como entrada, asegurándote de que:\n\nespecifiques correctamente las covariables categóricas (grupos de factores) y las covariables numéricas (por ejemplo, concentración),\nlos nombres de las filas en “Bac.factors” coincidan con los de “Bac.counts”, y\nluego elimines los factores que no deseas mostrar.\n\nPara extraer solo las columnas density y species de nuestro data frame de factores, podemos utilizar la función select del paquete “dplyr” y luego usar estos para codificar con colores nuestras muestras.\n\nBac.factorsDS &lt;- select(Bac.factors, Density, Species)\n\npheatmap(Bac.Log10.counts500, annotation_col = Bac.factorsDS)\n\n\n\n\n\n\n\n\nLos colores son bastante feos. Crear los tuyos propios es complicado, pero implica crear vectores de colores nombrados y luego agregarlos a una lista. Esto representa la información de anotación de colores. Debemos definir colores para las covariables categóricas (grupos de factores) y rangos de colores para las covariables numéricas (por ejemplo, concentración).\n\n# Reordenar los niveles de densidad a Esparcido, Denso, Otro\nBac.factorsDS$Density &lt;- factor(Bac.factorsDS$Density, levels = c(\"Sparse\", \"Dense\", \"Other\"))\nDensityCol &lt;- c(\"darkorchid\", \"darkorange\", \"grey80\")\nnames(DensityCol) &lt;- levels(Bac.factorsDS$Density)\n\n# Reordenar las especies a Hierba marina, Caulerpa, Otro\nBac.factorsDS$Species &lt;- factor(Bac.factorsDS$Species, levels = c(\"Seagrass\", \"Caulerpa\", \"Other\"))\nSpeciesCol &lt;- c(\"forestgreen\", \"blue3\", \"grey80\")\nnames(SpeciesCol) &lt;- levels(Bac.factorsDS$Species)\n\n# Agregar a una lista, donde los nombres coinciden con los del dataframe de factores\nAnnColour &lt;- list(\n  Density = DensityCol,\n  Species = SpeciesCol\n)\n\n# Verificar el resultado\nAnnColour\n\n$Density\n      Sparse        Dense        Other \n\"darkorchid\" \"darkorange\"     \"grey80\" \n\n$Species\n     Seagrass      Caulerpa         Other \n\"forestgreen\"       \"blue3\"      \"grey80\" \n\n\nAhora podemos volver a dibujar el mapa de calor con los colores elegidos.\n\npheatmap(Bac.Log10.counts500, annotation_col = Bac.factorsDS, annotation_colors = AnnColour)\n\n\n\n\n\n\n\n\nEn este caso, al permitir que los datos hablen por sí mismos (con agrupamiento de filas y columnas por defecto), se muestra que las muestras sin vegetación (Other) son diferentes de las muestras de macrófitos (independientemente de la densidad) y que las muestras de pastos marinos generalmente se agrupan juntas. Ten en cuenta que otros métodos (por ejemplo, ordenación) pueden mostrar comparaciones de muestra a muestra mucho mejores que los mapas de calor, pero los mapas de calor revelan los patrones de las variables a diferencia de esos métodos. Comprender lo que están haciendo los datos a menudo se pasa por alto en el análisis multivariante.\n\n\nMétodos de agrupamiento de filas y columnas\nPor defecto, pheatmap utiliza la distancia euclidiana como medida de similitud y agrupa las muestras según el método ‘complete’. Hay varios otros métodos de distancia y agrupamiento disponibles mediante argumentos adicionales: clustering_distance_rows, clustering_distance_cols y clustering_method.\nAlgunos son mejores que otros, pero tendrás que consultar más en la literatura sobre esto. Sin embargo, para el agrupamiento, el método de agrupamiento ‘average’ parece superior en muchas aplicaciones de ciencias de la computación. Una vez más, la ventaja de los mapas de calor es que puedes ver qué están haciendo los datos en relación a las opciones que has elegido.\nVeamos qué se produce al utilizar la distancia de ‘Manhattan’ como medida de similitud entre filas y columnas, y el método de agrupamiento ‘average’.\n\npheatmap(Bac.Log10.counts500,\n  clustering_distance_rows = \"manhattan\",\n  clustering_distance_cols = \"manhattan\", clustering_method = \"average\",\n  annotation_colors = AnnColour, annotation_col = Bac.factorsDS\n)\n\n\n\n\n\n\n\n\nPuedes ver que cambiar el agrupamiento ha cambiado significativamente el mapa de calor producido.\n\n\nEscalamiento de variables\nEs posible que deseemos comparar la abundancia de cada OTU bacteriano solo entre las muestras en lugar de contrastar su abundancia con otras OTUs de abundancia variable. Para hacer esto, podemos escalar las abundancias dentro de cada OTU de manera que la escala de colores muestre solo el rango relativo de abundancia para cada OTU individual. En este ejemplo, eso implica escalar la abundancia para cada fila con scale=\"row\".\n\npheatmap(Bac.Log10.counts500,\n  scale = \"row\", clustering_distance_rows = \"manhattan\",\n  clustering_method = \"average\",\n  annotation_colors = AnnColour, annotation_col = Bac.factorsDS\n)\n\n\n\n\n\n\n\n\nAhora podemos ver cuántas desviaciones estándar se encuentra la abundancia en Log10 de un solo OTU con respecto a la media para ese OTU en una muestra, en comparación solo con otras muestras para ese OTU. La leyenda muestra que el número de desviaciones estándar va desde +3 hasta -3. Podemos observar cómo varios OTUs bacterianos están subrepresentados en el sedimento no vegetado (abundancias azules en la parte inferior izquierda) en comparación con el sedimento con macrófitas (abundancias amarillas/rojas).\n\n\nOrdenar por grupo\nTambién podemos ordenar las muestras por sus grupos o tratamientos en lugar de ordenar por similitud entre filas o columnas. Esto se hace ordenando los datos de entrada y desactivando el agrupamiento de las columnas con cluster_cols=FALSE.\n\nSampleOrder &lt;- order(Bac.factorsDS$Species, Bac.factorsDS$Density)\n\npheatmap(Bac.Log10.counts500[, SampleOrder],\n  cluster_cols = FALSE,\n  clustering_method = \"average\", annotation_colors = AnnColour, annotation_col = Bac.factorsDS\n)\n\n\n\n\n\n\n\n\n\n\nUtilizar mapas de calor después de análisis estadísticos\nSi hemos analizado nuestros datos multivariados e identificado las variables que difieren entre tratamientos, podemos elegir mostrar solo esas variables en el mapa de calor. En este ejemplo, solo analizaremos los OTUs bacterianos que difieren entre los dos niveles del factor “Especie” después de eliminar las muestras no vegetadas.\nAnalizaremos las abundancias de todos los OTUs con modelos lineales generalizados multivariados utilizando la función manyglm del paquete mvabund. Los detalles de ese análisis no se describen aquí (consulta la Introducción a mvabund para obtener ayuda con ese paquete). Ten en cuenta que ejecutar anova.manyglm puede ser bastante lento.\n\n# Crear factor y archivo de datos sin las muestras U1, U2 y U3\nBac.factorsDS_noU &lt;- filter(Bac.factors, Treatment_ID != \"U\")\nBac.counts500DS_noU &lt;- select(Bac.counts500, -contains(\"U\"))\n\n# Mvabund\nlibrary(mvabund)\ndat.mva &lt;- mvabund(t(Bac.counts500DS_noU))\nplot(dat.mva)\ndat.nb &lt;- manyglm(dat.mva ~ Species * Density, data = Bac.factorsDS_noU)\ndat.aov &lt;- anova.manyglm(dat.nb, p.uni = \"unadjusted\", nBoot = 500)\ndat.aov$uni.p[, 1:5]\nSpeciesDiffs &lt;- which(dat.aov$uni.p[\"Species\", ] &lt; 0.05 & dat.aov$uni.p[\"Species:Density\", ] &gt; 0.05)\n\nPodemos incluir el argumento cutree para las filas y columnas, para dividir los datos en los dos grupos esperados. Cruzamos los dedos para que muestre lo que esperamos.\n\n# Crear un vector que se utilizará para seleccionar muestras que no son del sedimento\nDS &lt;- Bac.factors$Treatment_ID != \"U\"\n\npheatmap(Bac.Log10.counts500[SpeciesDiffs, DS],\n  scale = \"row\",\n  clustering_method = \"average\", annotation_col = Bac.factorsDS,\n  cutree_rows = 2, cutree_cols = 2\n)\n\n\n\n\n\n\n\n\nY los resultados son como se esperaba. La mitad superior del mapa de calor muestra aquellas variables sobre-representadas en la especie Caulerpa. La mitad inferior muestra aquellas sobre-representadas en las praderas marinas.\n\n\nMás ayuda\nEscribe ?pheatmap para obtener ayuda de R sobre esta función.\nAutor: Shaun Nielsen\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/multivariate-vis/index.html",
    "href": "graphics/multivariate-vis/index.html",
    "title": "Datos Multivariados",
    "section": "",
    "text": "R cuenta con una amplia gama de funciones y paquetes para visualizar datos multivariados. Aquí tienes ayuda para algunas de las técnicas más comúnmente utilizadas:\n\nEscalamiento multidimensional\n\nAnálisis de componentes principales\n\nAnálisis de agrupamiento - Cluster"
  },
  {
    "objectID": "graphics/multivariate-vis/mds/index.html",
    "href": "graphics/multivariate-vis/mds/index.html",
    "title": "Escalamiento multidimensional",
    "section": "",
    "text": "El escalamiento multidimensional (MDS, por sus siglas en inglés) es un enfoque popular para representar gráficamente las relaciones entre objetos (por ejemplo, parcelas o muestras) en un espacio multidimensional. La reducción de dimensiones mediante MDS se logra tomando el conjunto original de muestras y calculando una medida de disimilitud (distancia) para cada comparación pareja de muestras. Las muestras se representan gráficamente generalmente en dos dimensiones de manera que la distancia entre los puntos en el gráfico aproxime su disimilitud multivariada lo más cercanamente posible.\nUna de las aplicaciones más comunes de MDS en las ciencias ambientales es examinar la similitud de diferentes comunidades ecológicas basadas en su composición de especies. En estos análisis, se registra la abundancia de todas las especies dentro de muestras replicadas (parcelas o cuadrantes en el campo, sitios diferentes, etc.) y los datos en bruto toman la forma de una matriz de especies (las variables) por muestras.\nConsidera un ejemplo en el que los investigadores querían contrastar la especificidad de alimentación de herbívoros marinos en cinco especies de macroalgas. Se recolectaron veinte individuos replicados de cada una de las siete especies de macroalgas en el puerto de Sydney, y se registró la abundancia de siete especies de crustáceos herbívoros en cada réplica (una matriz de datos de 100 muestras por 7 variables, datos de Poore et al. 2000).\nEl escalamiento multidimensional puede crear un gráfico de ordenación a partir de cualquier medida de similitud o disimilitud entre muestras, y existen muchas medidas diferentes para calcular la disimilitud entre muestras. La más básica de estas es la distancia euclidiana (es decir, simplemente la distancia en línea recta entre dos puntos en el espacio multivariado). Sin embargo, para contrastes de composición de especies, la medida de distancia más comúnmente utilizada es la métrica de disimilitud de Bray-Curtis. La razón de su popularidad es que, en comparación con otras medidas, Bray-Curtis maneja mejor la gran proporción de ceros (por ejemplo, ausencias de especies) que se encuentran comúnmente en conjuntos de datos ecológicos. Esta medida no considera las ausencias compartidas como similares, lo cual tiene sentido biológicamente.\nIn addition to the choice of distance measure there are a number of different classes of MDS. Here we focus on what is referred to as non-metric MDS (nMDS), a non-parametric rank-based method that is comparatively robust to non-linear relationships between the calculated dissimilarity measure and the projected distance between objects.\nAdemás de la elección de la medida de distancia, existen diferentes clases de MDS. Aquí nos enfocamos en lo que se conoce como MDS no métrico (nMDS), un método no paramétrico basado en rangos que es comparativamente robusto frente a relaciones no lineales entre la medida de disimilitud calculada y la distancia proyectada entre objetos."
  },
  {
    "objectID": "graphics/multivariate-vis/mds/index.html#supuestos-a-verificar",
    "href": "graphics/multivariate-vis/mds/index.html#supuestos-a-verificar",
    "title": "Escalamiento multidimensional",
    "section": "Supuestos a verificar",
    "text": "Supuestos a verificar\nComo enfoque basado en clasificación NMDS, es comparativamente robusto ante la no linealidad entre la distancia entre objetos en el espacio y su disimilitud multivariada.\nStress. No es un supuesto del proceso, pero el factor más importante a considerar después de generar un gráfico de MDS es el “Stress”. Este proporciona una medida del grado en que la distancia entre las muestras en el espacio dimensional reducido (generalmente 2 dimensiones) se corresponde con la distancia multivariada real entre las muestras. Valores de estrés más bajos indican una mayor conformidad y, por lo tanto, son deseables. Valores de Stress altos indican que no hubo una disposición bidimensional de los puntos que refleje sus similitudes. Una regla general es que los valores de Stress idealmente deberían ser inferiores a 0.2 o incluso 0.1.\nPuedes obtener el valor de estrés de tu gráfico con:\n\nHerb_community.mds$stress\n\n[1] 0.1485347\n\n\nSi el valor de Stress es mayor que 0.2, se recomienda incluir una dimensión adicional, pero recuerda que los cerebros humanos no están muy preparados para visualizar objetos en más de 2 dimensiones.\nTransformación y estandarización. La transformación de conjuntos de datos antes de crear un gráfico de MDS a menudo es deseable, no para cumplir con supuestos de normalidad, sino para reducir la influencia de valores extremos. Por ejemplo,\n\nHerb_community.sq &lt;- sqrt(Herb_community)\nHerb_community.sq.mds &lt;- metaMDS(comm = Herb_community.sq, distance = \"bray\", trace = FALSE)\n\nTambién se puede realizar la estandarización de muestras o variables (por su media o totales) para crear gráficos donde todas las variables tengan igual influencia, o gráficos donde las diferencias entre las muestras se basen únicamente en valores relativos y no en su magnitud. El paquete vegan proporciona varios métodos comunes de estandarización utilizados para conjuntos de datos de comunidades en la función decostand.\nRecuerda que cualquiera de estas modificaciones realmente cambia tu pregunta. Si el ejemplo anterior se realizó con muestras estandarizadas por abundancia total, entonces el gráfico representaría una comparación de composición de especies sin influencia de las diferentes abundancias encontradas en los hábitats.\nTambién ten en cuenta que la ordenación basada en distancias ha sido criticada por no tener en cuenta las relaciones de varianza y media que son comunes en conjuntos de datos de abundancia como el utilizado aquí, y por carecer de un modelo estadístico subyacente - consulta a Hui et al. (2014) “Model-based approaches to unconstrained ordination” en Methods in Ecology and Evolution 6:399-411. link.\n\nComunicación de los resultados\nVisual. Los resultados de un MDS se presentan mejor visualmente como un gráfico de ordenación bidimensional (como el anterior). Raramente se utiliza un gráfico tridimensional.\nEscrito. La descripción escrita de los análisis de MDS (a menudo en la leyenda de la figura) debería mencionar qué métrica de disimilitud se utilizó, si los datos fueron transformados o estandarizados, y presentar el valor de estrés. En un análisis formal, los gráficos de MDS suelen ir acompañados de alguna prueba estadística multivariada de disimilitud entre grupos de tratamiento/observación, por ejemplo, a través de la función adonis en vegan.\n\n\nMás ayuda\nEscribe ?metaMDS para obtener ayuda sobre esta función en R.\n\nQuinn, GP y MJ Keough (2002) Experimental design and data analysis for biologists. Cambridge University Press. Capítulo 18. Escalamiento multidimensional y análisis de conglomerados.\n\n\nMcKillup, S (2012) Statistics explained. An introductory guide for life scientists. Cambridge University Press. Capítulo 22. Conceptos introductorios del análisis multivariado.\n\nAutor: Andrew Letten\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/multivariate-vis/pca/index.html",
    "href": "graphics/multivariate-vis/pca/index.html",
    "title": "Análisis de Componentes Principales",
    "section": "",
    "text": "El Análisis de Componentes Principales (PCA) es una de las técnicas estadística multivariada más utilizadas. La motivación principal detrás del PCA es reducir o resumir un gran número de variables en un número menor de variables derivadas que puedan ser fácilmente visualizadas en un espacio bidimensional o tridimensional. Por ejemplo, el PCA podría ser utilizado para comparar la química de diferentes cuencas hidrográficas basado en múltiples variables o para cuantificar la variación fenotípica entre especies basado en múltiples medidas morfológicas.\nEl nuevo conjunto de variables creado por el PCA puede ser utilizado en otros análisis, pero más comúnmente como un nuevo conjunto de ejes en los cuales se pueden graficar los datos multivariados.\nEl primer componente principal (PC) se ajusta de tal manera que explique la máxima cantidad de variación en los datos. Puedes pensar en esto como una línea de mejor ajuste en el espacio multivariado, lo más cerca posible de todos los puntos con la variación maximizada a lo largo de la línea y minimizada en la dirección perpendicular a la línea. El segundo PC se ajusta en ángulo recto al primero (es decir, ortogonalmente, sin correlación) de manera que explique la mayor cantidad posible de la variación restante. Luego, se pueden ajustar componentes adicionales, que deben ser ortogonales a los componentes existentes, mediante el mismo proceso iterativo.\nVisualizar esto en dos dimensiones ayuda a entender el enfoque. Los puntos de datos se graficarán en los nuevos ejes azules (los componentes principales) en lugar de los ejes originales negros. ¡Ahora imagina ajustar esas líneas en más de tres dimensiones!\nConsidera a una fisióloga vegetal que intenta cuantificar las diferencias en la forma de las hojas entre dos especies de árboles. Registró la longitud total (hoja + pecíolo), la longitud de la hoja, el ancho en el punto más ancho, el ancho a la mitad de la hoja y la longitud del pecíolo de diez hojas de cada especie. Estos datos son de cinco dimensiones (es decir, cinco variables medidas) y podemos usar el PCA para extraer dos nuevas variables que nos permitirán visualizar los datos en menos dimensiones.\nEs muy probable que existan relaciones fuertes entre las variables en nuestro conjunto de datos de ejemplo (por ejemplo, longitud de la hoja versus longitud total). Esto significa que los componentes principales probablemente expliquen una buena parte de la variación (imagina ajustar una línea recta a lo largo de una colección de puntos con forma de salchicha en el espacio multivariado). Si todas las variables estuvieran completamente no correlacionadas entre sí, entonces el PCA no funcionaría muy bien (imagina tratar de ajustar una línea de mejor ajuste a una colección de puntos en forma de esfera en el espacio multivariado)."
  },
  {
    "objectID": "graphics/multivariate-vis/pca/index.html#más-ayuda",
    "href": "graphics/multivariate-vis/pca/index.html#más-ayuda",
    "title": "Análisis de Componentes Principales",
    "section": "Más ayuda",
    "text": "Más ayuda\nEscribe ?princomp para obtener ayuda en R sobre esta función.\nUna página interactiva interesante que te ayudará a comprender qué hace el ACP se encuentra aquí.\n\nQuinn, GP y MJ Keough (2002) Experimental design and data analysis for biologists. Cambridge University Press. Cap. 17. Componentes principales y análisis de correspondencia.\n\n\nMcKillup, S (2012) Statistics explained. An introductory guide for life scientists.. Cambridge University Press. Cap. 22. Conceptos introductorios del análisis multivariado.\n\nAutor: Andrew Letten\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/spatial-vis/basic-raster/index.html",
    "href": "graphics/spatial-vis/basic-raster/index.html",
    "title": "Fundamentos de imagenes raster",
    "section": "",
    "text": "Este tutorial se centra en introducir los conceptos básicos de lo que es un raster, y luego cómo importar datos raster a R y realizar algunas manipulaciones básicas. Si solo necesitas crear un mapa de ubicación de sitios o algo similar, es posible que sea mejor comenzar con este tutorial.\n\nEmpecemos configuración\nVerifica si puedes ver los datos que vamos a utilizar (tu directorio de trabajo debe ser la ubicación de este archivo)\n\nfile.exists(\"Cairns_Mangroves_30m.tif\")\nfile.exists(\"SST_feb_2013.img\")\nfile.exists(\"SST_feb_mean.img\")\n\nInstala los paquetes que vamos a necesitar: el paquete raster es la biblioteca principal para objetos raster en R, dismo tiene algunos envoltorios útiles para diversas funciones de muestreo (además de otras cosas interesantes relacionadas con la SDM), y rgdal tiene muchos controladores para leer y escribir varios formatos de datos espaciales.\n\ninstall.packages(c(\"raster\", \"dismo\", \"rgdal\"))\n\nVerifica que puedas cargarlos\n\nlibrary(raster)\nlibrary(dismo)\nlibrary(rgdal)\n\n\n\n¿Qué es un raster?\nAhora estamos listos para comenzar, pero primero, ¿qué es un raster? Bueno, simplemente, es una cuadrícula de coordenadas en las que podemos definir ciertos valores, y mostramos los elementos de la cuadrícula correspondiente según esos valores. Los datos raster son esencialmente una matriz, pero un raster es especial en el sentido de que definimos la forma y el tamaño de cada elemento de la cuadrícula, y generalmente dónde debe ubicarse la cuadrícula en algún espacio conocido (es decir, un sistema de coordenadas geográficas proyectadas).\n\n\n\nComprensión de los datos raster\nCrea un objeto raster y consultalo\n\ndummy_raster &lt;- raster(ncol = 10, nrow = 10) # vamos a crear un raster pequeño\nnrow(dummy_raster) # número de píxeles\nncol(dummy_raster) # número de píxeles\nncell(dummy_raster) # número total de píxeles\n# plot(dummy_raster) # no se muestra el gráfico porque el raster está vacío\nhasValues(dummy_raster) # puedes verificar si tu raster tiene datos\nvalues(dummy_raster) &lt;- 1 # asignar un valor de píxel al raster, en este caso 1\nplot(dummy_raster) # todo el raster tiene un valor de píxel de 1\n\nCrea un raster de números aleatorios para poder ver qué está sucediendo de manera más sencilla.\n\nvalues(dummy_raster) &lt;- runif(ncell(dummy_raster)) # se asigna un número aleatorio a cada píxel\nplot(dummy_raster) # ahora el raster tiene píxeles con números aleatorios\nvalues(dummy_raster) &lt;- runif(ncell(dummy_raster))\nplot(dummy_raster)\ndummy_raster[1, 1] # podemos consultar rasters (y seleccionar los valores de la matriz) utilizando la indexación estándar de R\ndummy_raster[1, ]\ndummy_raster[, 1]\n\nUtiliza esto para consultar interactivamente el raster - presiona “esc” para salir\n\nclick(dummy_raster)\n\n¿Qué tiene de especial un objeto raster?\n\nstr(dummy_raster) # observa el CRS y la extensión, además de otros atributos\ncrs(dummy_raster) # verifica el sistema de coordenadas en el formato PROJ.4 por defecto\nxmax(dummy_raster) # verifica la extensión máxima\nxmin(dummy_raster)\nymax(dummy_raster)\nymin(dummy_raster)\nextent(dummy_raster)\nres(dummy_raster) # resolución\nxres(dummy_raster) # ancho de píxel\nyres(dummy_raster) # altura de píxel\n\n\nEjercicios\n\nCrea un raster con una cara sonriente (pista: crea un raster en blanco y luego utiliza la indexación para cambiar los valores secuencialmente).\nExtrae algunos datos de vectores y matrices del raster (pista: utiliza la indexación o funciones como ?as.matrix).\nGenera un ubconjunto el raster en un trozo más pequeño (más complicado - consulta ?crop).\n\n\n\n\nTrabajando con datos reales de raster\nImporta los datos de manglares de Cairns y échales un vistazo\n\nmangrove &lt;- raster(\"Cairns_Mangroves_30m.tif\")\ncrs(mangrove) # obtener la proyección\nplot(mangrove, col = topo.colors(\"2\")) # observa dos valores de píxel, 0 (no manglar) y 1 (manglar)\nNAvalue(mangrove) &lt;- 0 # crear un único conjunto de datos binario donde los manglares tienen un valor de ráster 1\nplot(mangrove, col = \"mediumseagreen\")\n\nLa leyenda es un poco extraña - podemos cambiarla a una leyenda categórica haciendo esto, pero generalmente nos quedaremos con la barra continua predeterminada para reducir el desorden en el código\n\ncols &lt;- c(\"white\", \"red\")\nplot(mangrove, col = cols, legend = F)\nlegend(x = \"bottomleft\", legend = c(\"no mangrove\", \"mangrove\"), fill = cols)\n\nProcesamiento sencillo\n\nagg_mangrove &lt;- aggregate(mangrove, fact = 10) \n\npar(mfrow = c(2, 2))\nplot(mangrove, col = \"mediumseagreen\")\nplot(agg_mangrove, col = \"firebrick\")\nplot(agg_mangrove, col = \"firebrick\")\nplot(mangrove, col = \"mediumseagreen\", add = TRUE) \n\nCrea un búfer sencillo\n\nbuf_mangrove &lt;- buffer(agg_mangrove, width = 1000) # buffer\nplot(buf_mangrove, col = \"peachpuff\")\nplot(mangrove, col = \"mediumseagreen\", add = T) \n\nTen en cuenta que en este punto, podríamos jugar con los márgenes si nos importara, por ejemplo. par(mar = c(2,1,2,1), oma = c(2,1,2,1)).\nConvierte el raster en datos de puntos, y luego importa los datos de puntos como raster\n\npts_mangrove &lt;- rasterToPoints(mangrove)\nstr(pts_mangrove)\n\npar(mfrow = c(2, 2))\nplot(mangrove)\nplot(rasterFromXYZ(pts_mangrove)) \n\nNAvalue(mangrove) &lt;- -999\npts_mangrove &lt;- rasterToPoints(mangrove)\nplot(rasterFromXYZ(pts_mangrove))\n\nNAvalue(mangrove) &lt;- 0 \ndev.off()\n\nExporta tus datos - vamos a probar con el raster agregado\n\nKML(agg_mangrove, \"agg_mangrove.kml\", overwrite = TRUE)\nwriteRaster(agg_mangrove, \"agg_mangrove.tif\", format = \"GTiff\")\n\n¿Y los rasters de múltiples bandas? El paquete raster los maneja de la misma manera, solo que el atributo nbands() es &gt;1 - piensa en un array en lugar de una matriz.\n\nmultiband &lt;- raster(\"multiband.tif\")\nnbands(multiband)\nnrow(multiband)\nncol(multiband)\nncell(multiband)\n\nCrear nuestro propio raster multibanda?\n\nfor (i in 1:4) {\n  assign(x = paste0(\"band\", i), value = raster(ncol = 10, nrow = 10))\n}\nvalues(band1) &lt;- runif(100)\nvalues(band2) &lt;- runif(100)\nvalues(band3) &lt;- runif(100)\nvalues(band4) &lt;- runif(100)\nmultiband_stack &lt;- stack(list(band1, band2, band3, band4))\nnlayers(multiband_stack)\nplot(multiband_stack)\n\nGenerando una imagen RGB?\n\nplotRGB(multiband_stack, r = 1, g = 2, b = 3)\nrange(multiband_stack)\nplotRGB(multiband_stack, r = 1, g = 2, b = 3, scale = 1) \nplotRGB(multiband_stack, r = 3, g = 2, b = 1, scale = 1)\nplotRGB(multiband_stack, r = 2, g = 3, b = 4, scale = 1)\n\nOtras funciones de procesamiento útiles\n\n?crop\n?merge\n?trim\n?interpolate\n?reclassify\n?rasterToPolygons\n\nAlgunas funciones de análisis útiles\n\n?zonal \n?focal \n?calc \n?distance \n?sampleRandom\n?sampleRegular\n?sampleStratified\n\nHoy no entraremos en detalles sobre los sistemas de coordenadas y proyección, pero de manera muy breve, CRS() y crs() son las funciones/objetos clave.\n\ncrs(mangrove)\nproj4string(mangrove)\n\nlatlong &lt;- \"+init=epsg:4326\"\nCRS(latlong)\neastnorth &lt;- \"+init=epsg:3857\"\nCRS(eastnorth)\n\nlatlongs_mangrove &lt;- rasterToPoints(mangrove, spatial = T)\nlatlongs_mangrove\nprojected_pts_mangrove &lt;- spTransform(latlongs_mangrove, CRS(eastnorth))\nprojected_pts_mangrove\n\n\nEjercicios\n\nImporta el raster \"Landsat_TIR.tif\", que es una imagen TIR (infrarrojo térmico) del satélite Landsat 8 capturada sobre un área de cultivo.\nSupongamos que modelamos los valores TIR mediante regresión lineal para calcular la temperatura real en el suelo, y beta0 fue 0.5 y beta1 fue 0.1 (es decir, y = 0.1x + 0.5). Haz un mapa de temperatura (pista: ?calc, y deberás escribir una función).\nAgrega un título y etiquetas de ejes al gráfico, y utiliza colores que tengan sentido para la temperatura.\nCrea un raster coincidente (en extensión y número de píxeles, para la solución más fácil) con códigos de zona (para cada píxel), y luego calcula la temperatura media/desviación estándar en esas zonas (pista: ?values y ?zonal).\n\n\n\n\nAmpliando el análisis de rasters\nAhora hagamos un recorrido rápido por los tipos de análisis que podemos realizar, y con suerte, descubramos una comprensión más profunda del análisis de rasters en R.\nCarga algunos datos de SST (temperatura de la superficie del mar) - Feb 2013 para el globo (como un dato aparte, verifica este enlace para obtener más conjuntos de datos globales marinos geniales:\n\nsst_feb &lt;- raster(\"SST_feb_2013.img\")\nplot(sst_feb)\n\n\n\n\nRecórtalos al Pacífico para poder comparar los datos de manglar\n\npacific_extent &lt;- extent(mangrove) + 80 \npacific_extent \nsst_feb_crop &lt;- crop(sst_feb, pacific_extent) \nplot(sst_feb_crop)\n\nCarga los datos de SST promedio a largo plazo para febrero\n\nsst_feb_mn &lt;- raster(\"SST_feb_mean.img\")\nplot(sst_feb_mn)\nsst_mn_crop &lt;- crop(sst_feb_mn, pacific_extent)\nplot(sst_mn_crop)\n\nAhora vamos a crear un mapa de anomalía de SST\n\nsst_anomaly &lt;- sst_feb_crop - sst_mn_crop \nplot(sst_anomaly) \nplot(sst_anomaly, col = rev(heat.colors(\"100\"))) \ncontour(sst_anomaly, add = T) \n\nConsultar valores individuales,\n\nminValue(sst_anomaly) \nmaxValue(sst_anomaly) \nplot(sst_anomaly == maxValue(sst_anomaly))\n\no gráficos/estadísticas para toda la imagen,\n\nplot(sst_anomaly &gt; 1)\npar(mar = c(3, 3, 3, 3))\nhist(sst_anomaly, main = \"February SST Anomaly\", xlab = \"sst anomaly\")\n\n¡o vamos a ser un poco más ingeniosos!\n\nmax_anom &lt;- which.max(sst_anomaly)\nmax_xy &lt;- xyFromCell(sst_anomaly, max_anom)\nplot(sst_anomaly,\n  col = rev(heat.colors(\"100\")),\n  main = \"2013 Feb SST anomaly + hottest point\"\n)\npoints(max_xy, pch = 8, cex = 2)\n\n\n\n\n¿Muestrear puntos condicionalmente? Claro. Sin embargo, más adelante veremos una mejor forma de hacerlo.\n\nxy &lt;- xyFromCell(sst_anomaly, sample(1:ncell(sst_anomaly), 20)) \npoints(xy)\nextract(sst_feb, xy)\n\nPrueba también ?getValues. Bueno, recapitulemos cómo escribir de nuevo en el disco\n\nwriteRaster(sst_anomaly, \"sst_anomaly.tif\", format = \"GTiff\")\nKML(sst_anomaly, \"sst_anomaly.kml\")\nsave(sst_anomaly, file = \"sst_anomaly_feb.RData\")\nsave(sst_feb_mn, file = \"sst_feb_mn.RData\") \n\n¿Qué sucede con esos dos comandos save()? Algo más que debes entender sobre la forma en que el paquete raster maneja los archivos ráster es que, para los rásteres más grandes, no se almacena todo el archivo en la memoria, sino que es solo un puntero al archivo. Puedes probar si lo es o no.\n\ninMemory(sst_feb_mn) \ninMemory(sst_anomaly) \n\nVimos stack() anteriormente, y podemos usarlo para imágenes de múltiples bandas, pero también para apilar diferentes fuentes de información. brick() funciona de la misma manera, excepto que los objetos RasterBrick están diseñados para datos más pequeños, y un RasterBrick solo puede apuntar a un archivo, a diferencia de los objetos RasterStack, que pueden apuntar a varios archivos.\n\nsst_stack &lt;- stack(sst_mn_crop, sst_feb_crop, sst_anomaly)\nplot(sst_stack)\nnlayers(sst_stack)\nplot(sst_stack, 2)\nnames(sst_stack)[2] &lt;- \"SST_feb_2013\"\nnames(sst_stack)[3] &lt;- \"SST_anomaly\"\n\nAsí que podemos ver por qué eso podría ser útil para diversas aplicaciones de teledetección y modelado.\n\n\nModelado e interpolación\nAhora veamos un ejemplo rápido de lo que podemos hacer con rásteres en el contexto del modelado de distribución de especies y el modelado espacial. Primero, extraigamos algunos puntos aleatorios; asegúrate de haber ejecutado library(dismo).\n\nrpoints_sst &lt;- randomPoints(sst_stack, 500) \nplot(sst_stack, 2)\npoints(rpoints_sst, pch = 16, cex = 0.7)\nsst_samp &lt;- extract(sst_stack, rpoints_sst) \nstr(sst_samp)\nsst_samp &lt;- data.frame(sst_samp,\n  lat = rpoints_sst[, 2], lon = rpoints_sst[, 1]\n)\nplot(sst_samp$SST_anomaly ~ sst_samp$SST_feb_2013)\n# abline(lm(sst_samp$SST_anomaly ~ sst_samp$SST_feb_2013))\n# plot(mgcv::gam(sst_samp$SST_anomaly ~ s(sst_samp$SST_feb_2013)), resid = T)\n\n¿Y si tuviéramos algunos datos biológicos reales en esos puntos? Bueno, inventemos algunos y luego ajustemos un modelo a ellos.\n\nsst_samp$shark_abund &lt;- rpois(n = nrow(sst_samp), lambda = round(sst_samp$SST_feb_2013))\nplot(sst_samp$shark_abund ~ sst_samp$SST_feb_2013)\nshark_glm &lt;- glm(sst_samp$shark_abund ~ sst_samp$SST_feb_2013 + sst_samp$SST_anomaly,\n  data = sst_samp, family = \"poisson\"\n)\nsummary(shark_glm)\n\nPor lo general, usaríamos predict() en un objeto de ajuste de modelo, y también podemos usarlo de manera similar para predecir datos ráster.\n\nhead(predict(shark_glm, type = \"response\"))\nshark_predict &lt;- predict(shark_glm, new_data = sst_samp, type = \"response\")\nplot(shark_predict,\n  col = rev(rainbow(n = 10, start = 0, end = 0.3)),\n  main = \"Shark abundance as a function of SST\"\n)\n\n\n\n\n\n\nVamos a intentar algo diferente, digamos el riesgo de ataque. ¿No se trata solo de la abundancia, sino también de algo más, como su nivel de agresividad o promedio?\n\nsst_samp$shark_aggression &lt;- sst_samp$lat * -1 \nsst_samp$shark_attack &lt;- scale(sst_samp$shark_abund * sst_samp$shark_aggression)\nattack_lm &lt;- lm(shark_attack ~ SST_feb_2013 + SST_anomaly,\n  data = sst_samp\n)\nshark_attack &lt;- predict(sst_stack, attack_lm, type = \"response\")\nplot(shark_attack,\n  col = rev(rainbow(n = 10, start = 0, end = 0.3)),\n  main = \"Shark attack index!\"\n)\n\n\n\n\nEste es un ejemplo rápido y tonto. Si fuéramos menos perezosos, podríamos crear una nueva pila de imágenes con las predicciones de abundancia y los valores de latitud, extraer las muestras aleatorias nuevamente y volver a ajustar el modelo y las predicciones.\n\nEjercicios\n\nIntenta generar algunas estadísticas (valores o gráficos) para la anomalía de la SST en diferentes regiones, ya sea en todo el mundo o en Australia.\nPrueba algunas operaciones matemáticas de bandas o algunas declaraciones condicionales utilizando múltiples raster o un RasterStack.\nCrea otro escenario de SDM, ya sea utilizando datos descargados o datos totalmente simulados.\nSi solo tuviéramos los datos físicos en algunos puntos y quisiéramos convertirlos en un mapa de SST ponderado geográficamente o interpolado, podrías muestrear algunos de los puntos y luego usar library(gstat) para probar una interpolación IDW (ponderada por distancia inversa). Realiza algunas interpolaciones, variando la cantidad de puntos utilizados, y observa cómo afecta a tu producto interpolado.\n\n\n\n\nAlgo más para investigar\nAlgunos de los paquetes comúnmente utilizados para el análisis de datos espaciales.\n\nlibrary(sp) \nlibrary(maps)\nlibrary(rasterVis)\nlibrary(maptools)\nlibrary(mapproj)\nlibrary(rgeos) \n\nTambién podrías intentar obtener datos climáticos bioclimáticos utilizando la función getData() del paquete raster, o obtener varios tipos de mapas utilizando la función gmap() del paquete dismo o la función map del paquete maps. Si visitas la página de GitHub de este tutorial, encontrarás algunas respuestas a los ejercicios, además de algunos extras relacionados con las sugerencias anteriores.\nAutor: Mitchell Lyons\nAño: 2017\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/spatial-vis/index.html",
    "href": "graphics/spatial-vis/index.html",
    "title": "Datos espaciales",
    "section": "",
    "text": "R cuenta con una amplia variedad de funciones y paquetes para visualizar datos espaciales. Esta página enlazará con una serie de tutoriales para manejar datos espaciales y crear mapas.\n\nCreación de mapas simples con ggmap\nFundamentos de imagenes raster\nMapas interactivos con R"
  },
  {
    "objectID": "graphics/spatial-vis/interactive/index.html",
    "href": "graphics/spatial-vis/interactive/index.html",
    "title": "Mapas interactivos en R",
    "section": "",
    "text": "Ser capaz de producir mapas interactivos sobre la marcha puede acelerar considerablemente el análisis exploratorio y es una herramienta útil para mostrar datos que serían menos informativos en un mapa estático.\nLeaflet es una biblioteca de JavaScript de código abierto que se utiliza para crear mapas interactivos en sitios web. En esta publicación, vamos a ver el paquete leaflet para R y ¡crear algunos mapas interactivos geniales!"
  },
  {
    "objectID": "graphics/spatial-vis/interactive/index.html#instalación",
    "href": "graphics/spatial-vis/interactive/index.html#instalación",
    "title": "Mapas interactivos en R",
    "section": "Instalación",
    "text": "Instalación\nEl paquete leaflet de R se puede instalar desde CRAN ejecutando:\n\ninstall.packages(\"leaflet\")"
  },
  {
    "objectID": "graphics/spatial-vis/interactive/index.html#fundamentos",
    "href": "graphics/spatial-vis/interactive/index.html#fundamentos",
    "title": "Mapas interactivos en R",
    "section": "Fundamentos",
    "text": "Fundamentos\n¡Crear un mapa interactivo básico es simple!\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.3.2\n\nleaflet() %&gt;%\n  addTiles()\n\n\n\n\n\nEl paquete Leaflet de R ha sido diseñado para ser utilizado con el operador pipe (%&gt;%), lo que facilita la adición de capas y controles como una barra de escala y un mini mapa.\nSin embargo, la mayoría de las veces tendremos un área o un sitio de estudio en el que estamos interesados y que queremos ver:\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addScaleBar() %&gt;%\n  setView(lng = 151.2, lat = -33.86, zoom = 10) %&gt;%\n  addMiniMap()"
  },
  {
    "objectID": "graphics/spatial-vis/interactive/index.html#marcadores",
    "href": "graphics/spatial-vis/interactive/index.html#marcadores",
    "title": "Mapas interactivos en R",
    "section": "Marcadores",
    "text": "Marcadores\nVeamos algunos datos de ocurrencia de especies de GBIF utilizando el paquete rgbif: Mostraremos todas las observaciones de eucaliptos dentro de la región de los Macquarie Marshes:\n\n# install.packages('gbif')\nlibrary(rgbif)\ngbif_query &lt;- occ_search(\n  genusKey = 7493935,\n  geometry = rgbif::gbif_bbox2wkt(minx = 147.8, miny = -30.6, maxx = 147.4, maxy = -31)\n)\neuc &lt;- gbif_query$data\neuc$label &lt;- paste(euc$name, \"|\", euc$vernacularName, \"|\", euc$year, \"-\", month.abb[euc$month])\n## \nbase &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  addScaleBar() %&gt;%\n  setView(lat = mean(euc$decimalLatitude), lng = mean(euc$decimalLongitude), zoom = 10)\n\nbase %&gt;% addMarkers(\n  lng = euc$decimalLongitude, lat = euc$decimalLatitude,\n  label = euc$label\n)"
  },
  {
    "objectID": "graphics/spatial-vis/interactive/index.html#agrupamiento-de-marcadores",
    "href": "graphics/spatial-vis/interactive/index.html#agrupamiento-de-marcadores",
    "title": "Mapas interactivos en R",
    "section": "Agrupamiento de marcadores",
    "text": "Agrupamiento de marcadores\n¡Bien! pero está un poco desordenado, podemos agregar agrupamiento especificando el argumento clusterOptions para intentar solucionar este problema:\n\nbase %&gt;%\n  addMarkers(\n    lng = euc$decimalLongitude, lat = euc$decimalLatitude,\n    clusterOptions = markerClusterOptions(),\n    label = euc$label\n  )"
  },
  {
    "objectID": "graphics/spatial-vis/interactive/index.html#colorear-marcadores",
    "href": "graphics/spatial-vis/interactive/index.html#colorear-marcadores",
    "title": "Mapas interactivos en R",
    "section": "Colorear marcadores",
    "text": "Colorear marcadores\nTambién puedes agregar marcadores circulares que pueden tener colores personalizados y agregar una leyenda:\n\ncolor_function &lt;- colorFactor(\"RdYlBu\", domain = NULL)\n\nbase %&gt;%\n  addCircleMarkers(\n    lng = euc$decimalLongitude, lat = euc$decimalLatitude,\n    color = color_function(euc$name),\n    label = euc$label\n  ) %&gt;%\n  addLegend(pal = color_function, values = euc$name)"
  },
  {
    "objectID": "graphics/spatial-vis/interactive/index.html#otras-cosas-para-probar",
    "href": "graphics/spatial-vis/interactive/index.html#otras-cosas-para-probar",
    "title": "Mapas interactivos en R",
    "section": "¡Otras cosas para probar!",
    "text": "¡Otras cosas para probar!\n\naddMeasure() agrega una regla y un control de estimación de área al mapa.\naddProviderTiles() Otros tiles (mapas base) se pueden agregar utilizando esta función. ¡Prueba: leaflet() %&gt;% addProviderTiles(provider = providers$CartoDB)!\naddLayersControl() agrega un selector para elegir múltiples capas si las has agregado.\naddRasterImage() ¡crea una superposición de imagen a partir de datos ráster!\naddGeoJSON() agrega polígonos GeoJSON al mapa interactivo."
  },
  {
    "objectID": "graphics/spatial-vis/interactive/index.html#recursos",
    "href": "graphics/spatial-vis/interactive/index.html#recursos",
    "title": "Mapas interactivos en R",
    "section": "Recursos:",
    "text": "Recursos:\n\nGuía de leaflet de RStudio\nrgbif\n\nAutor: John Wilshire"
  },
  {
    "objectID": "graphics/spatial-vis/simple-maps/index.html",
    "href": "graphics/spatial-vis/simple-maps/index.html",
    "title": "Mapas simples con ggmap",
    "section": "",
    "text": "Saber hacer un mapa simple es una habilidad muy útil para todo tipo de aplicaciones. Aquí, introduciremos algunos conceptos básicos para trabajar con datos espaciales y utilizaremos R para producir mapas simples y fáciles. Podrías utilizar la misma técnica para crear un mapa para una presentación o una figura para un informe o publicación.\nExisten muchos paquetes útiles para graficar y manipular datos espaciales en R. Para simplificar, vamos a utilizar únicamente el paquete ggmap en este ejercicio. Para comenzar, instala este paquete y cárgalo en R.\nlibrary(ggmap)"
  },
  {
    "objectID": "graphics/spatial-vis/simple-maps/index.html#configuración-de-ggmap",
    "href": "graphics/spatial-vis/simple-maps/index.html#configuración-de-ggmap",
    "title": "Mapas simples con ggmap",
    "section": "Configuración de ggmap",
    "text": "Configuración de ggmap\nEl paquete ggmap “facilita la obtención de teselas de mapas raster de servicios populares de mapas en línea como Google Maps y Stamen Maps, y su representación utilizando el marco de trabajo ggplot2”.\nPara utilizar el paquete, necesitarás configurar una clave de API de Google Maps. Para obtener información sobre cómo hacer esto, consulta la página principal del paquete en GitHub. También se puede utilizar una interfaz alternativa a OpenStreetMap con funciones ligeramente diferentes.\n\nCreando tu primer mapa\nEn este ejemplo, supongamos que hemos recopilado datos de cinco sitios cerca del Parque Nacional Royal en Nueva Gales del Sur, Australia, y queremos mostrar las ubicaciones que visitamos.\nUtilizando la función get_map, podemos descargar una imagen de Google Maps en cualquier ubicación especificada por su nombre:\n\nbasemap &lt;- get_map(\"Royal National Park\", zoom = 12)\n\ny luego representarla con la función ggmap:\n\nggmap(basemap)\n\n\n\n\nEl argumento de get_map es simplemente cualquier lugar que se pueda buscar en Google Maps (en este caso, ‘Parque Nacional Royal’) - ¡fácil!\nPara agregar datos de puntos para nuestros cinco sitios de estudio, primero necesitamos crear un nuevo marco de datos que contenga el nombre del lugar, la latitud y la longitud de cada sitio (o importar estos datos desde una hoja de cálculo).\n\nptdata &lt;- data.frame(\n  \"PlaceName\" = c(\n    \" Hacking R. \", \" Cabbage Tree Ck. \",\n    \" Marley Ck. \", \" Wattamolla Ck. \",\n    \" Waratah R. \"\n  ),\n  \"Long\" = c(\n    151.054823, 151.124566, 151.139216,\n    151.115645, 150.960431\n  ),\n  \"Lat\" = c(\n    -34.074918, -34.090796, -34.111972,\n    -34.136046, -34.172308\n  )\n)\n\nPodemos agregar estos puntos a nuestro mapa basándonos en las coordenadas de latitud (y) y longitud (x). Es importante que los puntos y las otras características del mapa funcionen en la misma escala, o ‘proyección’. Estas ubicaciones se estimaron a partir de datos de Google Maps por esta razón, puedes estimar las tuyas propias a través de LatLong.net).\n\nggmap(basemap) + geom_point(data = ptdata, aes(x = Long, y = Lat))\n\n\n\n\n¡Bien hecho! ¡Has creado tu primer mapa con R! ¡Fácil! Puede que hayas notado que ggmap utiliza código gráfico familiar del paquete ggplot (consulta plotting with ggplot).\n\n\nCreación de un mapa listo para presentación\nAjustando algunas opciones, podemos hacer que se vea más adecuado para una presentación.\nPrimero, ajusta algunas opciones en get_map:\n\nusa bound para elegir la extensión / cuadro delimitador del mapa. Aquí, estamos utilizando los valores mínimo y máximo de latitud y longitud de nuestras coordenadas del sitio.\nprueba maptype = \"hybrid\" para ver imágenes de satélite y datos de carreteras.\naumenta el nivel de zoom para mejorar los detalles.\n\n\nbound &lt;- c(\n  left = min(ptdata$Long) - 0.05, bottom = min(ptdata$Lat) - 0.05,\n  right = max(ptdata$Long) + 0.05, top = max(ptdata$Lat) + 0.05\n)\n\n\nPres_basemap &lt;- get_map(bound, zoom = 11, maptype = \"hybrid\")\n\nSegundo, ajusta algunas opciones para la representación gráfica:\nAjustes de geom_point para los puntos en cada ubicación del sitio: * usa colour = \"white\" para que los puntos destaquen. * usa size para cambiar el tamaño de los puntos.\nAjustes de geom_text para las etiquetas de cada sitio: * usa size para cambiar el tamaño del texto. * usa colour = \"white\" para que las etiquetas destaquen. * usa hjust y vjust para ajustar la posición horizontal y vertical de las etiquetas. * usa labs para definir las etiquetas de los ejes x e y.\n\nggmap(Pres_basemap) +\n  geom_point(\n    data = ptdata, aes(x = Long, y = Lat),\n    colour = \"white\"\n  ) +\n  geom_text(\n    data = ptdata,\n    aes(x = Long, y = Lat, label = PlaceName),\n    size = 4,\n    colour = \"white\",\n    hjust = \"inward\"\n  ) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\n\n\n\n\n\nCreación de un mapa listo para publicación\nPara un artículo o informe, probablemente desees un mapa más simple en blanco y negro.\n\nusa bound para establecer la extensión como se mencionó anteriormente.\nprueba get_stamenmap para obtener un mapa base austero (por ejemplo, Stamen “toner-lite”).\nusa theme_bw() de ggplot para obtener un tema despojado.\n\n\nPub_basemap &lt;- get_stamenmap(bound, zoom = 13, maptype = \"toner-lite\")\n\n\nggmap(Pub_basemap) +\n  geom_point(data = ptdata, aes(x = Long, y = Lat, label = PlaceName)) +\n  geom_text(\n    data = ptdata,\n    aes(x = Long, y = Lat, label = PlaceName),\n    size = 4,\n    fontface = \"bold\",\n    hjust = \"right\"\n  ) +\n  theme_bw() +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\nMás ayuda\nLee la documentación de ggmap para obtener más ejemplos e ideas.\nConsulta la hoja de referencia de ggmap del Centro Nacional para el Análisis y la Síntesis Ecológica.\nAutor: Kingsley Griffin, ediciones de Daniel Falster\nAño: 2017\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Una introducción al Manejo de Datos, producción de Gráficos y desarrollo de Análisis Estadístico en Ciencias Ambientales",
    "section": "",
    "text": "Los estudiantes e investigadores en el campo de las ciencias ambientales necesitan adquirir una amplia variedad de habilidades cuantitativas en el uso de software analíticos y herramientas de procesamiento de datos. Entre estas habilidades se incluyen el dominio de R, sistemas de información geográfica (SIG) y el procesamiento de datos remotos. Es cada vez más importante asegurar la transparencia en el procesamiento de datos, respaldado por un análisis estadístico sólido, para respaldar las conclusiones de la investigación científica y los monitoreos utilizados en la gestión y formulación de políticas. En este sitio web, ofrecemos una breve introducción a las técnicas de organización de datos, creación de gráficos y análisis estadísticos para apoyar a los profesionales en este campo.\n\n\n\n\nObjetivos:\nEste sitio presenta una breve introducción a técnicas para la organización de datos, generación de gráficos y análisis de datos, que incluye los siguientes temas:\n\nConsideraciones para organizar datos durante la entrada de datos.\nCómo manipular y administrar datos para garantizar la transparencia y control de versiones.\nInstrucciones para ejecutar análisis estadístico con código de muestra en R u otro software relevante (Introducción a R).\nLa justificación de la técnica de los diferentes análisis estadísticos en lenguaje sencillo (por qué se usa).\nCómo interpretar los resultados y las suposiciones que se deben comprobar.\nVisualización de los resultados y formas de comunicar eficazmente los resultados en informes o publicaciones científicas.\nDónde encontrar ayuda adicional.\n\n\n\nDesarrollo del contenido\nEste sitio web integra diferentes técnicas utilizadas por investigadores y posgraduados de la Universidad de Nueva Gales del Sur, Australia, quienes han proporcionado sus conocimientos y habilidades para ayudar a otros a desarrollar habilidades en el procesamiento y análisis de datos.\nLos recursos que se presentan fueron desarrollados inicialmente para ayudar en la enseñanza de habilidades cuantitativas a estudiantes de pregrado y posgrado en la Escuela de Ciencias Biológicas, Terrestres y Ambientales en la Universidad de Nueva Gales del Sur, con el apoyo de una subvención de Innovación en Enseñanza y Aprendizaje otorgada a Alistair Poore, Will Cornwell, Iain Suthers y Richard Kingsford. El desarrollo de la segunda generación de este sitio fue posible gracias al apoyo del Centro de Investigación en Evolución y Ecología.\n\n\nEditores del sitio:\n\nProfesor Alistair Poore\nProfesor Asociado Will Cornwell\nProfesor Asociado Danial Falster\nDr. Fonti Kar\n\n\n\nAutores de la página:\nKeryn Bain, Rachel Blakey, Stephanie Brodie, Corey Callaghan, Will Cornwell, Kingsley Griffin, Matt Holland, James Lavender, Andrew Letten, Shinichi Nakagawa, Shaun Nielsen, Alistair Poore, Gordana Popovic, Fiona Robinson y Jakub Stoklosa.\n\n\nColaboradores\n¡Gracias a ellos  por hacer de los programas de código abierto un lugar mejor!"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Environmental Computing",
    "section": "",
    "text": "Attribution 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More considerations\n for the public: \nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "statistics/categorical/contingency-tables/index.html",
    "href": "statistics/categorical/contingency-tables/index.html",
    "title": "Tablas de contingencia",
    "section": "",
    "text": "Las tablas de contingencia se utilizan para probar asociaciones entre dos o más variables categóricas. Los datos toman la forma de recuentos de observaciones que han sido clasificadas por estas variables categóricas.\n\nPor ejemplo, considera los colores del pico y la cara del ave en peligro de extinción Gouldian Finch (Erythrura gouldiae) que vive en las sabanas del norte de Australia. Son polimórficos, teniendo caras de color rojo, amarillo o negro. Sus picos también son de color rojo, amarillo o negro, pero el color del pico no siempre es igual al color de la cara. Todas las aves en una muestra podrían ser clasificadas en las nueve combinaciones posibles de color de cara y color de pico, y los recuentos de cada combinación se mantendrían en una tabla bidimensional:\n\n\n            Black face Red face Yellow face\nBlack bill          16        5           6\nRed bill            19       20           6\nYellow bill         18       22          22\n\n\nPara probar si una variable está asociada con la otra, contrastamos los recuentos observados con los recuentos esperados si no hubiera asociación (es decir, si las dos variables fueran completamente independientes entre sí). En este ejemplo, estaríamos probando si el color del pico varía de forma independiente al color de la cara.\nLos recuentos esperados bajo este modelo nulo se calculan a partir de los totales de filas y columnas de la tabla que contiene los datos observados. Para cada celda de la tabla, el recuento esperado es:\n(total de la fila x total de la columna) / total general\nCuando hacemos esto para cada celda, obtenemos una nueva tabla con todos los valores esperados:\n\n\n            Black face  Red face Yellow face\nBlack bill    10.67910  9.470149    6.850746\nRed bill      17.79851 15.783582   11.417910\nYellow bill   24.52239 21.746269   15.731343\n\n\nEstos son los recuentos esperados si la hipótesis nula es verdadera. En este ejemplo, si el número de aves con cara negra, roja y amarilla está en las mismas proporciones para cada uno de los colores del pico.\nLuego, los recuentos observados se contrastan con los recuentos esperados utilizando la estadística de prueba \\(\\chi^2\\).\n\\(\\chi^{2} = \\sum_{i=1}^{k} \\frac{(O_{i}-E_{i})^2}{E_{i}}\\)\ndonde O y E son los números observados y esperados en cada una de las celdas de la tabla.\n\nEjecutando el análisis\nCon una calculadora, podrías calcular \\(\\chi^2\\) y luego determinar la probabilidad de obtener ese valor de \\(\\chi^2\\) a partir de una tabla de la distribución de probabilidad conocida de \\(\\chi^2\\). En R, podemos obtener esa probabilidad de la siguiente manera:\n\npchisq(x, df, lower.tail = FALSE)\n\ncon x = tu valor de \\(\\chi^2\\), y grados de libertad (df) = (número de filas-1) x (número de columnas-1). La parte lower.tail = FALSE te da la probabilidad de que \\(\\chi^2\\) sea mayor que tu valor.\nMás fácil es ejecutar todo el análisis en R. Primero, debes ingresar las frecuencias observadas como una matriz.\n\nGfinch &lt;- matrix(c(16, 19, 18, 5, 20, 22, 6, 6, 22), nrow = 3, dimnames = list(c(\"Black bill\", \"Red bill\", \"Yellow bill\"), c(\"Black face\", \"Red face\", \"Yellow face\")))\n\nnrow le indica a R cuántas filas tienes. dimnames etiqueta las filas y columnas.\nVerifica que has ingresado los datos correctamente simplemente ingresando el nombre de la matriz que acabas de crear.\n\nGfinch\n\nEjecuta el análisis de contingencia con la función chisq.test.\n\nchisq.test(x, correct = F)\n\ndonde x es el nombre de la matriz que contiene los datos observados (para este ejemplo, usa el objeto Gfinch).\n\n\nInterpretando los resultados\nLa salida de una tabla de contingencia es muy simple: el valor de \\(\\chi^2\\), los grados de libertad y el valor p. El valor p proporciona la probabilidad de que tus frecuencias observadas provengan de una población donde la hipótesis nula era verdadera.\n\n\n\n    Pearson's Chi-squared test\n\ndata:  Gfinch\nX-squared = 12.881, df = 4, p-value = 0.01187\n\n\nEn este ejemplo, la probabilidad de que las frecuencias observadas provengan de una población donde la hipótesis nula era verdadera es 0.01187. Luego, rechazaríamos la hipótesis nula y concluiríamos que existe una asociación entre el color del pico y el color de la cara.\nEs importante recordar que esto no está probando ninguna de las variables por separado (por ejemplo, si las aves de cara negra se encuentran más comúnmente que las aves de cara roja o amarilla), sino una asociación entre ambas variables (es decir, si el color del pico es independiente del color de la cara).\nPara explorar qué celdas de la tabla tienen más observaciones de las esperadas o menos observaciones de las esperadas, observa los residuos estandarizados. Estos son las diferencias entre los valores observados y esperados, estandarizados por la raíz cuadrada de los valores esperados. Están estandarizados porque la comparación de las diferencias absolutas (observado - esperado) puede ser engañosa cuando el tamaño de los valores esperados varía. Por ejemplo, una diferencia de 5 a partir de una expectativa de 10 es un aumento del 50%, pero una diferencia de 5 a partir de una expectativa de 100 es solo un cambio del 5%.\n\nchisq.test(Gfinch)$residuals\n\n            Black face    Red face Yellow face\nBlack bill    1.628236 -1.45259188  -0.3250357\nRed bill      0.284793  1.06130659  -1.6033875\nYellow bill  -1.317120  0.05441038   1.5804894\n\n\nEn este ejemplo, se puede observar que las aves con cara negra tienen más probabilidades de tener un pico negro de lo que se esperaría por casualidad, y menos probabilidades de tener un pico amarillo.\n\n\nSupuestos a verificar\nIndependencia. La prueba de \\(\\chi^2\\) asume que las observaciones se clasifican en cada categoría de manera independiente entre sí. Esto es un problema de diseño de muestreo y generalmente se evita mediante el muestreo aleatorio. En este ejemplo, habría problemas si eligieras deliberadamente aves con una combinación de color de pico y cara que no estuviera presente en las aves ya muestreadas.\nTamaño de la muestra. La estadística \\(\\chi^2\\) solo se puede comparar de manera confiable con la distribución \\(\\chi^2\\) si los tamaños de muestra son lo suficientemente grandes. Debes verificar que al menos el 20% de las frecuencias esperadas sean mayores que 5. Puedes ver las frecuencias esperadas para cada categoría al agregar $expected al final de tu prueba de \\(\\chi^2\\). Por ejemplo,\n\nchisq.test(Gfinch)$expected\n\nSi este supuesto no se cumple, puedes combinar categorías (si tienes más de dos y tiene sentido hacerlo), realizar una prueba de aleatorización o considerar un modelo log-lineal.\n\n\nComunicación de los resultados\nEscrito. Los resultados del análisis de una tabla de contingencia se pueden presentar fácilmente en el texto de la sección de resultados. Por ejemplo, “Hubo una asociación significativa entre el color del pico y el color de la cara de los pinzones Gouldianos (\\(\\chi^2\\) = 12.88, df = 2, P = 0.01).”\nVisual. Los datos de recuento se presentan mejor en un gráfico de barras con los recuentos en el eje Y y las categorías en el eje X.\n\nbarplot(Gfinch, beside = T, ylab = \"Count\", xlab = \"Face colour\", col = c(\"black\", \"red\", \"yellow\"))\nlegend(\"topright\", inset = 0.15, c(\"Black bill\", \"Red bill\", \"Yellow bill\"), pch = 15, col = c(\"black\", \"red\", \"yellow\"))\n\n\n\n\nConsulta los módulos de visualización para obtener versiones mejoradas de estas figuras que sean adecuadas para informes o publicaciones.\n\n\nMás ayuda\nEscribe chisq.test para obtener la ayuda de R para esta función.\nQuinn y Keough (2002) **Experimental design and data analysis for biologists*. Cambridge University Press. Capítulo 14. Análisis de frecuencias.\nMcKillup (2012) Statistics explained. An introductory guide for life scientists.. Cambridge University Press. Capítulo 20.3 Comparación de proporciones entre dos o más muestras independientes.\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/categorical/fishers-exact/index.html",
    "href": "statistics/categorical/fishers-exact/index.html",
    "title": "Prueba Exacta de Fisher",
    "section": "",
    "text": "Algunas personas perciben un olor distintivo en su orina después de comer espárragos, mientras que otras nunca perciben el olor. Estas diferencias podrían surgir de la variación entre las personas en el perfil químico de la orina (es decir, cómo se metabolizan los compuestos del espárrago) o de la variación en la capacidad de diferentes personas para detectar el olor.\nUn artículo (Pelchat et al. 2010 Chemical Senses) revisó estos estudios y presentó los siguientes datos que describen la variación entre cuatro poblaciones de estudio:\nIsrael (Lison et al. 1980) | China (Hoffenberg 1983) | EE. UU. (Sugarman and Neelon 1985) | EE. UU. (Lison et al. 1980)\n\nasparagus &lt;- matrix(c(328, 96, 10, 11, 0, 2, 5, 10), nrow = 4, dimnames = list(c(\"Israel\", \"China\", \"USA.1\", \"USA.2\"), c(\"Can detect odour\", \"Cannot detect odour\")))\nasparagus\n\n       Can detect odour Cannot detect odour\nIsrael              328                   0\nChina                96                   2\nUSA.1                10                   5\nUSA.2                11                  10\n\n\nQ1 ¿Cuántas personas se esperaría que perciban el olor en cada población si todas las poblaciones de estudio tuvieran la misma proporción de personas capaces de detectar el olor?\n\nchisq.test(asparagus, correct = F)$expected\n\n       Can detect odour Cannot detect odour\nIsrael        315.93074          12.0692641\nChina          94.39394           3.6060606\nUSA.1          14.44805           0.5519481\nUSA.2          20.22727           0.7727273\n\n\nQ2 ¿Qué prueba estadística podrías usar para detectar diferencias entre las poblaciones en la percepción del olor? Asegúrate de verificar que se cumplan las suposiciones de la prueba.\nQ3 Realiza la prueba seleccionada. ¿Cuál es el valor de p obtenido?\n\nfisher.test(asparagus)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  asparagus\np-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nEres un ecólogo conductual que estudia la dieta de los possums cola de anillo comunes (Pseudocheirus peregrinus) en dos sitios (sitios A y B) dominados por dos especies de Eucalyptus (E. ovata y E. sideroxylon).\nObservas que los possums en el sitio A tienden a comer E. ovata, mientras que en el sitio B principalmente comen E. sideroxylon. Para probar si las poblaciones en cada sitio difieren en las preferencias alimentarias, colocas collares de radio a siete possums en el sitio A y ocho possums en el sitio B. Sigues a cada possum y anotas la especie del primer árbol del cual observas que come hojas.\nEn el sitio A, ves que seis possums comen E. ovata y uno come E. sideroxylon. En el sitio B, ninguno come E. ovata y ves que los ocho comen E. sideroxylon.\nQ1 ¿Cuál es la probabilidad exacta de observar este patrón o uno más extremo por pura casualidad?\n\npossum &lt;- matrix(c(6, 0, 1, 8), nrow = 2, dimnames = list(c(\"Site A\", \"Site B\"), c(\"E. ovata\", \"E. sideroxylon\")))\npossum\nfisher.test(possum)\n\nQ2 ¿Existe evidencia que sugiera que los zarigüeyas en los sitios A y B difieren en su uso de los árboles alimentarios?\nAutor: Alistair Poore"
  },
  {
    "objectID": "statistics/categorical/goodness-of-fit/index.html",
    "href": "statistics/categorical/goodness-of-fit/index.html",
    "title": "Prueba de bondad de ajuste",
    "section": "",
    "text": "Las pruebas de bondad de ajuste \\(\\chi^2\\) se utilizan para comprobar si las frecuencias observadas en dos o más categorías difieren de las esperadas bajo un modelo dado. Por ejemplo, ¿cuál es la probabilidad de que una muestra de 60 mujeres y 40 hombres en una clase provenga de una población donde la proporción de sexos sea en realidad de 1:1? En este ejemplo, hay una variable categórica única de sexo, con dos categorías: masculino y femenino.\n\nLa estadística de prueba es:\n\\(\\chi^{2} = \\sum_{i=1}^{k} \\frac{(O_{i}-E_{i})^2}{E_{i}}\\)\ndonde O y E son los números observados y esperados en cada una de las categorías de 1 a k.\nLos números observados provienen de tus observaciones reales, en este ejemplo 60 y 40. Los números esperados son obtenidos a partir de una expectativa teórica de las frecuencias bajo el modelo que se está probando. En este ejemplo, si estuvieras probando una proporción esperada de hombres y mujeres de 1:1, entonces esperarías tener 50 mujeres y 50 hombres en una muestra de 100 personas. Para este ejemplo,\n\\(\\chi^2 = \\frac{(60-50)^2}{50}+\\frac{(40-50)^2}{50}\\)\n\nEjecutando el análisis\nPuedes calcular \\(\\chi^2\\) fácilmente con una calculadora. Luego tendrías que determinar la probabilidad de obtener ese valor de \\(\\chi^2\\) a partir de la distribución de probabilidad conocida de \\(\\chi^2\\).\n\npchisq(x, df, lower.tail = FALSE)\n\ncon x = el valor de \\(\\chi^2\\) y grados de libertad (df) = número de categorías - 1. La parte lower.tail = FALSE te da la probabilidad de que \\(\\chi^2\\) sea mayor que tu valor.\nAlternativamente, puedes hacer todo esto de una sola vez con la función chisq.test.\n\nchisq.test(x, p)\n\ndonde x = los datos observados (es decir, las frecuencias en cada categoría) y p son las probabilidades esperadas para cada categoría.\nEn este ejemplo, usaríamos:\n\nchisq.test(x = c(60, 40), p = c(0.5, 0.5))\n\ndonde x es el rango de números observados y p tiene las probabilidades esperadas.\nEs importante destacar que debes utilizar los conteos reales como tus datos observados, no sus proporciones (es decir, 60 y 40, no 0.6 y 0.4). Esto tiene sentido si comprendes que una proporción de sexos de 6:4 en una muestra de 10 personas es más probable que ocurra por casualidad al muestrear de una proporción igual de sexos que una proporción de 600:400 en una muestra de 1000 personas.\nNo estás limitado a solo dos categorías, ni a una expectativa de que los conteos en cada categoría sean iguales. Por ejemplo, para probar si los conteos 10, 20 y 70 en tres categorías provienen de una población con frecuencias esperadas de 0.25, 0.25 y 0.5, utilizarías:\n\nchisq.test(x = c(10, 20, 70), p = c(0.25, 0.25, 0.5))\n\n\n\nInterpretación de los resultados\nEl resultado de una prueba de bondad de ajuste es muy simple: el valor de \\(\\chi^2\\), los grados de libertad (número de categorías - 1) y el valor p. El valor p indica la probabilidad de que tus conteos observados provengan de una población con las frecuencias esperadas que especificaste.\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(60, 40)\nX-squared = 4, df = 1, p-value = 0.0455\n\n\nEn el ejemplo de la proporción de sexos, deberías haber obtenido un valor p de 0.0455, lo que nos indica que es poco probable obtener una muestra de 60 mujeres y 40 hombres de una población con una proporción de sexos igual. Concluiríamos entonces que es probable que se hayan muestreado de una población que no tiene una proporción de sexos igual.\nPara explorar qué categorías tienen más observaciones de las esperadas o menos observaciones de las esperadas, observa los residuos estandarizados.\n\nchisq.test(x = c(60, 40), p = c(0.5, 0.5))$residuals\n\nEstos son las diferencias entre lo observado y lo esperado, estandarizadas por la raíz cuadrada de lo esperado. Están estandarizadas porque cualquier contraste de las diferencias absolutas (observado - esperado) puede ser engañoso cuando el tamaño de los valores esperados varía. Por ejemplo, una diferencia de 5 en comparación con una expectativa de 10 representa un aumento del 50%, pero una diferencia de 5 en comparación con una expectativa de 100 es solo un cambio del 5%.\nExplorar los residuos se vuelve importante cuando hay más de dos categorías en la prueba, ya que la prueba de \\(\\chi^2\\) solo te dirá si las frecuencias observadas difieren de las esperadas en todas las categorías, no en qué categoría en particular está sobre o subrepresentada.\n\n\nSupuestos a verificar\nIndependencia. La prueba de \\(\\chi^2\\) asume que las observaciones se clasifican en cada categoría de forma independiente entre sí. Este es un problema de diseño de muestreo y generalmente se evita mediante un muestreo aleatorio. En el ejemplo de la proporción de sexos, habría problemas si eligieras deliberadamente mujeres para agregar a tu muestra si crees que ya tienes suficientes hombres.\nTamaño de muestra. La estadística de \\(\\chi^2\\) solo se puede comparar de manera confiable con la distribución de \\(\\chi^2\\) si los tamaños de muestra son lo suficientemente grandes. Debes verificar que al menos el 20% de las frecuencias esperadas sean mayores que 5. Puedes ver los conteos esperados para cada categoría al agregar $expected al final de tu prueba de \\(\\chi^2\\). Por ejemplo,\n\nchisq.test(x = c(60, 40), p = c(0.5, 0.5))$expected\n\nSi no se cumple esta suposición, puedes combinar categorías (si tienes más de dos), realizar una prueba de aleatorización o considerar el modelado log-lineal.\n\n\nComunicación de los resultados\nEscrito. Los resultados de una prueba de bondad de ajuste \\(\\chi^2\\) se pueden presentar fácilmente en el texto de la sección de resultados. Por ejemplo, “La proporción de sexos de la clase de 100 estudiantes difirió significativamente de una proporción 1:1 (”\\(\\chi^2\\) = 4, gl = 1, P = 0.0455)“.\nVisual. Los datos de conteo se presentan mejor como un gráfico de barras con los conteos en el eje Y y las categorías en el eje X.\n\nbarplot(c(60, 40), xlab = \"Sex\", ylab = \"Count\", names = c(\"Female\", \"Male\"))\n\n\n\n\nConsulta los módulos de gráficos para obtener versiones mejoradas de estas figuras adecuadas para informes o publicaciones.\n\n\nMás ayuda\nEscribe ?chisq.test para obtener la ayuda de R sobre esta función.\nQuinn y Keough (2002) Experimental design and data analysis for biologists. Cambridge University Press. Capítulo 14. Análisis de frecuencias.\nMcKillup (2012) Statistics explained. An introductory guide for life scientists.. Cambridge University Press. Capítulo 20.2. Comparación de frecuencias observadas y esperadas: la prueba de chi-cuadrado de bondad de ajuste.\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/categorical/index.html",
    "href": "statistics/categorical/index.html",
    "title": "Análisis de datos categóricos",
    "section": "",
    "text": "Algunas pruebas comúnmente utilizadas para contrastar las frecuencias de observaciones entre variables categóricas.\n\nPruebas de bondad de ajuste\n\nTablas de contingencia"
  },
  {
    "objectID": "statistics/gams/index.html",
    "href": "statistics/gams/index.html",
    "title": "Modelos Aditivos Generalizados (GAM)",
    "section": "",
    "text": "Muchos datos en las ciencias ambientales no se ajustan a modelos lineales simples y se describen mejor mediante “modelos oscilantes”, también conocidos como Modelos Aditivos Generalizados (GAM, por sus siglas en inglés).\nComencemos con un famoso tweet de Gavin Simpson, que se resume en: 1. Los GAM son solo GLM. 2. Los GAM ajustan términos oscilantes. 3. Usa + s(x) en lugar de x en tu sintaxis. 4. Usa method = \"REML\". 5. Siempre revisa gam.check().\nBásicamente, esto es todo: una extensión de los modelos lineales generalizados (GLM) con una función de suavizado. Por supuesto, puede haber muchas cosas sofisticadas sucediendo cuando ajustas un modelo con términos suaves, pero solo necesitas comprender la justificación y un poco de teoría básica. También hay muchas cosas aparentemente mágicas sucediendo cuando intentamos entender lo que sucede detrás de escena en, por ejemplo, lmer o glmer, ¡pero los usamos todo el tiempo sin reservas!"
  },
  {
    "objectID": "statistics/gams/index.html#un-ejemplo-real-rápido",
    "href": "statistics/gams/index.html#un-ejemplo-real-rápido",
    "title": "Modelos Aditivos Generalizados (GAM)",
    "section": "Un ejemplo real rápido",
    "text": "Un ejemplo real rápido\n\nAhora veremos un ejemplo real rápido. Solo rascaremos la superficie, y en un tutorial futuro lo analizaremos en más detalle. Vamos a analizar algunos datos de CO\\(_2\\) de Mauna Loa. Ajustaremos un par de GAM al conjunto de datos para tratar de separar las tendencias intra e interanuales.\nPrimero, carga los datos, puedes descargarlos aquí.\n\nCO2 &lt;- read.csv(\"mauna_loa_co2.csv\")\n\nQueremos analizar la tendencia interanual primero, así que convirtamos la fecha en una variable de tiempo continua (tomaremos un subconjunto para la visualización).\n\nCO2$time &lt;- as.integer(as.Date(CO2$Date, format = \"%d/%m/%Y\"))\nCO2_dat &lt;- CO2\nCO2 &lt;- CO2[CO2$year %in% (2000:2010), ]\n\nAhora, vamos a graficar y observar un término suave para el tiempo.\n\\[y = \\beta_0 + f_{\\mathrm{tendencia}}(tiempo) + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2)\\]\n\nggplot(CO2_dat, aes(time, co2)) +\n  geom_line()\n\n\n\n\nPodemos ajustar un GAM para estos datos usando:\n\nCO2_time &lt;- gam(co2 ~ s(time), data = CO2, method = \"REML\")\n\nlo cual ajusta un modelo con un único término suave para el tiempo. Podemos observar los valores predichos para esto:\n\nplot(CO2_time)\n\n\n\n\nObserva cómo el término suave se reduce realmente a un término lineal ‘normal’ aquí (con un edf de 1): eso es lo bueno de los splines de regresión penalizados. Pero si revisamos el modelo, veremos que algo no está bien.\n\npar(mfrow = c(2, 2))\ngam.check(CO2_time)\n\n\n\n\nLos gráficos de residuos muestran un patrón extraño de subida y bajada, claramente hay alguna estructura de dependencia (y probablemente podemos suponer que tiene algo que ver con fluctuaciones intra-anuales). Intentemos nuevamente e introduzcamos algo llamado un suavizador cíclico.\n\\[y = \\beta_0 + f_{\\mathrm{intrannual}}(mes) + f_{\\mathrm{trend}}(tiempo) + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2)\\] El término suavizador cíclico, \\(f_{\\mathrm{intrannual}}(mes)\\), está compuesto por funciones de base al igual que hemos visto anteriormente, excepto que los puntos finales del spline están restringidos a ser iguales, lo cual tiene sentido cuando estamos modelando una variable que es cíclica (a través de meses/años).\nAhora veremos el argumento bs = para elegir el tipo de suavizador y el argumento k = para elegir el número de puntos de anclaje, porque los splines de regresión cúbicos tienen un número fijo de puntos de anclaje. Usaremos 12 puntos de anclaje, porque hay 12 meses.\n\nCO2_season_time &lt;- gam(co2 ~ s(month, bs = \"cc\", k = 12) + s(time), data = CO2, method = \"REML\")\n\nVeamos los términos suavizados ajustados:\n\npar(mfrow = c(1, 2))\nplot(CO2_season_time)\n\n\n\n\nAl observar ambos términos suavizados, podemos ver que el suavizador mensual está captando esa subida y bajada mensual del CO\\(_2\\) - al observar las magnitudes relativas (es decir, fluctuación mensual vs. tendencia a largo plazo), podemos ver lo importante que es desentrañar las componentes de la serie temporal. Veamos cómo lucen los diagnósticos del modelo ahora:\n\npar(mfrow = c(2, 2))\ngam.check(CO2_season_time)\n\n\n\n\nMucho mejor. Veamos cómo se compara la componente estacional con la tendencia a largo plazo completa.\n\nCO2_season_time &lt;- gam(co2 ~ s(month, bs = \"cc\", k = 12) + s(time), data = CO2_dat, method = \"REML\")\npar(mfrow = c(1, 2))\nplot(CO2_season_time)\n\n\n\n\nHay más en la historia, ¿quizás autocorrelaciones espaciales de algún tipo? gam puede hacer uso de las estructuras de autocorrelación espacial disponibles en el paquete nlme, más sobre eso la próxima vez. Espero que por ahora los GAM no parezcan tan aterradores o mágicos, y que puedas comenzar a aprovechar lo que realmente es un marco de modelado increíblemente flexible y poderoso.\n\nComunicando los resultados\nPuedes presentar los resultados de un GAM (Modelo Aditivo Generalizado) de manera similar a cualquier otro modelo lineal, con la diferencia principal de que, para los términos suaves, no hay un solo coeficiente del cual puedas inferir (por ejemplo, tamaño del efecto negativo o positivo). Por lo tanto, debes confiar en interpretar los efectos parciales de los términos suaves visualmente (por ejemplo, a través de una llamada a plot(gam_model)) o inferir a partir de los valores predichos. Por supuesto, puedes incluir términos lineales normales en el modelo (ya sean continuos o categóricos, e incluso en un marco de tipo ANOVA) e inferir a partir de ellos como lo harías normalmente. De hecho, los GAM a menudo son útiles para tener en cuenta un fenómeno no lineal que no es de interés directo, pero que debe tenerse en cuenta al hacer inferencias sobre otras variables.\nPuedes trazar los efectos parciales llamando a la función plot en un modelo GAM ajustado, y también puedes analizar los términos paramétricos utilizando posiblemente la función termplot. Puedes usar ggplot para modelos simples como hicimos anteriormente en este tutorial, pero para modelos más complejos, es bueno saber cómo generar los datos utilizando predict. Aquí simplemente utilizamos la serie de tiempo existente, pero tú generarías tus propios datos para el argumento newdata=.\n\nCO2_pred &lt;- data.frame(\n  time = CO2_dat$time,\n  co2 = CO2_dat$co2,\n  predicted_values = predict(CO2_season_time, newdata = CO2_dat)\n)\nggplot(CO2_pred, aes(x = time)) +\n  geom_point(aes(y = co2), size = 1, alpha = 0.5) +\n  geom_line(aes(y = predicted_values), colour = \"red\")\n\n\n\n\n\n\nMás ayuda\nLa ayuda de R ?gam es muy buena y hay mucha información para leer. Echa un vistazo a ?gamm, ?gamm4 y ?bam. Utiliza citation(\"mgcv\") para obtener una variedad de artículos con explicaciones técnicas más detalladas. El libro sobre GAMs con R es particularmente bueno (hay una versión de 2017). Un blog genial con mucho contenido sobre GAMs: https://www.fromthebottomoftheheap.net/\nAutor: Mitchell Lyons\nAño: 2017\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/glms/glm-1/index.html",
    "href": "statistics/glms/glm-1/index.html",
    "title": "Introducción y Datos Binarios",
    "section": "",
    "text": "Introducción y datos binomiales\nLos modelos lineales (por ejemplo, regresión lineal) se utilizan para modelar la relación entre una variable de respuesta continua \\(y\\) y una o más variables explicativas \\(x_1, x_2, \\cdots\\). Cuando tenemos una variable de respuesta discreta, utilizamos modelos lineales generalizados (GLM).\n\nPor ejemplo, si hemos realizado un estudio en una playa y queremos analizar cómo varía la presencia de un cangrejo en función del tiempo y la distancia desde la línea de agua, la variable de respuesta es discreta: la presencia o ausencia de un cangrejo en una determinada réplica. Las primeras filas del conjunto de datos se verían así:\n\n\n  X Dist Time CrabPres\n1 1    0    5        0\n2 2    0    5        1\n3 3    0    5        1\n4 4    0    5        0\n5 5    0    5        0\n6 6    2    5        0\n\n\nPropiedades de los GLM. Los datos de respuesta discreta, como los conteos y los datos de presencia/ausencia, generalmente muestran una relación media-varianza. Por ejemplo, para los conteos que en promedio son 5, esperaríamos que la mayoría de las muestras estuvieran entre aproximadamente 1 y 9, pero para los conteos que en promedio son 500, la mayoría de las observaciones tenderán a estar entre 450 y 550, lo que nos dará una varianza mucho mayor cuando la media sea grande.\nLos modelos lineales asumen una varianza constante. Es posible que hayas aprendido a transformar datos de conteo y luego ajustar un modelo lineal. Esto puede reducir la relación media-varianza, pero no la eliminará por completo, especialmente si tienes muchos ceros en tus datos. Para analizar datos discretos de manera precisa, necesitamos utilizar GLM.\nUn GLM realiza algunas suposiciones importantes (las comprobaremos más adelante en nuestros ejemplos):\n\nLos \\(y\\) observados son independientes, condicionales a algunos predictores \\(x\\)\nLa respuesta \\(y\\) proviene de una distribución conocida con una relación media-varianza conocida\nExiste una relación lineal entre una función conocida \\(g\\) de la media de \\(y\\) y los predictores \\(x\\)\n\n\\[ g(\\mu_y) = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots \\]\nNota: las funciones de enlace g() son una parte importante del ajuste de GLM, pero están fuera del alcance de este tutorial introductorio. Lo único que necesitas saber es que el enlace predeterminado para datos binomiales es logit() y para datos de conteo es log(). Para obtener más información, consulta ?family.\n\nEjecutando el análisis\nDatos binomiales\nPrimero, te mostraremos cómo ajustar un modelo a datos binomiales, es decir, datos de presencia/ausencia o datos 0/1. Ajustar GLM (modelos lineales generalizados) utiliza una sintaxis muy similar a la de los modelos lineales. Utilizamos la función glm en lugar de lm. También necesitamos agregar un argumento family para especificar si tus datos son de conteo, binomiales, etc.\nPara este ejemplo práctico, descarga el conjunto de datos de muestra sobre la presencia y ausencia de cangrejos en la playa, Crabs.csv, e impórtalo a R.\n\nCrab_PA &lt;- read.csv(\"Crabs.csv\", header = T)\n\nPara probar si la probabilidad de presencia de cangrejos cambia con el tiempo (un factor) y la distancia (una variable continua), ajustamos el siguiente modelo. La variable de respuesta (presencia/ausencia de cangrejos) es binomial, por lo que usamos family=binomial.\n\nft.crab &lt;- glm(CrabPres ~ Time * Dist, family = binomial, data = Crab_PA)\n\n\n\nSupuestos a verificar\nAntes de examinar los resultados de nuestro análisis, es importante verificar que nuestros datos cumplan con los supuestos del modelo que utilizamos. Veamos todos los supuestos en orden.\nSupuesto 1: Los valores observados \\(y\\) son independientes, condicionales a algunos predictores \\(x\\)\nNo podemos verificar este supuesto a partir de los resultados, pero puedes asegurarte de que sea cierto tomando una muestra aleatoria para tu diseño experimental. Si tu diseño experimental involucra alguna pseudo-replicación, este supuesto se violará. Para ciertos tipos de pseudo-replicación, puedes utilizar modelos mixtos en su lugar.\nSupuesto 2: La respuesta \\(y\\) proviene de una distribución conocida con una relación media-varianza conocida\nLa relación media-varianza es la principal razón por la que utilizamos GLM en lugar de modelos lineales. Necesitamos verificar si la distribución modela bien la relación media-varianza de nuestros datos. Para datos binomiales, esto no es una gran preocupación, pero más adelante, cuando analicemos datos de conteo, será muy importante. Para verificar este supuesto, observamos un gráfico de residuos e intentamos ver si hay una forma de abanico.\n\nplot(ft.crab, which = 1)\n\n\n\n\nDesafortunadamente, la función que permite la nacionalización delos resultados de glm nos muestra un gráfico muy extraño debido a la discreción de los datos (es decir, muchos puntos superpuestos).\nPara obtener un gráfico más útil, podemos ajustar el modelo utilizando la función manyglm del paquete mvabund. Necesitamos un ligero cambio en el argumento family; para manyglm, escribimos family = \"binomial\".\n\nlibrary(mvabund)\nft.crab.many &lt;- manyglm(CrabPres ~ Time * Dist, family = \"binomial\", data = Crab_PA)\nplot(ft.crab.many)\n\n\n\n\nAhora podemos buscar una forma de abanico en el gráfico residual. Para estos datos, no parece haber una forma de abanico, por lo que podemos concluir que la suposición de media-varianza que hizo el modelo fue razonable para nuestros datos. Los residuos en este gráfico tienen una componente aleatoria. Si ves un patrón, es mejor repetir el gráfico varias veces para ver si el patrón es real.\nSuposición 3: Existe una relación lineal entre una función conocida \\(g\\) de la media de \\(y\\) y los predictores \\(x\\).\nPara verificar esta suposición, revisamos el gráfico residual anterior en busca de no linealidad o una forma de U. En nuestro caso, no hay evidencia de no linealidad. Si los residuos parecen descender y luego ascender, o ascender y luego descender, es posible que necesitemos agregar una función polinómica de los predictores utilizando la función poly.\n\n\nInterpretación de los resultados\nSi todas las verificaciones de suposiciones están bien, podemos analizar los resultados que nos proporcionó el modelo. Las dos funciones principales para la inferencia son las mismas que para los modelos lineales: summary y anova.\nLos valores p que te dan si usas glm para ajustar el modelo funcionan bien en muestras grandes, aunque aún son aproximados. Para modelos binomiales en particular, los valores p de la función summary pueden ser curiosos, y preferimos usar la función anova para ver si los predictores son significativos. La función summary() sigue siendo útil para ver la ecuación del modelo.\n\nanova(ft.crab, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: CrabPres\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   \nNULL                         56     71.097            \nTime       1   6.6701        55     64.427 0.009804 **\nDist       1   0.7955        54     63.631 0.372448   \nTime:Dist  1   0.1647        53     63.466 0.684852   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value for Time is small (P&lt;0.01), so we conclude there is an effect of time on the presence of crabs, but no effect of distance or an interaction between time and distance. This sample is reasonably large, so these p-values should be a good approximation. For a small sample it is often better to use resampling to calculate p-values. When you use manyglm the summary and anova functions use resampling by default.\nEl p-valor para tiempo es pequeño (p &lt; 0.01), por lo tanto, concluimos que hay un efecto del tiempo en la presencia de cangrejos, pero no hay un efecto de la distancia ni una interacción entre el tiempo y la distancia. Esta muestra es bastante grande, por lo que estos p-valor deberían ser una buena aproximación. Para una muestra pequeña, a menudo es mejor utilizar remuestreo para calcular los p-valor. Cuando utilizas manyglm, las funciones summary y anova utilizan remuestreo de forma predeterminada.\n\nanova(ft.crab.many)\n\nTime elapsed: 0 hr 0 min 0 sec\n\n\nAnalysis of Deviance Table\n\nModel: CrabPres ~ Time * Dist\n\nMultivariate test:\n            Res.Df Df.diff   Dev Pr(&gt;Dev)  \n(Intercept)     56                         \nTime            55       1 6.670    0.011 *\nDist            54       1 0.795    0.370  \nTime:Dist       53       1 0.165    0.700  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nArguments: P-value calculated using 999 iterations via PIT-trap resampling.\n\n\nEn este caso, los resultados son bastante similares, pero en muestras pequeñas a menudo puede marcar una gran diferencia.\nTambién puedes utilizar summary con las funciones glm o manyglm. Esto se interpreta de manera similar a la regresión lineal, pero necesitamos incluir la función de enlace g.\n\nsummary(ft.crab)\n\n\nCall:\nglm(formula = CrabPres ~ Time * Dist, family = binomial, data = Crab_PA)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -3.00604    1.47469  -2.038   0.0415 *\nTime         0.25835    0.17439   1.481   0.1385  \nDist        -0.03193    0.23923  -0.133   0.8938  \nTime:Dist    0.01143    0.02830   0.404   0.6863  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.097  on 56  degrees of freedom\nResidual deviance: 63.466  on 53  degrees of freedom\nAIC: 71.466\n\nNumber of Fisher Scoring iterations: 4\n\n\nSi \\(p\\) es la probabilidad de presencia de cangrejos, esta salida nos dice\n\\[ logit(p) = -3.01 + 0.26 \\times \\text{Time} -0.03 \\times \\text{Dist} +0.01 \\times \\text{Time} \\times \\text{Dist}\\]\n\n\nComunicación de los resultados\nLos resultados de los GLM se comunican de la misma manera que los resultados de los modelos lineales. Para un predictor es suficiente con escribir una línea, por ejemplo: “Hay evidencia sólida de que la presencia de cangrejos varía con el tiempo (p = 0.01)”. Para varios predictores, es mejor mostrar los resultados en una tabla. También debes indicar qué distribución se utilizó (por ejemplo, Binomial para presencia/ausencia, Poisson o binomial negativa para recuentos) y si se utilizó remuestreo para la inferencia.\n\n\nMás ayuda\nPuedes escribir ?glm y ?manyglm en R para obtener ayuda con estas funciones.\n\nFaraway, JJ. 2005. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nZuur, A, EN Ieno and GM Smith. 2007. Analysing ecological data. Springer Science & Business Media, 2007.\n\nMás consejos sobre la interpretación de coeficientes en GLMs\nAutor: Gordana Popovic\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/glms/glm-2/index.html",
    "href": "statistics/glms/glm-2/index.html",
    "title": "Datos de recuento",
    "section": "",
    "text": "Datos de recuento\nEsto es una continuación de Modelos lineales generalizados 1, que introdujo los GLM y proporcionó instrucciones para datos binarios. Lee eso primero para entender cuándo se utilizan los GLM. En esta página, cubriremos el uso de los GLM para datos de recuento y presentaremos brevemente cómo se pueden utilizar para otros tipos de datos que puedas tener.\n\nEjecución del análisis\n\nPara este ejemplo práctico, tenemos recuentos de diferentes grupos de animales en sitios de control y sitios donde se ha llevado a cabo la regeneración del matorral (tratamiento). Queremos saber si las actividades de regeneración del matorral han afectado el recuento de babosas.\nDescarga el conjunto de datos de muestra, Revegetation.csv, e impórtalo en R para ver cómo se organizan los datos:\n\nReveg &lt;- read.csv(\"Revegetation.csv\", header = T)\n\nSi visualizas el histograma de frecuencias de los recuentos de babosas, verás que está muy sesgado, con muchos valores pequeños y pocos recuentos grandes (el nombre de la variable, Soleolifera, es el nombre de orden de las babosas terrestres).\n\nhist(Reveg$Soleolifera)\n\n\n\n\nLa distribución predeterminada para datos de recuento es la distribución de Poisson. La distribución de Poisson asume que la varianza es igual a la media. Esta es una suposición bastante restrictiva que a menudo se viola en los datos de recuento ecológicos. Es posible que necesitemos utilizar la distribución negativa binomial, que es más flexible.\nPodemos utilizar un GLM para probar si los recuentos de babosas (del orden Soleolifera) difieren entre los sitios de control y los sitios regenerados. Para ajustar el GLM, utilizaremos la función manyglm en lugar de glm para tener acceso a gráficos de residuos más útiles.\nPara ajustar el GLM, carga el paquete mvabund y luego ajusta el siguiente modelo:\n\nlibrary(mvabund)\nft.sol.pois &lt;- manyglm(Soleolifera ~ Treatment, family = \"poisson\", data = Reveg)\n\ndonde Soleolifera es la variable de respuesta y Treatment es la variable predictora (con dos niveles, control y regenerado).\n\n\nSuposiciones a verificar\nAntes de ver los resultados, debemos revisar el gráfico de residuos para verificar las suposiciones.\n\nplot(ft.sol.pois)\n\nWarning in default.plot.manyglm(x, res.type = res.type, which = which, caption\n= caption, : Only the first 1 colors will be used for plotting.\n\n\n\n\n\nEs difícil decir si hay alguna no linealidad en este gráfico, esto se debe a que el predictor es binario (tratamiento vs reforestado). Al observar la suposición de varianza, parece que hay una forma de abanico. Los residuos están más dispersos a la derecha que a la izquierda, a esto lo llamamos sobredispersión.\nEsto nos indica que la suposición de varianza de la distribución de Poisson puede ser demasiado restrictiva y deberíamos probar una distribución diferente. En cambio, podemos ajustar una distribución binomial negativa en manyglm cambiando el argumento family a family=\"negative binomial\".\n\nft.sol.nb &lt;- manyglm(Soleolifera ~ Treatment, family = \"negative binomial\", data = Reveg)\n\nObserva nuevamente el gráfico de residuos:\n\nplot(ft.sol.nb)\n\nWarning in default.plot.manyglm(x, res.type = res.type, which = which, caption\n= caption, : Only the first 1 colors will be used for plotting.\n\n\n\n\n\nEsto parece haber mejorado el gráfico de residuos. Ya no hay una fuerte forma de abanico, así que podemos proceder y examinar los resultados.\n\n\nInterpretación de los resultados\nSi todas las verificaciones de suposiciones están bien, podemos observar los resultados que nos proporcionó el modelo con las mismas dos funciones de inferencia utilizadas para los modelos lineales: summary y anova.\n\nanova(ft.sol.nb)\n\nTime elapsed: 0 hr 0 min 0 sec\n\n\nAnalysis of Deviance Table\n\nModel: Soleolifera ~ Treatment\n\nMultivariate test:\n            Res.Df Df.diff   Dev Pr(&gt;Dev)   \n(Intercept)     48                          \nTreatment       47       1 10.52    0.002 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nArguments: P-value calculated using 999 iterations via PIT-trap resampling.\n\nsummary(ft.sol.nb)\n\n\nTest statistics:\n                     wald value Pr(&gt;wald)    \n(Intercept)               1.502     0.032 *  \nTreatmentRevegetated      3.307     0.001 ***\n--- \nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTest statistic:  3.307, p-value: 0.001 \nArguments: P-value calculated using 999 resampling iterations via pit.trap resampling.\n\n\nAmbas pruebas indican una fuerte evidencia de un efecto del tratamiento (p&lt;0.01). Para extraer la ecuación del modelo, podemos observar los coeficientes obtenidos del ajuste.\n\nft.sol.nb$coefficients\n\n                     Soleolifera\n(Intercept)           -0.9162907\nTreatmentRevegetated   2.1202635\n\n\nLa función de enlace predeterminada para modelos de Poisson y binomial negativo es \\(log()\\). Si escribimos la media de conteo como \\(\\lambda\\)\n\\[ \\log(\\lambda) = -0.92 + 2.12 \\times \\text{Treatment}\\]\n\n\nComunicación de los resultados\nEscrita. Los resultados de los GLM se comunican de la misma manera que los resultados de los modelos lineales. Para un predictor, basta con escribir una línea, por ejemplo: “Hay evidencia sólida de un efecto positivo de la regeneración de arbustos en la abundancia de babosas del orden Soleolifera (p &lt; 0.001)”. Para múltiples predictores, es mejor mostrar los resultados en una tabla. También debes indicar qué distribución se utilizó (por ejemplo, binomial negativa) y si se utilizó remuestreo para inferencia. “Utilizamos un modelo lineal generalizado binomial negativo debido a la sobredispersión evidente en los datos. La inferencia se realizó utilizando remuestreo bootstrap con 1000 remuestras (valor predeterminado al usar manyglm)”.\nVisual. En este ejemplo, un diagrama de caja sería una forma efectiva de visualizar las diferencias en los recuentos de babosas entre los sitios de control y los sitios revegetados.\n\nboxplot(Soleolifera ~ Treatment, ylab = \"Count\", xlab = \"Treatment\", data = Reveg)\n\n\n\n\n\n\nOtros tipos de datos\nSi tienes datos continuos positivos con ceros, como datos de biomasa, es posible que la distribución tweedie pueda modelar esto, aunque tiene algunas suposiciones bastante restrictivas. Puedes usar family=\"tweedie\" con la función manyglm. Asegúrate de examinar los gráficos de residuos para detectar violaciones de las suposiciones.\nPara datos continuos estrictamente positivos, se puede utilizar una distribución gamma. Esto está disponible en la función glm utilizando family=gamma.\n\n\nMás ayuda\nPuedes escribir ?glm y ?manyglm en R para obtener ayuda con estas funciones.\n\nFaraway, JJ. 2005. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nZuur, A, EN Ieno and GM Smith. 20074. Analysing ecological data. Springer Science & Business Media, 2007.\n\nMás consejos sobre la interpretación de coeficientes en GLMs\nAutor: Gordana Popovic\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/glms/index.html",
    "href": "statistics/glms/index.html",
    "title": "Modelos Lineales Generalizados (GLM)",
    "section": "",
    "text": "Los modelos lineales generalizados (GLM) se utilizan cuando la distribución de los datos no se ajusta a las suposiciones de los modelos lineales, específicamente las suposiciones de residuos distribuidos normalmente y ausencia de relación entre la varianza y la media (por ejemplo, datos de presencia/ausencia, recuento o altamente sesgados).\n\nModelos lineales generalizados 1: Introducción y datos binomiales\nModelos lineales generalizados 2: Datos de recuento\nInterpretación de coeficientes en glms"
  },
  {
    "objectID": "statistics/glms/interpret-glm-coeffs/index.html",
    "href": "statistics/glms/interpret-glm-coeffs/index.html",
    "title": "Interpretación de los GLMs",
    "section": "",
    "text": "En los modelos lineales, la interpretación de los parámetros del modelo es lineal. Por ejemplo, si estás modelando la altura de las plantas en función de la altitud y el coeficiente para la altitud es -0.9, entonces la altura de las plantas disminuirá en 1.09 unidades por cada aumento de 1 unidad en la altitud.\nPara los modelos lineales generalizados, la interpretación no es tan directa. Aquí explicaremos cómo interpretar los coeficientes en los modelos lineales generalizados (GLMs). Primero, deberás leer nuestras páginas sobre GLMs para datos binarios y de conteo, así como nuestra página sobre la interpretación de coeficientes en modelos lineales.\n\nGLMs de Poisson y binomial negativo\nEn los GLMs de Poisson y binomial negativo, utilizamos una función de enlace logarítmico. El modelo real que ajustamos con una covariable \\(x\\) se ve así:\n\\[ Y \\sim \\text{Poisson} (\\lambda) \\]\n\\[  log(\\lambda) = \\beta_0 + \\beta_1 x \\]\naquí \\(\\lambda\\) es la media de Y. Entonces, si tenemos un valor inicial de la covariable \\(x_0\\), el valor predicho de la media \\(\\lambda_0\\) se calcula de la siguiente manera:\n\\[  log(\\lambda_0) = \\beta_0 + \\beta_1 x_0 \\]\nSi ahora aumentamos la covariable en 1, obtenemos una nueva media \\(\\lambda_1\\),\n\\[  log(\\lambda_1) = \\beta_0 + \\beta_1 (x_0 +1) = \\beta_0 + \\beta_1 x_0 +\\beta_1 = log(\\lambda_0) + \\beta_1\\]\nPor lo tanto, el logaritmo de la media de Y aumenta en \\(\\beta_1\\) cuando aumentamos x en 1. Pero en realidad no nos interesa cómo cambia el logaritmo de la media, nos gustaría saber cómo cambia Y en promedio. Si tomamos el exponencial de ambos lados,\n\\[  \\lambda_1 = \\lambda_0 exp(\\beta_1)\\]\nEntonces, la media de Y se multiplica por \\(exp(\\beta_1)\\) cuando aumentamos \\(x\\) en 1 unidad.\n\nN &lt;- 120\nx &lt;- rnorm(N)\nmu &lt;- exp(1 + 0.2 * x)\nY &lt;- rpois(N, lambda = mu)\nglm1 &lt;- glm(Y ~ x, family = poisson)\nglm1$coefficients\n\n(Intercept)           x \n   1.057614    0.217644 \n\nexp(glm1$coefficients[2])\n\n       x \n1.243144 \n\n\n\n\nGLMs binomiales\n\nRegresión logística\nLas cosas se vuelven mucho más complicadas en los GLMs binomiales. El modelo aquí es en realidad un modelo de log-odds (probabilidades), por lo que necesitamos comenzar con una explicación de eso. Las probabilidades de un evento son la probabilidad de éxito dividida por la probabilidad de fracaso. Entonces, si la probabilidad de éxito es \\(p\\), las probabilidades son:\n\\[\\text{Odds} = \\frac{p}{1-p}\\]\nA medida que \\(p\\) aumenta, también lo hacen las probabilidades. La ecuación para una regresión logística se ve así:\n\\[ Y \\sim \\text{binomial}(p) \\]\n\\[ \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x \\]\nOmitiendo algunos cálculos que son muy similares a los anteriores, podemos obtener una interpretación para el coeficiente de \\(x\\) en el modelo en términos de las odds. Cuando aumentamos \\(x\\) en una unidad, las odds se multiplican por \\(exp(\\beta_1)\\). Las odds no son la cosa más intuitiva de interpretar, pero aumentan cuando \\(p\\) aumenta, por lo que si tu coeficiente \\(\\beta_1\\) es positivo, aumentar \\(x\\) aumentará tu probabilidad.\n\nbY &lt;- Y &gt; 0 \nbin1 &lt;- glm(bY ~ x, family = binomial)\nsummary(bin1)\n\n\nCall:\nglm(formula = bY ~ x, family = binomial)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.4246     0.3520   6.889 5.64e-12 ***\nx             0.4972     0.3255   1.527    0.127    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.530  on 119  degrees of freedom\nResidual deviance: 71.154  on 118  degrees of freedom\nAIC: 75.154\n\nNumber of Fisher Scoring iterations: 5\n\n\nCuando incrementamos \\(x\\) en una unidad, las probabilidades de Y se multiplican por \\(exp(\\beta_1) = 2.11\\).\n\n\nLogaritmos complementarios (log-log)\nPosiblemente un modelo más intuitivo sea una regresión binomial con una función de enlace log-log. Esta función de enlace se basa en la suposición de que tienes algunos recuentos, que siguen una distribución de Poisson, pero hemos decidido convertirlos en presencia/ausencia.\n\\[ Y \\sim \\text{binomial} (p) \\] \\[  log(-log(1-p)) = \\beta_0 + \\beta_1 x \\]\nEn ese caso, puedes interpretar tus coeficientes de manera similar a la regresión de Poisson. Cuando aumentas \\(x\\) en 1 unidad, la media de tu recuento subyacente (que has convertido en presencia/ausencia) se multiplica por \\(exp( \\beta_1 )\\).\n\nlibrary(mvabund)\nbin2 &lt;- manyglm(bY ~ x, family = binomial(link = \"cloglog\"))\ncoef(bin2)\n\n                   bY\n(Intercept) 0.9111128\nx           0.1984341\n\n\nLa interpretación es ahora la misma que en el caso de Poisson, cuando aumentamos \\(x\\) en 1 unidad, la media del recuento subyacente se multiplica por \\(exp( \\beta_1 )\\).\n\n\nModelo binomial logarítmico\nEs posible utilizar una función de enlace logarítmico con la distribución binomial family = binomial(link = log). En este caso, puedes interpretar los coeficientes como multiplicadores de las probabilidades por \\(exp( \\beta_1 )\\), sin embargo, estos modelos pueden darte probabilidades predichas mayores que 1 y a menudo no convergen (no dan una respuesta).\n\n\n\nDesplazamientos\nA veces sabemos que el efecto de una variable en la respuesta es proporcional, de modo que cuando duplicamos dicha variable, esperamos que la respuesta se duplique en promedio. El caso más común en el que se ve esto es con la intensidad de muestreo.\n\nSi muestreamos suelo y contamos criaturas, suponiendo que todas las demás cosas son iguales, esperarías el doble de criaturas en el doble de cantidad de suelo. Si tienes una variable como esta, es tentador dividir tu respuesta (conteo) por la cantidad de suelo para estandarizar los datos. Desafortunadamente, esto tomará los conteos, que sabemos cómo modelar con glms, y los convertirá en algo que no sabemos cómo modelar. Afortunadamente, esta situación se resuelve fácilmente utilizando desplazamientos (offsets). Primero, vamos a simular algunos datos para la cantidad de suelo, la profundidad (nuestra variable predictora) y los datos de conteo (con una distribución de Poisson) donde los conteos dependen de cuánto suelo se muestreó.\n\nsoil &lt;- exp(rbeta(N, shape1 = 8, shape2 = 1))\ndepth &lt;- rnorm(N)\nmu &lt;- soil * exp(0.5 + 0.5 * depth)\ncount &lt;- rpois(N, lambda = mu)\n\nAhora, podemos modelar los conteos con la profundidad como nuestra variable predictora y la cantidad de suelo como un desplazamiento (offset).\n\noff_mod &lt;- glm(Y ~ depth + offset(log(soil)), family = poisson)\nsummary(off_mod)\n\n\nCall:\nglm(formula = Y ~ depth + offset(log(soil)), family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.15753    0.05361   2.938   0.0033 **\ndepth        0.02575    0.04737   0.544   0.5867   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 174.73  on 119  degrees of freedom\nResidual deviance: 174.43  on 118  degrees of freedom\nAIC: 497.25\n\nNumber of Fisher Scoring iterations: 5\n\n\nSi ignoráramos la cantidad de suelo, podríamos llegar a conclusiones erróneas. Si la cantidad de suelo está correlacionada con otra variable en tu modelo, omitir el desplazamiento afectará el coeficiente de esa variable, como se discute en la interpretación condicional/marginal aquí. El desplazamiento también a menudo explicará gran parte de la variación en la respuesta, por lo que incluirlo te dará un mejor modelo en general. ¿Qué pasa si no estás seguro si la relación es exactamente proporcional? En ese caso, simplemente incluye la variable en tu modelo como un coeficiente y el modelo decidirá la mejor relación entre ella y tu respuesta.\n\ncoef_mod &lt;- glm(Y ~ depth + log(soil), family = poisson)\nsummary(coef_mod)\n\n\nCall:\nglm(formula = Y ~ depth + log(soil), family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.41615    0.48851   4.946 7.58e-07 ***\ndepth        0.02997    0.04849   0.618  0.53649    \nlog(soil)   -1.50442    0.54503  -2.760  0.00578 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 162.84  on 119  degrees of freedom\nResidual deviance: 155.34  on 117  degrees of freedom\nAIC: 480.16\n\nNumber of Fisher Scoring iterations: 5\n\n\nEl coeficiente estimado por el modelo es cercano a 1, lo cual sería equivalente a un desplazamiento.\nAutor: Gordana Popovic\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/index.html",
    "href": "statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Estas páginas contienen algunas introducciones a una amplia gama de análisis estadísticos comúnmente utilizados en las ciencias ambientales."
  },
  {
    "objectID": "statistics/linear-models/anova/anova-factorial/index.html",
    "href": "statistics/linear-models/anova/anova-factorial/index.html",
    "title": "ANOVA Factorial",
    "section": "",
    "text": "El análisis de varianza (ANOVA) es una de las técnicas más utilizadas en las ciencias biológicas y ambientales. El ANOVA se utiliza para contrastar una variable dependiente continua y en diferentes niveles de una o más variables independientes categóricas x. Las variables independientes se denominan factor o tratamiento, y las diferentes categorías dentro de ese tratamiento se denominan niveles. Aquí consideraremos diseños con dos o más factores que se aplican a las unidades experimentales (lee primero la página sobre ANOVA de un factor si no estás familiarizado/a con el ANOVA).\nFrecuentemente, queremos probar diferencias en una variable de respuesta debido a dos o más factores. Estos diseños experimentales o de muestreo nos permiten probar los efectos de cada uno de estos factores por separado (llamados efectos principales) y probar si los dos factores interactúan entre sí. Para estos diseños, utilizamos versiones más complejas del ANOVA que el diseño más simple que prueba los efectos de un solo factor.\n\nConsideremos un ejemplo en el que un investigador está probando los efectos de la contaminación por metales en el número de especies encontradas en invertebrados marinos sésiles (esponjas, briozoos y ascidias, entre otros). Les gustaría saber si el cobre reduce la riqueza de especies, pero también saben que la riqueza de invertebrados puede depender de si el sustrato es vertical u horizontal. En consecuencia, llevaron a cabo un experimento en el que se registró la riqueza de especies en muestras replicadas en cada una de las seis combinaciones de enriquecimiento de cobre (“Ninguno”, “Bajo”, “Alto”) y orientación (“Vertical”, “Horizontal”). El diseño experimental se denomina factorial porque todos los niveles de un tratamiento están representados en todos los niveles del otro tratamiento (también llamado ortogonal).\nEl ANOVA factorial probará: * si hay diferencias en la riqueza entre los tres niveles de enriquecimiento de cobre * si hay diferencias en la riqueza entre los dos niveles de orientación del sustrato * si hay alguna interacción entre el cobre y la orientación\nTienes tres hipótesis nulas: * no hay diferencia entre las medias para cada nivel de cobre, Ho: \\(\\mu_{Ninguno} = \\mu_{Bajo} = \\mu_{Alto}\\)\n\nno hay diferencia entre las medias para cada nivel de orientación, Ho: \\(\\mu_{Vertical} = \\mu_{Horizontal}\\)\nno hay interacción entre los factores\n\nEsto es mucho mejor que ejecutar dos ANOVAs separados de un solo factor que contrasten los efectos del cobre para cada nivel de orientación, porque se obtiene más potencia estadística (mayor número de grados de libertad) para las pruebas de interés y se obtiene una prueba formal de la interacción entre los factores, lo cual suele ser científicamente interesante.\nTen en cuenta que un ANOVA es un modelo lineal, al igual que la regresión lineal, excepto que las variables predictoras son categóricas en lugar de continuas. Con dos variables predictoras, el modelo lineal es:\n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]\ndonde \\(\\mu\\) es la media general, \\(\\alpha_i\\) es el efecto del i-ésimo grupo del primer factor, \\(\\beta_i\\) es el efecto del j-ésimo grupo del segundo factor y \\((\\alpha\\beta)\\) es la interacción.\nAunque tenemos dos factores y un efecto de interacción, esto requiere ajustar más de 3 parámetros en nuestro modelo debido a que tenemos 3 niveles del Factor A (Cobre) y 2 niveles del Factor B (Orientación) (¡si puedes determinar cuántos parámetros deben ajustarse en este modelo, oficialmente eres un aficionado a la estadística! Esto es complicado incluso para los que están “en el saber”).\nCon dos factores, el ANOVA divide la varianza total en un componente que puede explicarse mediante la primera variable predictora (entre los niveles del tratamiento A), un componente que puede explicarse mediante la segunda variable predictora (entre los niveles del tratamiento B), un componente que puede explicarse mediante la interacción y un componente que no puede explicarse (dentro de los niveles, la varianza residual). La estadística de prueba F se calcula tres veces para probar cada una de las hipótesis nulas. Para dos factores fijos, las razones F son:\n\\[F = \\frac{MS_{A}}{MS_{dentro}}\\]\n\\[F = \\frac{MS_{B}}{MS_{dentro}}\\]\n\\[F = \\frac{MS_{AB}}{MS_{dentro}}\\]\ndonde MS son las medias cuadráticas, una medida de variación. La probabilidad de obtener el valor observado de F se calcula a partir de la distribución de probabilidad conocida de F, con dos grados de libertad (uno para el numerador = el número de niveles - 1) y uno para el denominador. Ten en cuenta que estas razones F cambiarán si alguno de los factores es aleatorio (ver más abajo la distinción entre factores fijos y aleatorios).\n\nEjecutando el análisis\nTu conjunto de datos debe estar formateado de manera que las mediciones de cada réplica sean una fila y cada una de las variables sea una columna, correspondiendo a la variable dependiente y, el Factor A y el Factor B.\nDescarga el conjunto de datos de muestra Sessile.csv, e impórtalo a R. Verifica que tus variables predictoras sean factores utilizando la función str.\n\nSessile &lt;- read.csv(file = \"Sessile.csv\", header = TRUE)\nstr(Sessile)\n\nCon nuestras variables predictoras asignadas correctamente como factores, ahora podemos realizar el análisis. Al igual que con otros tipos de modelos lineales, tenemos una fórmula del modelo con la variable dependiente, y, a la izquierda del ~ y las variables predictoras a la derecha. Para este diseño de dos factores, utilizamos:\n\nSessile.aov &lt;- aov(Richness ~ Copper * Orientation, data = Sessile)\n\nTen en cuenta que al especificar un modelo con * entre los dos predictores, R automáticamente incluye tanto las variables como su interacción. Este mismo modelo también se puede escribir de la siguiente manera:\n\nSessile.aov &lt;- aov(Richness ~ Copper + Orientation + Copper:Orientation, data = Sessile)\n\nEl resultado de este análisis se puede ver utilizando la función summary en el objeto creado.\n\nsummary(Sessile.aov)\n\nExactamente el mismo modelo también se puede ejecutar utilizando la función de modelo lineal, lm.\n\nSessile.lm &lt;- lm(Richness ~ Copper * Orientation, \n                 data = Sessile)\nanova(Sessile.lm)\n\n\n\nInterpretación de los resultados\n\n\n                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nCopper              2   3330  1665.0  192.53  &lt; 2e-16 ***\nOrientation         1    240   240.0   27.75 2.46e-06 ***\nCopper:Orientation  2    571   285.3   33.00 4.34e-10 ***\nResiduals          54    467     8.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEl resumen de un objeto ANOVA es una tabla con los grados de libertad (Df), sumas de cuadrados (Sum Sq), cuadrados medios (Mean Sq) para cada una de las variables predictoras (es decir, la variación entre los niveles de tus tratamientos), su interacción y para los residuos (es decir, la variación dentro de los niveles). También se presentan la estadística de prueba, el valor de F y su valor p asociado (Pr(&gt;F)).\nVerifica que tengas los grados de libertad correctos. Para un diseño de dos factores con factores fijos, son los siguientes: * Factor A: a - 1 (donde a = número de niveles del Factor A) * Factor B: b - 1 (donde b = número de niveles del Factor B) * Interacción (AB): (a-1)(b-1) * Residual: ab(n - 1) (donde n = tamaño de la muestra)\nLas sumas de cuadrados y cuadrados medios son medidas de variación. Hay tres estadísticas F, correspondientes a una prueba de cada uno de los efectos principales y una para la interacción. Los valores p son las probabilidades de los valores observados de F de la distribución F (con los grados de libertad dados).\nEn este ejemplo, hay evidencia sólida para rechazar las tres hipótesis nulas: * que todos los niveles del tratamiento de cobre son iguales (P &lt; 0.001), * que las orientaciones vertical y horizontal son iguales (P &lt; 0.001) * que no hay interacción entre el cobre y la orientación (P &lt; 0.001)\nUna interacción significativa significa que el efecto de un factor depende del otro. En este ejemplo, significaría que el efecto del cobre no fue consistente entre los hábitats verticales y horizontales. En consecuencia, la interpretación de los efectos principales se vuelve más compleja. Consulta Entendiendo las interacciones para obtener más ayuda sobre la interpretación de las interacciones en modelos lineales. Una forma rápida de ayudarte a entender una interacción, si la tienes, es examinar un gráfico de interacciones.\n\ninteraction.plot(Sessile$Copper, Sessile$Orientation, Sessile$Richness)\n\n\n\n\nAquí puedes ver que el efecto del cobre (una disminución en la riqueza de especies) es más pronunciado en los hábitats con orientación vertical, y que la diferencia entre los dos hábitats cambia con la exposición al cobre.\nComparaciones múltiples. Si detectas diferencias significativas en el ANOVA, entonces nos interesa saber exactamente qué niveles difieren entre sí y cuáles no. Recuerda que un valor p significativo en la prueba que acabas de realizar rechazaría la hipótesis nula de que las medias son iguales en todos los grupos, pero no identificaría cuáles son diferentes entre sí. Si no hay interacción, puedes realizar una prueba post-hoc en cada uno de los efectos principales (solo necesario si hay más de dos niveles para un efecto). Si hay una interacción, deberás considerar pruebas post-hoc que contrasten las medias de todas las combinaciones de ambos factores.\n\n\nSupuestos a verificar\nLos supuestos del ANOVA factorial son los mismos que para todos los modelos lineales, incluidos los ANOVA de un solo factor más simples (consultar ANOVA: un solo factor), que son independencia, normalidad y homogeneidad de varianzas. También debemos considerar dos aspectos nuevos:\n\nsi tus factores son fijos o aleatorios, y\nsi tu diseño de muestreo o experimental está balanceado (es decir, tiene el mismo número de réplicas en cada combinación de tratamientos).\n\nFactores fijos y aleatorios. Existe una distinción importante entre factores cuyos niveles son los únicos de interés (llamados fijos) y factores cuyos niveles se muestrean de una colección más amplia de posibles niveles (llamados aleatorios). Por ejemplo, si repitiéramos el experimento anterior en tres sitios diferentes en el puerto de Sídney, elegidos entre muchos posibles sitios, consideraríamos el sitio como un factor aleatorio. No nos interesan esos sitios en particular, pero nos gustaría saber si nuestros tratamientos experimentales fueron consistentes en los sitios. Por otro lado, si solo estás interesado en Darling Harbour y Circular Quay, entonces estos dos podrían considerarse dos niveles de un factor fijo. Tratar los sitios como un factor fijo en ese caso significa que tus conclusiones no se deben extrapolar a otros posibles sitios, sino que se limitan a esos sitios en particular.\nEstadísticamente, hay una gran diferencia entre factores fijos en los que has medido todos los niveles posibles de interés (por ejemplo, control vs. un solo tratamiento) y factores aleatorios en los que los niveles se muestrean de todos los posibles niveles. En el análisis de varianza, todo esto importa porque las pruebas F que se utilizan para probar tus hipótesis se construyen de manera diferente según los factores sean fijos o aleatorios. En el ejemplo anterior, todos los factores eran fijos y el denominador de todas las pruebas F fue \\(MS_{within}\\). En modelos con todos los factores aleatorios y modelos con una combinación de factores fijos y aleatorios (llamados modelos de efectos mixtos), se utilizan otros componentes de la variación como denominadores en las pruebas F.\nSi tienes factores aleatorios, necesitarás leer más que esta página de ayuda para establecer las razones F correctas para tu diseño, y es posible que necesites calcularlas manualmente. Ten en cuenta que el código presentado solo dará pruebas F correctas para diseños con todos los factores fijos. También debes considerar seriamente analizar tus datos como un modelo mixto.\nDiseños balanceados y desbalanceados. Idealmente, el ANOVA factorial se debería realizar con un diseño balanceado, es decir, con el mismo número de réplicas en cada combinación de factores. Los diseños balanceados tienen menos probabilidades de verse afectados por pequeñas desviaciones de las suposiciones de normalidad y homogeneidad de varianza. Desafortunadamente, en la práctica es común encontrarse con diseños desbalanceados, donde se tienen números desiguales de réplicas para cada nivel (por ejemplo, mal tiempo impidió muestrear el segundo sitio de manera intensiva, un voluntario perdió la hoja de datos, etc.).\nLos diseños desbalanceados son más susceptibles a violar las suposiciones del ANOVA y no hay una única forma de particionar el \\(SS_{total}\\) en los componentes de efecto principal e interacción. Las funciones aov y lm en R utilizan lo que se conocen como sumas de cuadrados de Tipo I, donde los términos del modelo se ajustan secuencialmente (es decir, cuánta variación se explica por el factor A, luego cuánta variación adicional se explica al agregar el factor B). Esto significa que el orden de los términos en el modelo importa: las fórmulas del modelo Y ~ A + B + A:B y Y ~ B + A + B:A darán resultados diferentes.\nHay bastante debate al respecto en la literatura estadística, pero muchos recomiendan utilizar lo que se conocen como sumas de cuadrados de Tipo II o Tipo III para diseños desbalanceados. Otros paquetes de software como SPSS, SYSTAT y Minitab utilizarán automáticamente sumas de cuadrados de Tipo III, donde el orden de los términos en el modelo no importa. Para acceder a estas sumas de cuadrados en R, podemos utilizar la función Anova del paquete car.\nNormalidad. La suposición de normalidad se puede verificar mediante un histograma de frecuencia de los residuos o mediante un gráfico de cuantiles donde los residuos se representan en función de los valores esperados de una distribución normal. El histograma de los residuos debería seguir una distribución normal. Si los puntos en el gráfico de cuantiles se encuentran principalmente en la línea, los residuos están distribuidos de manera normal. Ambos gráficos se pueden obtener a partir del objeto de modelo creado por la función aov.\n\npar(mfrow = c(1, 2))\nhist(Sessile.aov$residuals)\nplot(Sessile.aov, which = 2)\n\n\n\n\nViolaciones de la normalidad se pueden solucionar mediante transformaciones o mediante el uso de una distribución de errores diferente en un modelo lineal generalizado (GLM).\nHomogeneidad de varianza. La suposición de homogeneidad de varianza, es decir, que la variación en los residuos es aproximadamente igual en todo el rango de la variable predictora, se puede verificar trazando los residuos en función de los valores ajustados del objeto de modelo aov.\n\nplot(Sessile.aov, which = 1)\n\n\n\n\nVarianzas heterogéneas se indican por un patrón no aleatorio en la gráfica de residuos frente a los valores ajustados. Busca una distribución uniforme de los residuos en el eje y para cada uno de los niveles en el eje x. Una distribución en forma de abanico con mayor varianza en valores más altos en el eje x es un problema común cuando los datos están sesgados. Consulta el módulo de pruebas de suposiciones de los modelos lineales para obtener más información. Si hay patrones fuertes, una solución potencial es transformar la variable de respuesta y. Si esto no soluciona el problema, la mejor solución es utilizar una distribución de errores diferente en un modelo lineal generalizado (GLM).\nIndependencia. ANOVA asume que todas las medidas replicadas son independientes entre sí (es decir, igualmente probables de ser muestreadas de la población de valores posibles para cada nivel). Este problema debe tenerse en cuenta en la etapa de diseño. Si los datos están agrupados de alguna manera (por ejemplo, la mitad de las muestras de invertebrados medidas en un momento, y luego la otra mitad medida más tarde), se necesitan diseños más complejos para tener en cuenta factores adicionales (por ejemplo, un diseño con un factor adicional de tiempo de muestreo).\nExisten diversas medidas para tratar la falta de independencia. Estas incluyen asegurarse de que todos los predictores importantes estén en el modelo, promediar las observaciones anidadas o utilizar un modelo mixto.\n\n\nComunicación de los resultados\nEscrita. Los resultados de los efectos principales y cualquier interacción deben describirse en el texto de una sección de resultados. Cada prueba F se puede describir en el texto, por ejemplo: “El tratamiento de cobre y la orientación del sustrato interactuaron para afectar a la especie de invertebrados sésiles (F = 19.33, df = 2,54, p &lt; 0.001)”. Alternativamente, todas las pruebas se pueden incluir en una tabla similar a la que se muestra en la salida después de summary(Sessile.aov). La descripción de las pruebas principales se seguiría de una descripción de los resultados post hoc si se utilizan.\nRecuerda que la interpretación de los efectos principales se complica cuando hay una interacción significativa (ver arriba). En este ejemplo, aunque el cobre redujo la riqueza de especies, ese efecto no fue consistente entre los dos hábitats. En otros escenarios con una interacción, es posible que el cobre afecte la riqueza en un hábitat pero no en otro, lo que impide hacer una afirmación simple como “el cobre redujo la riqueza de especies” porque no siempre sería cierto.\nVisual. Un diagrama de caja o un gráfico de columnas con barras de error son adecuados para contrastar una variable continua en diferentes niveles de una variable categórica. Consulta los módulos de gráficos para obtener versiones listas para su publicación de estas figuras.\n\nboxplot(Richness ~ Copper * Orientation, \n        data = Sessile, \n        names = c(\"High.H\", \"Low.H\", \"None.H\", \"High.V\", \"Low.V\", \"None.V\"),\n        ylab = \"Species richness\", \n        xlab = \"Copper/Orientation\",\n        ylim = c(0, 80))\n\n\n\n\n\n\nMás ayuda\nEscribe ?aov o ?lm para obtener la ayuda de R sobre estas funciones.\n\nQuinn y Keough (2002) Experimental design and data analysis for biologist. Cambridge University Press. Capítulo 9: Análisis de varianza multifactorial.\n\n\nMcKillup (2012) Statistics explained. An introductory guide for life scientists. Cambridge University Press. Capítulo 13: Análisis de varianza de dos factores.\n\n\nUnderwood (1997) Experiments in ecology: Their logical design and interpretation using analysis of variance. Cambridge University Press.\n\nAutores: James Lavender & Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/linear-models/anova/anova-nested/index.html",
    "href": "statistics/linear-models/anova/anova-nested/index.html",
    "title": "ANOVA anidado",
    "section": "",
    "text": "Uso\nEn el diseño de ANOVA de dos vías, los dos factores se conocen como factoriales (es decir, hay todas las combinaciones de cada nivel de cada factor). Otros diseños experimentales presentan factores que se denominan anidados. Esto ocurre cuando cada nivel de uno de los factores es único para solo un nivel del otro factor. La diferencia se ilustra mejor con los siguientes dos diseños experimentales donde hay dos factores, A y B.\nEn el diseño anidado, los niveles del factor B aparecen en solo uno de los niveles del factor A, no en ambos. Esto suele suceder con factores como “sitio” o “área”: generalmente pertenecen a solo un nivel de tu otro factor (por ejemplo, si se contrastan el norte y el sur del puerto, un suburbio no podría estar en ambas categorías).\nSe dice que el factor B está anidado dentro del factor A, generalmente escrito como B(A). Estos diseños tienen diferentes fuentes de varianza en comparación con los diseños factoriales y no tienen un término de interacción. Los diseños son bastante comunes en ecología y ciencias ambientales, y se utilizan a menudo para descomponer la varianza en muestreos jerárquicos espaciales (por ejemplo, hábitats, áreas dentro de hábitats, parcelas dentro de áreas, etc.).\nLos ejemplos a continuación provienen de un experimento que investiga el impacto del visón americano introducido en pequeños roedores (topos) en Finlandia. La hipótesis era que el visón se alimenta de los topos, reduciendo así el número de topos y limitando su tamaño de población. Para probar esto, se eliminaron los visones de áreas grandes (&gt; 20 km2) en el Mar Báltico. La hipótesis nula del experimento es que el recuento medio de topos en las islas de los sitios de eliminación será igual al recuento medio de topos en las áreas de control.\n\nLos datos representan dos tratamientos (eliminación de visones (mink) y control) y dos áreas anidadas dentro de cada tratamiento. Luego, hay 10 muestras en cada área. Cada una de estas muestras representa el número de topillos (voles) individuales atrapados en una isla (todas las islas estaban separadas por más de 300 m, lo que garantiza cierta independencia) durante 4 noches.\nEl diseño es anidado porque un área no puede pertenecer tanto a un tratamiento de eliminación como a un control. Es útil pensar en las áreas como las réplicas para el tratamiento, y las muestras individuales como las réplicas para cada área.\n\n\nEjecutando el análisis\nLos datos para un diseño anidado deben estar en formato de muestras como filas y variables como columnas. Una columna debe corresponder a la variable dependiente y, en este caso, el número de topillos (voles). Otra columna debe contener los niveles del factor fijo A, Tratamiento. Finalmente, una columna para los niveles del factor aleatorio anidado B, Área.\n\n# Input \nMink &lt;- read.csv(file = \"Mink.csv\", \n                 header = TRUE)\n# Check the structure\nstr(Mink)\n\n'data.frame':   40 obs. of  3 variables:\n $ Treatment: chr  \"Control\" \"Control\" \"Control\" \"Control\" ...\n $ Area     : chr  \"area1\" \"area1\" \"area1\" \"area1\" ...\n $ Voles    : int  8 16 11 15 9 10 11 9 8 14 ...\n\n# Analysis\nMink.nested &lt;- aov(Voles ~ Treatment + Area %in% Treatment,\n                   data = Mink)\n\nsummary(Mink.nested)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment       1 1416.1  1416.1   38.39 3.81e-07 ***\nTreatment:Area  2  357.8   178.9    4.85   0.0136 *  \nResiduals      36 1328.0    36.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpretación de los resultados\nLa salida es un poco diferente al ANOVA factorial. La sección superior proporciona una prueba de significancia del efecto fijo, Tratamiento, que es lo que nos interesa. Los residuos para esta prueba son aquellos asociados con el efecto aleatorio Área.\nUn cociente F significativo (P&lt;0.05) para un factor anidado indica una alta varianza entre subgrupos dentro de un grupo. Por ejemplo, una diferencia en el número medio de topillos(voles) entre las dos áreas dentro de un tratamiento. Esto se espera, sin embargo, conlleva pocos grados de libertad para probar efectos principales. Además, sugiere que los tratamientos pueden no tener un efecto uniforme en los niveles del factor anidado. Por ejemplo, los topillos(voles) en un área de eliminación mostraron una respuesta más fuerte que en otra área.\nCocientes F\n\nEntre grupos = MSentre/MSsubgrupo\nFactor B = MSsubgrupo/MSResidual\n\nGrados de libertad\n\nEntre grupos = (a - 1) (donde a = número de niveles del Factor A)\nEntre subgrupos = a(b - 1) (donde b = número de niveles del Factor B)\nDentro de subgrupos = ab(n - 1) (donde n = tamaño de la muestra)\n\nCuando hay más de 2 niveles del factor fijo de interés, se puede utilizar un análisis post hoc para determinar qué grupos difieren. Consulta el módulo post-hoc para obtener más información.\nAgrupamiento\nSiempre examina los factores anidados antes de los niveles superiores. Si la varianza explicada por el factor anidado es insignificante, se puede agrupar. Si la razón F de MSsubgrupo/MSentre no es significativa, entonces no necesitas subgrupos y puedes realizar un análisis de un solo factor.\nIncluso si MSsubgrupo/MSentre no es significativo a = 0.05, puede haber efectos de subgrupos (puede haber un alto error de Tipo II). Por precaución, solo agrupa SS cuando P&gt;0.25, de modo que haya muy poca probabilidad de que aceptes incorrectamente la hipótesis nula de no efecto.\n\n\nSupuestos a verificar\nLos mismos supuestos de los modelos lineales se aplican a los ANOVA anidados: independencia, normalidad y homogeneidad de varianzas. La independencia debe considerarse en la etapa de diseño (ver el módulo de independencia). Ahora podemos verificar los supuestos de homogeneidad de varianza (es decir, que los residuos sean homogéneos, es decir, aproximadamente iguales) y normalidad (distribución equilibrada de los datos alrededor de la media, sin valores atípicos extraños).\nDebido a que las pruebas del factor fijo A utilizan las medias del factor anidado B, los supuestos de homogeneidad y normalidad se aplican con respecto a las medias del factor B. Es probable que la normalidad del factor A siga una distribución normal basada en el Teorema del Límite Central.\nRecomendamos una evaluación cualitativa de los supuestos, en lugar de una prueba formal como la prueba de Cochran. Los modelos lineales en general son bastante “robustos” ante la violación de estos supuestos (heterogeneidad y normalidad), dentro de ciertos límites.\nEl supuesto de normalidad se puede verificar mediante un histograma de frecuencias de los residuos o mediante un gráfico de cuantiles donde los residuos se trazan en función de los valores esperados de una distribución normal. El histograma de residuos debe seguir una distribución normal. Si los puntos en el gráfico de cuantiles se encuentran principalmente en la línea, los residuos se distribuyen de manera normal. Las violaciones de la normalidad se pueden corregir mediante transformaciones o mediante el uso de una distribución de errores diferente en un GLM. Consulta el módulo de GLM para obtener más información. La función plot() en un objeto de modelo proporciona una serie de diagnósticos gráficos del modelo, el segundo de los cuales es un gráfico de cuantiles.\n\nhist(Mink.nested$residuals)\n\n\n\nplot(Mink.nested, which = 2)\n\n\n\n\nHeterogeneidad de varianzas se indica por un patrón no aleatorio en el gráfico de residuos vs. ajustados. Si hay patrones fuertes, una solución potencial es transformar la variable de respuesta y. Si esto no soluciona el problema, la mejor solución es utilizar una distribución de error diferente en un marco de modelo lineal generalizado (GLM).\n\nplot(Mink.nested$residuals ~ Mink.nested$fitted)\n\n\n\n\n\n# Alternative option\nplot(Turtles.ANOVA, which = 1)\n\n\n\nComunicación de los resultados\nEscrita El factor de subgrupo rara vez es de interés y, por lo tanto, se le da poco énfasis en los resultados. Por ejemplo, la abundancia de topos fue significativamente mayor donde se eliminaron visones(mink) (F = 38.39, P &lt;0.001).\nVisual Un diagrama de caja sería una forma adecuada de mostrar las diferencias entre los grupos del factor de interés, en este caso Treatment.\n\n\nMás ayuda\nEscribe ?aov para obtener la ayuda de R para esta función.\nQuinn y Keough (2002) Experimental design and data analysis for biologist. Cambridge University Press.\nMcKillup (2012) Statistics explained. An introductory guide for life scientists. Cambridge University Press."
  },
  {
    "objectID": "statistics/linear-models/anova/anova-single/index.html",
    "href": "statistics/linear-models/anova/anova-single/index.html",
    "title": "ANOVA de un solo factor",
    "section": "",
    "text": "El análisis de varianza (ANOVA) es una de las técnicas más utilizadas en las ciencias biológicas y ambientales. El ANOVA se utiliza para contrastar una variable dependiente continua y en diferentes niveles de una o más variables independientes categóricas x. Las variables independientes se denominan factor o tratamiento, y las diferentes categorías dentro de ese tratamiento se denominan niveles. En este módulo, comenzaremos con el diseño más simple, aquellos con un solo factor.\nCuando se desean comparar las medias de grupo entre dos niveles, se utiliza una prueba t-test independiente. El ANOVA se utiliza para comparar las medias de grupo cuando hay más de dos niveles o cuando hay más de dos variables predictoras (ver ANOVA: factorial). La lógica de esta prueba es esencialmente la misma que la prueba t-test-independiente: compara la variación entre grupos con la variación dentro de los grupos para determinar si las diferencias observadas se deben al azar o no.\n\nPor ejemplo, para contrastar los tiempos de eclosión de los huevos de tortuga incubados a cuatro temperaturas diferentes (15°C, 20°C, 25°C y 30°C), el tiempo de eclosión es la variable de respuesta continua y la temperatura es la variable predictora categórica con cuatro niveles. La hipótesis nula sería que el tiempo medio de eclosión es igual para todas las temperaturas (Ho: \\(\\mu_{15} = \\mu_{20} = \\mu_{25} = \\mu_{30}\\)).\nCabe destacar que un ANOVA es un modelo lineal, al igual que la regresión lineal, excepto que las variables predictoras son categóricas en lugar de continuas.\n\\[y_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij}\\]\ndonde \\(\\mu\\) es la media general y \\(\\alpha_i\\) es el efecto del i-ésimo grupo.\nEs lo mismo que una regresión lineal múltiple con una variable predictora para cada nivel de la variable categórica (cada una codificada como una variable ficticia). Para la pregunta de si el tiempo de eclosión de las tortugas difiere entre las cuatro temperaturas de incubación, debemos ajustar cuatro parámetros para describir la respuesta media de cada temperatura (en lugar de solo una intercepción y una pendiente en una regresión lineal simple). Para este ejemplo, nuestra ecuación del modelo lineal tendrá esta forma:\n\\[TiempoDeEclosion = \\mu + \\beta_1.Temp_{15} + \\beta_2.Temp_{20} + \\beta_3.Temp_{25} + \\beta_4.Temp_{30} + \\varepsilon\\]\nEl ANOVA descompone la varianza total en un componente que puede explicarse por la variable predictora (entre los niveles del tratamiento) y un componente que no puede explicarse (dentro de los niveles, la varianza residual). El estadístico de prueba, F, es la razón de estas dos fuentes de variación.\n\\[F = \\frac{MS_{entre}}{MS_{dentro}}\\]\ndonde MS son los cuadrados medios, una medida de variación. La probabilidad de obtener el valor observado de F se calcula a partir de la conocida distribución de probabilidad de F, con dos grados de libertad (uno para el numerador = el número de niveles - 1) y uno para el denominador (número de réplicas por nivel - 1 x número de niveles).\n\nEjecutando el análisis\nLos datos deben estar formateados de tal manera que las réplicas individuales sean filas y las variables sean columnas separadas. Incluye una columna para la variable dependiente, y, y una columna correspondiente para la variable categórica, x. Descarga el conjunto de datos de muestra para el ejemplo de eclosión de tortugas, Turtles.csv, impórtalo en R y verifica que la variable de temperatura sea un factor con la función str.\n\nTurtles &lt;- read.csv(file = \"Turtles.csv\", header = TRUE)\nstr(Turtles)\n\nEn este caso, debido a que tenemos números para los cuatro niveles del tratamiento de Temperatura, necesitamos cambiar esa variable para que se convierta en un factor en lugar de un número entero.\n\nTurtles$Temperature &lt;- factor(Turtles$Temperature)\n\nAhora podemos ejecutar el análisis de varianza que contrasta el tiempo de eclosión (días) en diferentes temperaturas utilizando la función aov. Los argumentos de la función son simplemente una declaración de fórmula, y~x, donde la variable de respuesta se encuentra a la izquierda de ~, la variable predictora a la derecha y algún código para especificar qué marco de datos contiene esas variables.\n\nTurtles.aov &lt;- aov(Days ~ Temperature, data = Turtles)\n\nLa salida de este análisis se puede ver utilizando la función summary en el objeto creado.\n\nsummary(Turtles.aov)\n\nExactamente el mismo análisis se puede reproducir utilizando la función de modelo lineal lm.\n\nTurtles.lm &lt;- lm(Days ~ Temperature, data = Turtles)\nsummary(Turtles.lm)\n\n\n\nInterpretación de los resultados\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTemperature  3   8025  2675.2   15.98 9.08e-07 ***\nResiduals   36   6027   167.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa salida resumida de un objeto ANOVA es una tabla con los grados de libertad (Df), sumas de cuadrados (Sum Sq), cuadrados medios (Mean Sq) para la variable predictora (es decir, la variación entre los niveles de tu tratamiento) y para los residuos (es decir, la variación dentro de los niveles). También se presentan la estadística de prueba, el valor F y su valor p asociado (Pr(&gt;F)).\nPrimero verifica los grados de libertad. Los grados de libertad del factor Df = el número de niveles de tu factor - 1. Los grados de libertad residuales Df = a(n-1), donde : a es el número de niveles de tu factor y n es el tamaño de la muestra (repeticiones por nivel).\nLas sumas de cuadrados y los cuadrados medios son medidas de variación. La estadística F es la relación entre MSentre y MSdentro y el valor p es la probabilidad del valor F observado a partir de la distribución F (con los grados de libertad dados).\nLo principal a tener en cuenta en la tabla de ANOVA es si tu variable predictora tuvo un efecto significativo en tu variable de respuesta. En este ejemplo, la probabilidad de que las cuatro temperaturas de incubación sean iguales es &lt;0.001. Esto es muy poco probable y mucho menos que 0.05. Concluiríamos que hay una diferencia en los tiempos de eclosión entre las temperaturas. También nos interesa el valor de R^2, que nos indica cuánta variación fue explicada por el modelo.\nSi utilizas la función lm, obtienes un poco más de información en el resumen de la salida del modelo lineal.\n\n\n\nCall:\nlm(formula = Days ~ Temperature, data = Turtles)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.200  -9.225   1.650   9.025  19.400 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     58.400      4.092  14.273  &lt; 2e-16 ***\nTemperature20  -13.800      5.787  -2.385   0.0225 *  \nTemperature25   -9.200      5.787  -1.590   0.1206    \nTemperature30  -38.300      5.787  -6.619 1.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.94 on 36 degrees of freedom\nMultiple R-squared:  0.5711,    Adjusted R-squared:  0.5354 \nF-statistic: 15.98 on 3 and 36 DF,  p-value: 9.082e-07\n\n\nLa salida para la tabla de ANOVA estándar se encuentra al final y, encima de ella, obtienes los estimados de parámetros reales del modelo lineal (los \\(\\beta_1\\), \\(\\beta_2\\), etc. de arriba). En este ejemplo, las tortugas a 15°C eclosionaron después de 58.4 días, en promedio (la intercepción en el modelo). Los otros estimados de parámetros son las diferencias entre cada nivel de temperatura y la intercepción. Por ejemplo, a 20°C fueron 13.8 días más rápidas (es decir, la media para 20°C = 58.4-13.8 = 44.6 días).\nSi detectas alguna diferencia significativa en el ANOVA, nos interesa saber exactamente qué grupos difieren entre sí y cuáles no. Recuerda que un valor p significativo en la prueba que acabas de realizar rechazaría la hipótesis nula de que las medias de la variable dependiente son iguales en todos los grupos, pero no identificaría cuáles son diferentes entre sí. Para ver una comparación entre cada media y cada otra media, podemos utilizar una prueba post hoc de Tukey.\n\nTukeyHSD(Turtles.aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Days ~ Temperature, data = Turtles)\n\n$Temperature\n       diff       lwr        upr     p adj\n20-15 -13.8 -29.38469   1.784689 0.0982694\n25-15  -9.2 -24.78469   6.384689 0.3969971\n30-15 -38.3 -53.88469 -22.715311 0.0000006\n25-20   4.6 -10.98469  20.184689 0.8562615\n30-20 -24.5 -40.08469  -8.915311 0.0008384\n30-25 -29.1 -44.68469 -13.515311 0.0000785\n\n\n\n\nSupuestos a verificar\nLos supuestos importantes del ANOVA son la independencia, la homogeneidad de varianza y la normalidad. Recomendamos una evaluación cualitativa de los supuestos de normalidad y homogeneidad de varianza, examinando los patrones de variación en los residuos, en lugar de realizar una prueba formal como la prueba de Cochran. En general, los modelos lineales son bastante “robustos” frente a la violación de estos supuestos (heterogeneidad y normalidad), dentro de ciertos límites.\nNormalidad. El supuesto de normalidad se puede verificar mediante un histograma de frecuencias de los residuos o mediante el uso de un gráfico de cuantiles en el que los residuos se trazan frente a los valores esperados de una distribución normal. El histograma de residuos debería seguir una distribución normal. Si los puntos en el gráfico de cuantiles se encuentran principalmente en la línea, los residuos tienen una distribución normal. Ambos gráficos se pueden obtener a partir del objeto de modelo creado por la función aov.\n\npar(mfrow = c(1, 2)) \nhist(Turtles.aov$residuals)\nplot(Turtles.aov, which = 2)\n\n\n\n\nViolaciones de la normalidad se pueden solucionar mediante transformaciones o mediante el uso de una distribución de error diferente en un modelo lineal generalizado (GLM).\nHomogeneidad de varianza. La suposición de homogeneidad de varianza, es decir, que la variación en los residuos sea aproximadamente igual en todo el rango de la variable predictora, se puede verificar trazando los residuos contra los valores ajustados del objeto de modelo aov.\n\nplot(Turtles.aov, which = 1)\n\n\n\n\nVarianzas heterogéneas se identifican por un patrón no aleatorio en el gráfico de residuos versus valores ajustados. Busca una distribución uniforme de los residuos en el eje y para cada uno de los niveles en el eje x. Un distribución en forma de abanico con mayor varianza en valores más altos del eje x es un problema común cuando los datos están sesgados. Consulta el módulo de pruebas de suposiciones de modelos lineales para obtener más información. Si hay patrones fuertes, una solución potencial es transformar la variable de respuesta y. Si esto no soluciona el problema, la mejor solución es usar una distribución de error diferente en un modelo lineal generalizado (GLM).\nIndependencia. El ANOVA asume que todas las medidas replicadas son independientes entre sí (es decir, igualmente probables de ser muestreadas de la población de posibles valores para cada nivel). Este problema debe considerarse en la etapa de diseño. Si los datos están agrupados de alguna manera (por ejemplo, la mitad de los huevos de tortuga medidos en un momento y la otra mitad medidos más tarde), entonces se necesitan diseños más complejos para tener en cuenta factores adicionales (por ejemplo, un diseño con temperatura y tiempo como factores).\nExisten diversas medidas para tratar la no independencia. Estas incluyen asegurarse de que todos los predictores importantes estén en el modelo, promediar las observaciones anidadas o utilizar un modelo mixto.\n\n\nComunicación de los resultados\nEscrita. Los resultados de un ANOVA de un factor se expresan generalmente en texto como una frase corta, por ejemplo: “El tiempo de eclosión de las tortugas difirió entre las cuatro temperaturas de incubación (F = 15.98, gl = 3,36, p &lt; 0.001)”. Un efecto significativo sería seguido de una descripción escrita de los resultados de las pruebas post hoc (es decir, qué temperaturas difirieron de cuáles). Los resultados de las pruebas post hoc también se pueden agregar a la figura (por ejemplo, mediante la incorporación de etiquetas de los niveles que difieren).\nVisual. Un diagrama de caja o un gráfico de columnas con barras de error son adecuados para comparar una variable continua entre niveles de una variable categórica. Consulta la ayuda de gráficos para obtener versiones listas para su publicación de estas figuras.\n\nboxplot(Days ~ Temperature, data = Turtles, ylab = \"Tiempo de eclosión (días)\", xlab = \"Temperatura (°C)\")\n\n\n\n\n\n\nMás ayuda\nEscribe ?aov o ?lm para obtener la ayuda de R sobre estas funciones.\n\nQuinn y Keough (2002) Experimental design and data analysis for biologists. Cambridge University Press. Capítulo 8: Comparación de grupos o tratamientos - análisis de varianza.\n\n\nMcKillup (2012) Statistics explained. An introductory guide for life scientists. Cambridge University Press. Capítulo 11: Análisis de varianza de un solo factor.\n\n\nUnderwood, AJ (1997) Experiments in ecology: Their logical design and interpretation using analysis of variance. Cambridge University Press.\n\nAutor: James Lavender\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/linear-models/anova/index.html",
    "href": "statistics/linear-models/anova/index.html",
    "title": "ANOVA",
    "section": "",
    "text": "El análisis de varianza (ANOVA) es una de las técnicas más utilizadas en las ciencias biológicas y ambientales. El ANOVA se utiliza para contrastar una variable dependiente continua y en diferentes niveles de una o más variables independientes categóricas x. Las variables independientes se denominan factor o tratamiento, y las diferentes categorías dentro de ese tratamiento se denominan niveles. En este módulo, comenzaremos con el diseño más simple, aquellos con un solo factor.\nDonde se utilizaría una prueba t-test de muestras independientes para comparar las medias de grupos en dos niveles, el ANOVA se utiliza para la comparación de medias de grupos &gt;2, o cuando hay dos o más variables predictoras (ver ANOVA: factorial). La lógica de esta prueba es esencialmente la misma que la prueba t-test: compara la variación entre grupos con la variación dentro de los grupos para determinar si las diferencias observadas se deben al azar o no."
  },
  {
    "objectID": "statistics/linear-models/index.html",
    "href": "statistics/linear-models/index.html",
    "title": "Modelos Lineales",
    "section": "",
    "text": "Estas páginas contienen algunas introducciones respecto de los modelos lineales comúnmente utilizados que prueban la respuesta de una variable dependiente continua frente a una o más variables predictoras que pueden ser continuas o categóricas. Ten en cuenta que estos se denominan diferentes técnicas (por ejemplo, regresión vs. ANOVA) debido al uso común en la literatura que encontrarás; todos ellos involucran el mismo marco de modelado lineal.\n\n\n\n\n\n\nRegresión lineal\nAnálisis de varianza: factor único\nAnálisis de varianza: factorial\n\nComprensión de las interacciones\nInterpretación de coeficientes en modelos lineales\n\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/linear-models/interactions/index.html",
    "href": "statistics/linear-models/interactions/index.html",
    "title": "Comprendiendo las interacciones",
    "section": "",
    "text": "Los diseños de muestreo o experimentales con más de un factor nos brindan la oportunidad de probar si los factores interactúan. Una interacción entre factores ocurre cuando el efecto de un factor depende de las condiciones en el otro factor. Si no hay interacción, entonces los dos factores actúan de manera independiente. Al aprender estadística, las interacciones complican un poco nuestras vidas porque no podemos hacer afirmaciones simples sobre los efectos principales (cada factor de manera aislada): los resultados observados son debido a la combinación de factores. Sin embargo, es muy importante comprender esto, ya que a menudo nos interesa saber si varias variables ambientales actúan conjuntamente de manera independiente. Considera alguna de las siguientes preguntas:\n¿Varía el efecto de eliminar depredadores entre hábitats?\n¿Varía el efecto de la contaminación con la temperatura del agua?\n¿Varía el efecto de la temperatura con el suministro de alimento?\n¿Varía el efecto de la contaminación de nutrientes en diferentes tipos de suelo?\n¿Varían los beneficios de establecer una reserva según el tamaño de la reserva?\nTodas estas son preguntas que se responderían mediante una prueba estadística formal de la interacción entre factores.\n\n\n\nLa comprensión de las interacciones se puede entender mejor visualmente. Consideremos cómo el crecimiento de los corales puede verse afectado por dos factores de estrés muy importantes que actualmente afectan los océanos del mundo: el aumento de la temperatura y la acidificación. Un diseño experimental para probar esto podría ser aquel que cultiva corales en el laboratorio en todas las combinaciones posibles de varias temperaturas y varias condiciones de pH.\nPara simplificar, consideremos solo dos niveles de cada tratamiento: un tratamiento de control y un tratamiento de calentamiento para la temperatura, y un tratamiento de control y un tratamiento acidificado para el pH. Los corales se cultivarían en las cuatro combinaciones. Estas tres gráficas de crecimiento de corales ilustran escenarios en los que los dos factores de temperatura y pH no interactúan.\n\n\n\n\n\n\nEn la figura superior izquierda, hay un efecto del calentamiento (disminución del crecimiento en el tratamiento más cálido) y ningún efecto del pH.\nEn la figura superior derecha, hay un efecto del pH (disminución del crecimiento en el tratamiento ácido) y ningún efecto del calentamiento.\nEn la figura inferior, hay un efecto tanto de la temperatura (disminución del crecimiento en el tratamiento más cálido) como del pH (disminución del crecimiento en el tratamiento ácido).\n\nLos siguientes tres gráficos de crecimiento de coral ilustran escenarios en los que hay una interacción entre los factores de temperatura y pH.\n\n\n\n\n\n\nEn la figura superior izquierda, hay un efecto del pH (disminución del crecimiento en el tratamiento ácido) solo en el tratamiento cálido. El efecto del calentamiento solo es evidente en el tratamiento de pH ácido.\nEn la figura superior derecha, hay un efecto de la temperatura (disminución del crecimiento en el tratamiento cálido) y un efecto del pH (disminución del crecimiento en el tratamiento sin efecto del calentamiento), pero el efecto del tratamiento ácido es mayor en el tratamiento cálido. El efecto del calentamiento es mayor en el tratamiento ácido.\nEn la figura inferior, el efecto del tratamiento ácido en la temperatura cambia por completo en los dos tratamientos de temperatura. El crecimiento se reduce en el tratamiento ácido en la temperatura de control, pero aumenta en la temperatura cálida.\n\nEn todos estos casos con interacciones, debes tener en cuenta que las líneas no son paralelas (comparar con las primeras tres figuras anteriores).\nLas mismas ideas se aplican a diseños de muestreo o experimentales con más de dos niveles para cada factor, y para diseños en los que las variables predictoras son continuas en lugar de categóricas.\nEn esta figura, hay un efecto del pH (disminución del crecimiento en el tratamiento ácido) en los tratamientos de temperatura cálida y caliente, pero no en el tratamiento de temperatura de control. El crecimiento del coral siempre se redujo en el tratamiento caliente, pero en el tratamiento cálido, el crecimiento se desaceleró en comparación con el control solo en el tratamiento ácido.\n\nComunicación de resultados\nEscrita. Si tienes una interacción significativa entre dos factores en tu diseño, la comunicación de tus resultados requiere un texto que describa cómo los efectos de un factor dependen del otro. En los ejemplos con interacciones mencionados anteriormente, no puedes simplemente decir que “la acidificación del océano redujo el crecimiento de los corales” porque este resultado varió en función de los tratamientos de temperatura.\nEn diseños con factores categóricos, los tests post hoc se utilizan comúnmente para probar cuáles medias difieren entre sí (es decir, comparar todas las cuatro combinaciones de temperatura y pH en el diseño mencionado anteriormente). Los resultados de estos tests se pueden incluir en el texto o en las figuras para mostrar cuáles medias difieren entre sí. Luego tendrías soporte estadístico para afirmaciones que contrasten tratamientos particulares. Una frase para la sección de resultados en la figura superior izquierda de los gráficos de interacciones mencionados anteriormente podría ser algo como “La temperatura interactuó con el pH para afectar el crecimiento de los corales (Tabla 1), con un crecimiento reducido solo en el tratamiento cálido (Fig. 1)”, donde la Tabla 1 contendría los detalles estadísticos de la prueba realizada (por ejemplo, ANOVA factorial).\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/linear-models/linear-regression/index.html",
    "href": "statistics/linear-models/linear-regression/index.html",
    "title": "Regresión Lineal",
    "section": "",
    "text": "La regresión lineal es una de las técnicas estadísticas más ampliamente utilizadas en las ciencias de la vida y la tierra. Se utiliza para modelar la relación entre una variable de respuesta (también llamada variable dependiente) \\(y\\) y una o más variables explicativas (también llamadas variables independientes o predictoras) \\(x_{1}\\),\\(x_{2}\\)…\\(x_{n}\\). Por ejemplo, podríamos usar la regresión lineal para probar si la temperatura (la variable explicativa) es un buen predictor de la altura de las plantas (la variable de respuesta).\n\nEn la regresión lineal simple, con una única variable explicativa, el modelo toma la siguiente forma:\n\\[y = \\alpha + \\beta x + \\varepsilon \\]\ndonde \\(\\alpha\\) es la intersección (valor de \\(y\\) cuando \\(x\\) = 0), \\(\\beta\\) es la pendiente (cantidad de cambio en \\(y\\) por cada unidad de \\(x\\)) y \\(\\varepsilon\\) es el término de error. La inclusión del término de error, también llamado parte estocástica del modelo, es lo que hace que el modelo sea estadístico en lugar de matemático. El término de error se extrae de una distribución estadística que captura la variabilidad aleatoria en la respuesta. En la regresión lineal estándar se asume que esto sigue una distribución normal (Gaussiana).\nEs importante tener en cuenta que lo lineal en el modelo lineal no implica una relación lineal recta, sino más bien que la respuesta es una combinación lineal (aditiva) de los efectos de las variables explicativas. Sin embargo, debido a que tendemos a comenzar ajustando la relación más simple, muchos modelos lineales se representan mediante líneas rectas.\nTambién es importante destacar que la regresión lineal es solo un caso especial de un modelo lineal, donde tanto las variables de respuesta como las predictoras son continuas.\n\nEjecutando el análisis\nEl objetivo en la regresión lineal es obtener las mejores estimaciones para los coeficientes del modelo (\\(\\alpha\\) y \\(\\beta\\)). En R, puedes ajustar modelos lineales utilizando la función lm.\nPara este ejemplo práctico, descarga un conjunto de datos sobre alturas de plantas alrededor del mundo, Plant_height.csv, e impórtalo en R.\n\nPlant_height &lt;- read.csv(file = \"Plant_height.csv\", header = TRUE)\n\nEl argumento principal de lm es la fórmula del modelo y ~ x, donde la variable de respuesta se encuentra a la izquierda de la tilde (~) y la variable explicativa se encuentra a la derecha. lm también tiene un argumento opcional llamado data que te permite especificar un data frame del cual se tomarán las variables.\nPara probar si la altura de las plantas está asociada con la temperatura, modelaríamos la altura como la variable dependiente (en este caso estamos utilizando el logaritmo de la altura de las plantas) y la temperatura como la variable predictora.\n\nlm(loght ~ temp, data = Plant_height)\n\n\n\nInterpretación de los resultados\nPara obtener una salida detallada (por ejemplo, valores de coeficientes, R2, estadísticas de prueba, valores de p, intervalos de confianza, etc.), asigna la salida de la función lm a un nuevo objeto en R y utiliza la función summary para extraer información de ese objeto de modelo.\n\nmodel &lt;- lm(loght ~ temp, data = Plant_height)\nsummary(model)\n\n\nCall:\nlm(formula = loght ~ temp, data = Plant_height)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.97903 -0.42804 -0.00918  0.43200  1.79893 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.225665   0.103776  -2.175    0.031 *  \ntemp         0.042414   0.005593   7.583 1.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6848 on 176 degrees of freedom\nMultiple R-squared:  0.2463,    Adjusted R-squared:  0.242 \nF-statistic:  57.5 on 1 and 176 DF,  p-value: 1.868e-12\n\n\nLas estimaciones de los coeficientes te dan la pendiente e intersección. En este ejemplo, la ecuación de regresión para la altura (logarítmica) de las plantas en función de la temperatura es:\n\\[log(altura de la planta) = -0.22566 +0.0421.temperatura + \\varepsilon \\]\nLlamar a summary en un objeto de modelo produce mucha información útil, pero una de las cosas principales a tener en cuenta son las estadísticas t y los valores de p para cada coeficiente. Estas pruebas la hipótesis nula de que el valor real del coeficiente es 0.\nPor lo general, no nos importa si la intersección es cero o no, pero para el otro coeficiente (la pendiente), un valor significativamente diferente de cero indica que hay una asociación entre esa variable explicativa y la respuesta. En este ejemplo, un aumento en la temperatura se asocia con un aumento en la altura de las plantas.\nSi bien las estadísticas t y los valores de p indican una asociación significativa, la fuerza de la asociación se captura mediante el valor de R2, que es la proporción de varianza en la respuesta que se explica por la(s) variable(s) explicativa(s).\nLa estadística F y su valor de p asociado indican si el modelo en su conjunto es significativo. El modelo siempre será significativo si alguno de los coeficientes es significativo. Con solo una variable predictora, la probabilidad asociada a la prueba t, que prueba si la pendiente difiere de cero, es idéntica a la probabilidad asociada a la estadística F.\nTambién podemos obtener intervalos de confianza del 95% para los dos parámetros. Mostrar que los intervalos para la pendiente no incluyen cero es otra forma de mostrar que hay una asociación entre la variable dependiente y la variable predictora.\n\nconfint(model)\n\n                  2.5 %      97.5 %\n(Intercept) -0.43047074 -0.02085828\ntemp         0.03137508  0.05345215\n\n\n\n\nSupuestos a verificar\nLinealidad. ¡No tiene sentido tratar de ajustar una línea recta a datos que están curvados! Las relaciones curvilíneas producen patrones en forma de U en los gráficos de los residuos versus los valores ajustados. El uso de la función plot en un objeto de modelo proporciona una serie de diagnósticos gráficos del modelo, el primero de los cuales es un gráfico de residuos versus valores ajustados.\n\nplot(model, which = 1)\n\n\n\n\nLa ausencia de patrones fuertes en el gráfico anterior indica que el supuesto de linealidad es válido. Si hay patrones fuertes, una solución potencial es transformar el logaritmo del valor de respuesta. Observa que en el ejemplo anterior la altura de las plantas ya se había transformado logarítmicamente. Otra solución alternativa es ajustar un modelo lineal del valor de respuesta como una función polinómica (por ejemplo, cuadrática) del valor de respuesta. La forma más sencilla de hacer esto en R es utilizando la función poly.\nHaz clic aquí para ver una aplicación interactiva que muestra qué patrones de residuos se esperarían con relaciones curvadas.\nVarianza constante. Una distribución uniforme de los datos alrededor de la línea de regresión es deseable. Si el gráfico de residuos versus valores ajustados tiene forma de abanico, se viola el supuesto de varianza constante (también conocido como homogeneidad de varianza). La transformación logarítmica de la variable de respuesta puede solucionar este problema, pero si no lo hace, la mejor solución es utilizar una distribución de errores diferente en un marco de modelo lineal generalizado (GLM). Consulta Modelos lineales generalizados 1 para obtener más información.\nNormalidad. Las comprobaciones de si los datos siguen una distribución normal generalmente se realizan mediante la representación de un histograma de los residuos o mediante un gráfico de cuantiles donde los residuos se representan frente a los valores esperados de una distribución normal (el segundo de los gráficos obtenidos mediante plot(model)). Si los puntos en el gráfico de cuantiles se encuentran principalmente en la línea, los residuos siguen una distribución normal. Las violaciones de la normalidad se pueden solucionar mediante transformaciones o mediante el uso de una distribución de errores diferente en un GLM. Sin embargo, cabe destacar que la regresión lineal es razonablemente robusta frente a las violaciones de la normalidad.\n\npar(mfrow = c(1, 2)) # This code put two plots in the same window\nhist(model$residuals) # Histogram of residuals\nplot(model, which = 2) # Quantile plot\n\n\n\n\nIndependencia. Las observaciones de la variable respuesta deben ser independientes entre sí. Las observaciones no independientes son aquellas que están de alguna manera asociadas entre sí más allá de lo que se explica por la(s) variable(s) predictoras. Por ejemplo, las muestras recogidas en el mismo sitio, o repetidamente del mismo objeto, pueden ser más similares debido a algún factor adicional que no sea la variable explicativa medida. Garantizar la independencia es un problema de diseño experimental y de muestreo, y generalmente sabemos si los datos son independientes o no antes de nuestro análisis.\nExisten varias medidas para tratar la falta de independencia. Estas incluyen asegurarse de que todas las variables predictoras importantes estén en el modelo; promediar las observaciones anidadas; o utilizar un modelo mixto (ver Modelos mixtos 1).\n\n\nComunicación de los resultados\nEscrita. Los resultados de la regresión lineal se pueden presentar en el texto de diversas formas, pero a menudo basta con una breve oración, por ejemplo: “la altura de las plantas mostró una relación significativa (F = 57.5, p &lt; 0.01) negativa con la temperatura”. Sin embargo, si has realizado varios análisis (o si hay más de una variable predictora), puede ser útil presentar los resultados en forma de tabla con los valores de los coeficientes, los errores estándar y los valores de p para cada variable explicativa.\nVisual. Para una regresión lineal con una sola variable explicativa, es útil presentar los resultados en forma de gráfico de dispersión. La línea de mejor ajuste derivada del modelo se puede agregar con la función abline.\n\nplot(loght ~ temp, data = Plant_height, xlab = \"Temperature (C)\", ylab = \"log(Plant height)\", pch = 16)\nabline(model, col = \"red\")\n\n\n\n\n\n\nMás ayuda\nEscribe ?lm para obtener la ayuda de R para esta función.\n\nQuinn y Keough (2002) **Experimental design and data analysis for biologists.* Cambridge University Press. Capítulo 5: Correlación y regresión.\n\n\nMcKillup (2012) Statistics explained. An introductory guide for life scientists. Cambridge University Press. Capítulo 17: Regresión.\n\nMás consejos sobre la interpretación de coeficientes en modelos lineales\nAutor: Andrew Letten\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/linear-models/linear-regression/interpret-lm-coeffs/index.html",
    "href": "statistics/linear-models/linear-regression/interpret-lm-coeffs/index.html",
    "title": "Interpretación de Regresiones Lineales",
    "section": "",
    "text": "La interpretación de los coeficientes en modelos lineales (generalizados) es más sutil de lo que puedes darte cuenta, y tiene consecuencias en cómo probamos hipótesis y reportamos hallazgos. Comenzaremos hablando de las interpretaciones marginales vs. condicionales de los parámetros del modelo.\n\nEn este ejemplo, modelamos la altura de las plantas en función de la altitud y la temperatura. Estas variables están correlacionadas de manera negativa: hace más frío a mayor altitud. Comenzamos simulando algunos datos para reflejar esto.\n\nlibrary(mvtnorm)\n\n# Specify the sample size\nN &lt;- 1000\n\n# Specify the correlation between altitude and temperature\nrho &lt;- -0.4\n\n# This line of code creates two correlated variables\nX &lt;- rmvnorm(N, mean = c(0, 0), sigma = matrix(c(1, rho, rho, 1), 2, 2))\n\n# Extract the first and second columns to vectors named temp and alt and plot\ntemp &lt;- X[, 1]\nalt &lt;- X[, 2]\nplot(temp, alt)\n\n\n\n\nAhora podemos simular algunos datos de altura de las plantas. Aquí decimos que la altura media de las plantas es 2 (cuando todas las demás variables son 0). A medida que la temperatura aumenta en una unidad (manteniendo la altitud constante), la media de la altura aumentará en 1 unidad (beta[2] = 1), y de manera similar, a medida que aumentas la altitud en 1 unidad (manteniendo la temperatura constante), la media de la altura disminuirá en 1 (beta[3] = -1). La altura sigue una distribución normal con esta media y una desviación estándar de 2.\n\nbeta &lt;- c(2, 1, -1)\nmu &lt;- beta[1] + beta[2] * temp + beta[3] * alt\nheight &lt;- rnorm(N, mu, sd = 2)\n\nSi utilizamos un modelo lineal para encontrar los coeficientes, obtenemos lo que esperamos, estimaciones muy cercanas a los valores reales.\n\nlm_both &lt;- lm(height ~ temp + alt)\ndata.frame(estimated = round(lm_both$coefficients, 2), true = beta)\n\n            estimated true\n(Intercept)      1.95    2\ntemp             0.91    1\nalt             -1.02   -1\n\n\nLa interpretación de estos coeficientes es que si mantienes todo lo demás en el modelo constante (es decir, la temperatura) y agregas 1 a la altitud, entonces la altura media estimada disminuirá en 1.09. Ten en cuenta que el coeficiente depende de las unidades en las que se mide la altitud. Si la altitud se mide en metros, entonces el coeficiente te dice qué sucede cuando subes 1 metro.\nLa intersección es el valor predicho cuando todas las demás variables se establecen en 0, lo cual a veces tiene sentido (aquí sería la altura de las plantas a nivel del mar y a 0 grados de temperatura). Otras veces, 0 no es un valor significativo, y si deseas interpretar la intersección, podría tener sentido reescalar tus otras variables para que su media sea 0. Si haces esto, entonces la intersección es el valor predicho cuando todas las demás variables están en su nivel medio.\n¿Y si ahora tuviéramos un modelo solo con la temperatura?\n\nlm1 &lt;- lm(height ~ temp)\nlm1$coefficients\n\n(Intercept)        temp \n   1.949036    1.311288 \n\n\nEl coeficiente de temperatura ahora es 1.38, ¿qué está pasando? La altitud es un predictor importante de la altura de las plantas, y parte de la información sobre la altitud está contenida en la temperatura (recuerda que están correlacionadas, por lo que a medida que la altitud aumenta, la temperatura disminuye). El modelo tiene en cuenta esto cambiando el efecto de la temperatura para tener en cuenta la información que contiene sobre la altitud. Observa que el coeficiente de temperatura está incorrecto en aproximadamente 0.4, que es la cantidad de correlación entre las variables.\nNota: Cuando los estadísticos hablan de esto, usan las palabras condicional y marginal. Condicional es el efecto de una variable cuando las demás se mantienen constantes (como en lm_both), mientras que marginal es el efecto global (como en lm1). Nota: Si utilizas el código anterior para simular tus propios conjuntos de datos, obtendrás valores ligeramente diferentes para los coeficientes.\n\nPruebas de hipótesis\nEsta distinción tiene muchas consecuencias tanto para el modelado como para las pruebas de hipótesis. Generemos algunos datos en los que la altitud predice la altura, y la temperatura no tiene (información adicional), y luego probemos la temperatura.\n\nmu &lt;- 2 - 1 * alt\nheight &lt;- rnorm(N, mu, sd = 2)\n\nmod_temp &lt;- lm(height ~ temp)\nsummary(mod_temp)\n\n\nCall:\nlm(formula = height ~ temp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4669 -1.4440  0.0415  1.4019  6.3182 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.17215    0.06691  32.464  &lt; 2e-16 ***\ntemp         0.52793    0.06726   7.849 1.08e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.115 on 998 degrees of freedom\nMultiple R-squared:  0.05814,   Adjusted R-squared:  0.0572 \nF-statistic: 61.61 on 1 and 998 DF,  p-value: 1.076e-14\n\nanova(mod_temp)\n\nAnalysis of Variance Table\n\nResponse: height\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ntemp        1  275.7 275.714  61.609 1.076e-14 ***\nResiduals 998 4466.2   4.475                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa salida de este modelo nos está indicando que hay un efecto de la temperatura, aunque técnicamente no lo haya. No nos está dando información falsa si entendemos cómo interpretar los resultados del modelo. Debido a que la temperatura está correlacionada con la altitud, y hay un efecto de la altitud, cuando la altitud no está en el modelo, el modelo nos dice en general que hay un efecto de la temperatura que aumenta la altura en 0.45 (recuerda que la correlación fue 0.4). Si nuestra hipótesis es “¿La altura de las plantas cambia con la temperatura?”, la respuesta es sí, a mayor temperatura, más altas son las plantas.\nPero, ¿qué pasa con la altitud? Sabemos que el efecto de la temperatura que observamos se debe a que está correlacionado con la altitud, la temperatura no predice directamente la altura. Si queremos saber si hay un efecto de la temperatura después de controlar la altitud (manteniendo la altitud constante, es decir, condicional), entonces ajustamos el modelo con la altitud y luego probamos la temperatura.\n\nmod_temp_alt &lt;- lm(height ~ alt + temp)\nsummary(mod_temp_alt)\n\n\nCall:\nlm(formula = height ~ alt + temp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8432 -1.2804  0.0401  1.4006  5.8354 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.17447    0.06203  35.057   &lt;2e-16 ***\nalt         -0.89438    0.06976 -12.821   &lt;2e-16 ***\ntemp         0.17408    0.06818   2.553   0.0108 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.961 on 997 degrees of freedom\nMultiple R-squared:  0.1914,    Adjusted R-squared:  0.1898 \nF-statistic:   118 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\nanova(mod_temp_alt)\n\nAnalysis of Variance Table\n\nResponse: height\n           Df Sum Sq Mean Sq  F value  Pr(&gt;F)    \nalt         1  882.8  882.77 229.5506 &lt; 2e-16 ***\ntemp        1   25.1   25.07   6.5178 0.01083 *  \nResiduals 997 3834.1    3.85                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEl p-valor es aproximadamente 0.95, por lo que no tenemos evidencia de un efecto de la temperatura después de controlar la altitud.\nNota: La distinción entre interpretaciones condicionales y marginales también es válida para modelos lineales generalizados y modelos mixtos.\n\n\nCovariables categóricas\nCuando tenemos covariables categóricas (por ejemplo, tratamiento), hay varias formas de codificar el modelo, lo que dará diferentes interpretaciones para los coeficientes. Simulemos 120 puntos de datos con 40 en cada uno de los tres niveles de un tratamiento categórico.\n\nN &lt;- 120\n# The effect of treatment\ntrt.n &lt;- rep(c(-1, 0, 1), N / 3)\nmu &lt;- 2 + 1 * trt.n\n\n# Labels for the treatment\ntreatment &lt;- factor(rep(c(\"low\", \"med\", \"high\"), N / 3)) # group labels\n\n# Create, Y, a normally distributed response variable and plot against treatment\nY &lt;- rnorm(N, mu, sd = 2)\nboxplot(Y ~ treatment)\n\n\n\n\nSi introducimos el tratamiento como una covariable de la forma habitual, el modelo elegirá un tratamiento de referencia (aquí será “high” ya que los niveles se ordenan alfabéticamente), de modo que la intersección será la media de este grupo de referencia. Los demás coeficientes serán las diferencias entre los otros grupos y el grupo de referencia.\n\ncat_lm &lt;- lm(Y ~ treatment)\nsummary(cat_lm)\n\n\nCall:\nlm(formula = Y ~ treatment)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.763 -1.121 -0.085  1.138  6.566 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.3214     0.3305  10.049  &lt; 2e-16 ***\ntreatmentlow  -2.4975     0.4674  -5.343 4.56e-07 ***\ntreatmentmed  -1.8318     0.4674  -3.919  0.00015 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.09 on 117 degrees of freedom\nMultiple R-squared:  0.2075,    Adjusted R-squared:  0.1939 \nF-statistic: 15.31 on 2 and 117 DF,  p-value: 1.238e-06\n\n\nEntonces, el grupo “high” tiene una media de 2.65, y la diferencia entre las medias del grupo “low” y el grupo “high” es de -0.66, y la diferencia entre el grupo “med” y el grupo “high” es de -1.48. Si deseas tener otro grupo como grupo de referencia, puedes usar relevel para recodificar tu factor de tratamiento.\n\ntreatment &lt;- relevel(treatment, ref = \"low\")\ncat_lm &lt;- lm(Y ~ treatment)\nsummary(cat_lm)\n\n\nCall:\nlm(formula = Y ~ treatment)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.763 -1.121 -0.085  1.138  6.566 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.8239     0.3305   2.493   0.0141 *  \ntreatmenthigh   2.4975     0.4674   5.343 4.56e-07 ***\ntreatmentmed    0.6657     0.4674   1.424   0.1571    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.09 on 117 degrees of freedom\nMultiple R-squared:  0.2075,    Adjusted R-squared:  0.1939 \nF-statistic: 15.31 on 2 and 117 DF,  p-value: 1.238e-06\n\nboxplot(Y ~ treatment)\n\n\n\n\nAhora la intersección es la media del grupo “low”, y todos los demás coeficientes son las diferencias entre el grupo “low” y los demás. Otra cosa que puedes hacer es poner -1 en el modelo para eliminar la intersección y tener solo las medias de cada grupo como coeficientes.\n\ncat_lm &lt;- lm(Y ~ treatment - 1)\nsummary(cat_lm)\n\n\nCall:\nlm(formula = Y ~ treatment - 1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.763 -1.121 -0.085  1.138  6.566 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \ntreatmentlow    0.8239     0.3305   2.493   0.0141 *  \ntreatmenthigh   3.3214     0.3305  10.049  &lt; 2e-16 ***\ntreatmentmed    1.4896     0.3305   4.507 1.57e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.09 on 117 degrees of freedom\nMultiple R-squared:  0.5215,    Adjusted R-squared:  0.5092 \nF-statistic: 42.51 on 3 and 117 DF,  p-value: &lt; 2.2e-16\n\n\nAhora, los tres coeficientes son las medias de los grupos.\nContraste de los coeficientes También podemos ver los contrastes; estos son las diferencias entre todos los pares de grupos. Carga el paquete multcomp y utiliza glht (hipótesis lineales generales) para examinar todas las diferencias de pares.\n\nlibrary(multcomp)\n\ncont &lt;- glht(cat_lm, linfct = mcp(treatment = \"Tukey\"))\n\nsummary(cont)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = Y ~ treatment - 1)\n\nLinear Hypotheses:\n                Estimate Std. Error t value Pr(&gt;|t|)    \nhigh - low == 0   2.4975     0.4674   5.343  &lt; 1e-04 ***\nmed - low == 0    0.6657     0.4674   1.424 0.331852    \nmed - high == 0  -1.8318     0.4674  -3.919 0.000436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nCada línea de esta salida compara dos grupos entre sí. La primera línea, por ejemplo, compara el grupo “high” con el grupo “low”. Por lo tanto, la diferencia entre las medias de los grupos “high” y “low” es de 1.84. Los valores de p y los intervalos de confianza proporcionados por glht controlan las pruebas múltiples, lo cual es útil. Si deseas ver los intervalos de confianza para las diferencias entre los grupos.\n\nconfint(cont)\n\n\n     Simultaneous Confidence Intervals\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = Y ~ treatment - 1)\n\nQuantile = 2.3738\n95% family-wise confidence level\n \n\nLinear Hypotheses:\n                Estimate lwr     upr    \nhigh - low == 0  2.4975   1.3880  3.6070\nmed - low == 0   0.6657  -0.4439  1.7752\nmed - high == 0 -1.8318  -2.9414 -0.7223\n\n\nNota: En un modelo con múltiples covariables, las mismas reglas siguen aplicándose en cuanto a las interpretaciones condicionales y marginales de los coeficientes.\nInterpretación de los coeficientes en modelos lineales generalizados En los modelos lineales, la interpretación de los parámetros del modelo es lineal, como se discutió anteriormente. Para los modelos lineales generalizados, ahora lee la página de tutoriales sobre la interpretación de los coeficientes en esos modelos.\nAutor: Gordana Popovic Año: 2016 Última actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/meta-analysis/index.html",
    "href": "statistics/meta-analysis/index.html",
    "title": "Meta-Análisis",
    "section": "",
    "text": "EL meta-análisis se utilizan cada vez más en ecología, evolución y ciencias ambientales para encontrar patrones generales entre muchos estudios, resolver controversias entre estudios conflictivos y generar nuevas hipótesis. Estos tutoriales ofrecen una introducción para realizar meta-análisis con el paquete R metafor.\n\nMeta-análisis 1: Introducción y cálculo de tamaños de efecto\nMeta-análisis 2: Modelos de efecto fijo y efecto aleatorio\nMeta-análisis 3: Modelos más complejos"
  },
  {
    "objectID": "statistics/meta-analysis/meta-analysis-1/index.html",
    "href": "statistics/meta-analysis/meta-analysis-1/index.html",
    "title": "Introduction al meta-análisis",
    "section": "",
    "text": "Antecedentes\n¿Qué es un meta-análisis?\n\nEl meta-análisis es un resumen cuantitativo de estudios sobre el mismo tema.\n\n¿Por qué queremos realizar un meta-análisis?\n\nEncontrar generalidades\nAumentar el poder y la precisión\nExplorar las diferencias entre estudios\nResolver controversias entre estudios conflictivos (pruebas de hipótesis)\nGenerar nuevas hipótesis\n\n\nEl proceso de meta-análisis\n¿Cuántos pasos involucra el meta-análisis?\nUna respuesta basica se forma de 5 pasos\n\nFormular preguntas e hipótesis o encontrar un tema\nBúsqueda bibliográfica y recopilación de artículos\nExtracción de datos y codificación\nMeta-análisis y pruebas de sesgo de publicación\nInforme y publicación\n\nEn este tutorial solo consideramos el paso iv. Debes aprender los otros pasos en otro lugar. Para comenzar, recientemente escribimos un artículo de revisión que divide el proceso de meta-análisis en 10 preguntas (Nakagawa et al. 2017). Las 10 preguntas te guiarán para evaluar la calidad de un meta-análisis.\n\n¿La búsqueda es sistemática y está documentada de manera transparente?\n¿Qué pregunta y qué tamaño de efecto?\n¿Se tiene en cuenta la no independencia?\n¿Qué modelo meta-analítico?\n¿Se informa el nivel de consistencia entre los estudios?\n¿Se investigan las causas de la variación entre los estudios?\n¿Se interpretan los efectos en términos de importancia biológica?\n¿Se ha considerado el sesgo de publicación?\n¿Los resultados son realmente robustos e imparciales?\n¿Se resume el estado actual (y la falta) de conocimiento?\n\n\n\nMetafor para meta-análisis\nCreo que el paquete R metafor (Viechtbauer 2010) es el software meta-analítico más completo y el autor Wolfgang Viechtbauer, quien, debo decir, tiene el nombre más genial entre mis amigos, todavía lo está desarrollando activamente.\nPrimero, instala y carga el paquete metafor.\n\nlibrary(metafor)\nlibrary(metadat)\n\nMira el conjunto de datos llamado dat.curtis1998 incluido en el paquete metadat.\n\ndat &lt;- metadat::dat.curtis1998\nstr(dat)\n\n'data.frame':   102 obs. of  20 variables:\n $ id      : int  21 22 27 32 35 38 44 63 86 87 ...\n $ paper   : int  44 44 121 121 121 121 159 183 209 209 ...\n $ genus   : chr  \"ALNUS\" \"ALNUS\" \"ACER\" \"QUERCUS\" ...\n $ species : chr  \"RUBRA\" \"RUBRA\" \"RUBRUM\" \"PRINUS\" ...\n $ fungrp  : chr  \"N2FIX\" \"N2FIX\" \"ANGIO\" \"ANGIO\" ...\n $ co2.ambi: num  350 350 350 350 350 350 350 395 350 350 ...\n $ co2.elev: num  650 650 700 700 700 700 700 795 700 700 ...\n $ units   : chr  \"ul/l\" \"ul/l\" \"ppm\" \"ppm\" ...\n $ time    : int  47 47 59 70 64 50 730 365 365 365 ...\n $ pot     : chr  \"0.5\" \"0.5\" \"2.6\" \"2.6\" ...\n $ method  : chr  \"GC\" \"GC\" \"GH\" \"GH\" ...\n $ stock   : chr  \"SEED\" \"SEED\" \"SEED\" \"SEED\" ...\n $ xtrt    : chr  \"FERT\" \"FERT\" \"NONE\" \"NONE\" ...\n $ level   : chr  \"HIGH\" \"CONTROL\" \".\" \".\" ...\n $ m1i     : num  6.82 2.6 2.99 5.91 4.61 ...\n $ sd1i    : num  1.77 0.667 0.856 1.742 1.407 ...\n $ n1i     : int  3 5 5 5 4 5 3 3 20 16 ...\n $ m2i     : num  3.94 2.25 1.93 6.62 4.1 ...\n $ sd2i    : num  1.116 0.328 0.552 1.631 1.257 ...\n $ n2i     : int  5 5 5 5 4 3 3 3 20 16 ...\n\n\nEste conjunto de datos es del artículo de Curtis y Wang (1998). Analizaron el efecto del aumento de CO\\(_2\\) en rasgos de las plantas (principalmente cambios en la biomasa). Por lo tanto, tenemos información sobre el grupo de control (1) y el grupo experimental (2) (m = media, sd = desviación estándar), junto con información de especies y detalles experimentales. En meta-análisis, estas variables a menudo se denominan ‘moderadores’ (hablaremos un poco más sobre esto más adelante).\n\n\n\nCálculo de tamaños de efecto ‘estandarizados’\nPara comparar el efecto del aumento de CO\\(_2\\) en varios estudios, primero necesitamos calcular un tamaño de efecto para cada estudio, una métrica que cuantifica la diferencia entre nuestros grupos de control y experimental.\nExisten varios tamaños de efecto ‘estandarizados’, que son adimensionales. Cuando tenemos dos grupos para comparar, utilizamos dos tipos de estadísticas de tamaño de efecto. El primero es la diferencia de medias estandarizada (SMD, también conocida como \\(d\\) de Cohen o \\(d\\) de Hedge o \\(g\\); existen algunas diferencias sutiles entre ellos, pero no nos preocuparemos por ellas por ahora):\n\\[\\begin{equation}\n\\mathrm{SMD}=\\frac{\\bar{x}_{E}-\\bar{x}_{C}}{\\sqrt{\\frac{(n_{C}-1)sd^2_{C}+(n_{E}-1)sd^2_{E}}{n_{C}+n_{E}-2}}}\n\\end{equation}\\]\ndonde \\(\\bar{x}_{C}\\) y \\(\\bar{x}_{E}\\) son las medias del grupo de control y experimental, respectivamente, \\(sd\\) es la desviación estándar de la muestra (\\(sd^2\\) es la varianza de la muestra) y \\(n\\) es el tamaño de la muestra.\nY la varianza del error de la muestra es:\n\\[\\begin{equation}\nse^2_{\\mathrm{SMD}}= \\frac{n_{C}+n_{E}}{n_{C}n_{E}}+\\frac{\\mathrm{SMD}^2}{2(n_{C}+n_{E})}\n\\end{equation}\\]\nLa raíz cuadrada de esto se denomina ‘error estándar’ (o desviación estándar de la estimación, ¿confuso?). El inverso de esto (\\(1/se^2\\)) se utiliza como ‘peso’, pero las cosas son un poco más complicadas que esto, como descubriremos a continuación.\nOtro índice común se llama ‘cociente de respuesta’, que generalmente se presenta en su forma de logaritmo natural (lnRR):\n\\[\\begin{equation}\n\\mathrm{lnRR}=\\ln\\left({\\frac{\\bar{x}_{E}}{\\bar{x}_{C}}}\\right)\n\\end{equation}\\]\nY la varianza del error de muestreo es:\n\\[\\begin{equation}\nse^2_\\mathrm{lnRR}=\\frac{sd^2_{C}}{n_{C}\\bar{x}^2_{C}}+\\frac{sd^2_{E}}{n_{E}\\bar{x}^2_{E}}\n\\end{equation}\\]\nObtenemos esto utilizando la función escalc de metafor. Para obtener la diferencia media estandarizada, utilizamos:\n\n# SMD\nSMD &lt;- escalc(measure = \"SMD\", n1i = dat$n1i, n2i = dat$n2i, m1i = dat$m1i, m2i = dat$m2i,\n    sd1i = dat$sd1i, sd2i = dat$sd2i)\n\ndonde n1i y n2i son los tamaños de muestra, m1i y m2i son las medias, y sd1i y sd2i son las desviaciones estándar de cada uno de los estudios.\nEl objeto creado ahora tiene un tamaño de efecto (yi) y su varianza (vi) para cada estudio.\n\n\n\n       yi     vi \n1  1.8222 0.7408 \n2  0.5922 0.4175 \n3  1.3286 0.4883 \n4 -0.3798 0.4072 \n5  0.3321 0.5069 \n6  2.5137 0.9282 \n\n\nPara obtener la razón de respuesta (logaritmo transformado de la razón de medias), usaríamos:\n\nlnRR &lt;- escalc(measure = \"ROM\", n1i = dat$n1i, n2i = dat$n2i, m1i = dat$m1i, m2 = dat$m2i,\n    sd1i = dat$sd1i, sd2i = dat$sd2i)\n\nEl artículo original utilizó lnRR, así que lo usaremos, pero es posible que desees repetir el análisis a continuación utilizando SMD para ver si los resultados son consistentes.\nAgrega los tamaños de efecto al conjunto de datos original con cbind o bind_cols del paquete dplyr.\n\nlibrary(dplyr)\ndat &lt;- bind_cols(dat, lnRR)\n\nDeberías ver que se agregan yi (tamaño de efecto) y vi (varianza de muestreo).\n\n\n'data.frame':   102 obs. of  22 variables:\n $ id      : int  21 22 27 32 35 38 44 63 86 87 ...\n $ paper   : int  44 44 121 121 121 121 159 183 209 209 ...\n $ genus   : chr  \"ALNUS\" \"ALNUS\" \"ACER\" \"QUERCUS\" ...\n $ species : chr  \"RUBRA\" \"RUBRA\" \"RUBRUM\" \"PRINUS\" ...\n $ fungrp  : chr  \"N2FIX\" \"N2FIX\" \"ANGIO\" \"ANGIO\" ...\n $ co2.ambi: num  350 350 350 350 350 350 350 395 350 350 ...\n $ co2.elev: num  650 650 700 700 700 700 700 795 700 700 ...\n $ units   : chr  \"ul/l\" \"ul/l\" \"ppm\" \"ppm\" ...\n $ time    : int  47 47 59 70 64 50 730 365 365 365 ...\n $ pot     : chr  \"0.5\" \"0.5\" \"2.6\" \"2.6\" ...\n $ method  : chr  \"GC\" \"GC\" \"GH\" \"GH\" ...\n $ stock   : chr  \"SEED\" \"SEED\" \"SEED\" \"SEED\" ...\n $ xtrt    : chr  \"FERT\" \"FERT\" \"NONE\" \"NONE\" ...\n $ level   : chr  \"HIGH\" \"CONTROL\" \".\" \".\" ...\n $ m1i     : num  6.82 2.6 2.99 5.91 4.61 ...\n $ sd1i    : num  1.77 0.667 0.856 1.742 1.407 ...\n $ n1i     : int  3 5 5 5 4 5 3 3 20 16 ...\n $ m2i     : num  3.94 2.25 1.93 6.62 4.1 ...\n $ sd2i    : num  1.116 0.328 0.552 1.631 1.257 ...\n $ n2i     : int  5 5 5 5 4 3 3 3 20 16 ...\n $ yi      : num  0.547 0.143 0.438 -0.113 0.117 ...\n  ..- attr(*, \"ni\")= int [1:102] 8 10 10 10 8 8 6 6 40 32 ...\n  ..- attr(*, \"measure\")= chr \"ROM\"\n $ vi      : num  0.0385 0.0175 0.0328 0.0295 0.0468 ...\n\n\nVisualización del tamaño de efecto. Podemos visualizar las estimaciones puntuales (tamaño de efecto) y sus intervalos de confianza del 95%, basados en la varianza del error de muestreo, utilizando la función forest, que dibuja un diagrama de bosque para nosotros.\n\nforest(dat$yi, dat$vi)\n\n\n\n\nEl problema que observas es que cuando hay muchos estudios, un diagrama de bosque no funciona realmente (¡a menos que tengas una pantalla muy grande!). Veamos solo los primeros 12 estudios.\n\nforest(dat$yi[1:12], dat$vi[1:12])\n\n\n\n\nPodemos calcular muchos tipos diferentes de tamaños de efecto con escalc; otras estadísticas comunes de tamaño de efecto incluyen \\(Zr\\) (correlación transformada z de Fisher). Por cierto, junto con mis colegas, hemos propuesto un nuevo tamaño de efecto estandarizado llamado lnCVR (el logaritmo de la razón del coeficiente de variación, ¡es un nombre largo!), que compara la variabilidad de dos grupos en lugar de las medias. Veamos si puedes calcularlo con estos datos. De hecho, la versión de desarrollo de metafor te permite hacer esto con escalc - página de GitHub. lnCVR se llama “CVR” en escalc. De hecho, si reanalizas estos datos con lnCVR, ¡puede que puedas publicar un artículo! Nadie lo ha hecho aún. ¡Hazlo esta noche!\nUna vez que hayas calculado los tamaños de efecto, pasa a la siguiente página: Meta-análisis 2: modelos de efectos fijos y efectos aleatorios\n\n\nMás ayuda (referencias)\nVisita el sitio web del paquete metafor. Allí encontrarás muchos ejemplos prácticos.\n\nCurtis, P. S., y X. Z. Wang. 1998. A meta-analysis of elevated CO2 effects on woody plant mass, form, and physiology. Oecologia 113:299-313.\n\n\nNakagawa, S., R. Poulin, K. Mengersen, K. Reinhold, L. Engqvist, M. Lagisz y A. M. Senior. 2015. Meta-analysis of variation: ecological and evolutionary applications and beyond. Methods in Ecology and Evolution 6:143-152.\n\n\nViechtbauer, W. 2010. Conducting meta-analyses in R with the metafor package. Journal of Statistical Software 36:1-48.\n\nAutores: Shinichi Nakagawa y Malgorzata (Losia) Lagisz\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/meta-analysis/meta-analysis-2/index.html",
    "href": "statistics/meta-analysis/meta-analysis-2/index.html",
    "title": "Fixed-effect and Random-effect Models",
    "section": "",
    "text": "Primero lee nuestra introducción al metaanálisis con instrucciones para calcular tamaños del efecto. Ahora, consideramos los modelos estadísticos para estimar los tamaños del efecto medio y la influencia de las variables moderadoras.\n\nModelos de efecto fijo y efecto aleatorio\nExisten dos modelos principales de metaanálisis: 1) el modelo de efecto fijo y 2) el modelo de efecto aleatorio (en realidad, se menciona un tercer tipo, pero este es una extensión del segundo modelo). Debido a que los efectos fijos tienen un significado diferente en otro contexto, esta denominación puede resultar confusa. Por lo tanto, ahora las personas llaman al primer modelo ‘modelo de efecto común’. Puedes ver una representación visual de estos dos modelos (de la Figura 4 en Nakagawa et al. 2017).\n\n\n\n\n\n\n\n\nUna representación matemática del modelo de efecto común es:\n\\[\\begin{equation}\ny_i=b_0+e_i,\n\\\\\ne_i\\sim \\mathcal{N}(0,v_i),\\\n\\end{equation}\\]\\end{equation}\ndonde \\(y_i\\) es el tamaño del efecto \\(i\\) (del \\(i\\)-ésimo estudio), \\(b_0\\) es la media general (o media metaanalítica), \\(e_i\\) es una desviación de la media general y sigue una distribución normal con varianza específica del estudio \\(v_i\\) (que se puede obtener a partir de las fórmulas anteriores). Ten en cuenta que los pesos para este modelo son \\(1/v_i\\). Como puedes ver, este es un modelo muy simple; básicamente, estamos estimando la media considerando los pesos.\nEs importante destacar que este modelo asume que existe una media común para todos los estudios incluidos. ¿Es realista esto? Probablemente no, especialmente si estás combinando diferentes especies.\nEl modelo de efecto aleatorio se puede escribir de la siguiente manera:\n\\[\\begin{equation}\ny_i=b_0+s_i+e_i,\n\\\\\ns_i\\sim \\mathcal{N}(0,\\tau^2),\\\n\\\\\ne_i\\sim \\mathcal{N}(0,v_i),\\\n\\end{equation}\\]e_i(0,v_i),\n\\end{equation}\ndonde \\(s_i\\) es una desviación específica del estudio de la media general para el \\(i\\)-ésimo estudio, sigue una distribución normal con la varianza entre estudios, a menudo referida como \\(\\tau^2\\), y los demás términos son iguales a los mencionados anteriormente. A diferencia del modelo de efecto común, un modelo de efecto aleatorio asume que diferentes estudios tienen diferentes medias. Ten en cuenta que los pesos para este modelo son \\(1/(\\tau^2+v_i)\\). Volveremos a este punto, ya que resulta ser bastante importante.\n\n\nEjecutando un modelo de efecto común\nUtilizaremos la función rma de metafor para ejecutar un modelo de efecto común.\n\ncommon_m &lt;- rma(yi = yi, vi = vi, method = \"FE\", data = dat)\n\nEspecificamos el tamaño del efecto (yi), su varianza (vi), el método (“FE” para efecto fijo) y el marco de datos (dat).\nPara ver los resultados, usa summary en el objeto del modelo:\n\nsummary(common_m)\n\n\nFixed-Effects Model (k = 102)\n\n   logLik   deviance        AIC        BIC       AICc   \n-245.9580   769.0185   493.9160   496.5410   493.9560   \n\nI^2 (total heterogeneity / total variability):   86.87%\nH^2 (total variability / sampling variability):  7.61\n\nTest for Heterogeneity:\nQ(df = 101) = 769.0185, p-val &lt; .0001\n\nModel Results:\n\nestimate      se     zval    pval   ci.lb   ci.ub      \n  0.2088  0.0054  38.3374  &lt;.0001  0.1982  0.2195  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n¡Listo, fue fácil! La media general es estadísticamente significativa y está alrededor de 0.2. ¿Qué significa 0.2? Convertámoslo de vuelta a la escala original: una proporción de respuesta entre las medias del grupo de control y experimental.\n\nexp(0.2)\n\n[1] 1.221403\n\n\nEsto significa que un rasgo de planta (por ejemplo, la masa) fue un 22% más grande en el grupo experimental (RR\\(=\\bar{x}_{E}/\\bar{x}_{C}\\)), lo cual parece ser un efecto bastante grande (recuerda que necesitamos interpretar nuestros resultados de manera biológicamente significativa). Ten en cuenta que la desigualdad de Jensen establece que esto está un poco mal, pero no entraremos en eso hoy.\n\n\nEjecutando un modelo de efectos aleatorios\nAhora pasamos al modelo de efectos aleatorios, que es un modelo más realista. Nuevamente, utilizamos la función rma, pero esta vez cambiamos el método a REML, que es el método predeterminado y el mejor método para el metanálisis de efectos aleatorios.\n\nrandom_m &lt;- rma(yi = yi, vi = vi, method = \"REML\", data = dat)\nsummary(random_m)\n\n\nRandom-Effects Model (k = 102; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n  7.0449  -14.0898  -10.0898   -4.8596   -9.9674   \n\ntau^2 (estimated amount of total heterogeneity): 0.0262 (SE = 0.0053)\ntau (square root of estimated tau^2 value):      0.1619\nI^2 (total heterogeneity / total variability):   88.90%\nH^2 (total variability / sampling variability):  9.01\n\nTest for Heterogeneity:\nQ(df = 101) = 769.0185, p-val &lt; .0001\n\nModel Results:\n\nestimate      se     zval    pval   ci.lb   ci.ub      \n  0.2553  0.0198  12.8899  &lt;.0001  0.2165  0.2941  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCompare la media general de este modelo con el modelo de efecto común. Oh, la media general del modelo de efectos aleatorios es realmente mayor que la del modelo de efectos fijos. Vale, eso a veces sucede (descubriremos que esto probablemente sea una sobreestimación más tarde). Esperamos que el intervalo de confianza del 95% sea más amplio (es decir, más realista) en este modelo de efectos aleatorios, ya que este modelo tiene una mejor suposición que el modelo de efecto común.\n\n\nComprensión de la heterogeneidad\nLa salida del modelo de efectos aleatorios tiene más elementos que el modelo de efecto común. Tenemos tau^2 (\\(\\tau^2\\)) y I^2 (\\(I^2\\)), dos medidas muy comunes de heterogeneidad (nota que H^2, o \\(H^2\\), es una transformación de \\(I^2\\)).\n\nLa heterogeneidad es la variación en los tamaños del efecto que no se explica por la varianza del error de muestreo.\n\nEn otras palabras, es la variación real en los datos. Creo que \\(I^2\\) es un índice bastante importante, ya que puede indicar el porcentaje de variación real en tus datos metaanalíticos. Se puede calcular de la siguiente manera (como viste en la figura del modelo de efectos aleatorios anterior):\n\\[\\begin{equation}\nI^2=\\frac{\\tau^2}{(\\tau^2+\\bar{v})},\n\\end{equation}\\]\ndonde \\(\\bar{v}\\) es un valor representativo de \\(v_i\\) (o piensa en \\(\\bar{v}\\) como el promedio de \\(v_i\\), aunque no es exactamente eso). Observa que el denominador es la varianza total que existe en los datos. Los valores de referencia para \\(I^2\\) son 25, 50 y 75% para heterogeneidad baja, moderada y alta, respectivamente (Higgins et al. 2003).\nNuestro valor de \\(I^2\\) es del 88.9%, muy alto, y este valor es, como esperabas, estadísticamente significativo al ser probado con el valor \\(Q\\) (que sigue una distribución \\(\\chi^2\\) definida por el valor df, en este caso \\(df = 101\\)).\nRecientemente, hicimos un metaanálisis de metaanálisis (un metaanálisis secundario) para analizar cuál es el valor promedio de \\(I^2\\) en el campo de la ecología y la evolución (Senior et al. 2016). ¡El valor promedio fue del 92%! Por lo tanto, esto indica que realmente deberíamos ajustar el modelo de efectos aleatorios en lugar del modelo de efecto común, ya que este último asume que la heterogeneidad es cero o \\(\\tau^2=0\\) e \\(I^2 = 0\\). ¿O realmente es así? Lo descubriremos más adelante.\n\n\nMeta-regresión (el modelo de efectos aleatorios)\nLa existencia de heterogeneidad establece el escenario para la meta-regresión. Esto significa que ahora introducimos predictores (llamados ‘moderadores’ en la terminología metaanalítica) en nuestro modelo para explicar la heterogeneidad (equivalente a los modelos de regresión normales).\nAjustar una meta-regresión también es bastante sencillo. En este ejemplo, ajustaremos tres moderadores que fueron recopilados por los autores: 1) time (cuánto tiempo duró el experimento), 2) method (diferentes formas de aumentar el CO\\(_2\\)) y 3) fungroup (grupo funcional, es decir, angiospermas, gimnospermas o fijadores de N\\(_2\\)).\nUtilizamos nuevamente rma, pero ahora con una declaración de modelo.\n\nmetareg &lt;- rma(yi = yi, vi = vi, mod = ~time + method + fungrp, method = \"REML\",\n    data = dat)\nsummary(metareg)\n\n\nMixed-Effects Model (k = 102; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n  5.1938  -10.3876    3.6124   21.5628    4.8851   \n\ntau^2 (estimated amount of residual heterogeneity):     0.0267 (SE = 0.0056)\ntau (square root of estimated tau^2 value):             0.1634\nI^2 (residual heterogeneity / unaccounted variability): 87.16%\nH^2 (unaccounted variability / sampling variability):   7.79\nR^2 (amount of heterogeneity accounted for):            0.00%\n\nTest for Residual Heterogeneity:\nQE(df = 96) = 658.4083, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:6):\nQM(df = 5) = 2.9089, p-val = 0.7140\n\nModel Results:\n\n             estimate      se     zval    pval    ci.lb   ci.ub      \nintrcpt        0.3043  0.0516   5.8934  &lt;.0001   0.2031  0.4055  *** \ntime          -0.0001  0.0001  -1.0509  0.2933  -0.0002  0.0001      \nmethodGH      -0.0369  0.0567  -0.6501  0.5157  -0.1481  0.0743      \nmethodOTC      0.0308  0.0902   0.3410  0.7331  -0.1461  0.2076      \nfungrpGYMNO   -0.0454  0.0605  -0.7501  0.4532  -0.1640  0.0732      \nfungrpN2FIX    0.0044  0.1701   0.0258  0.9794  -0.3291  0.3379      \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBueno, ¡no explican nada! Observa el valor \\(R^2\\) y, como esperabas, las pruebas para moderadores (nuevamente el valor Q) indican que no son significativos. ¡Un modelo terrible! Así que nos damos por vencidos aquí (en un metaanálisis real, necesitas hacer esto de manera más sistemática, preferiblemente basado en tus hipótesis a priori).\n\n\nComprobando el sesgo de publicación\nBien, parece que un aumento de CO\\(_2\\) promueve el crecimiento de las plantas (lo cual puede no ser sorprendente), pero estamos asumiendo que el conjunto de datos que tenemos no sufre de sesgo de publicación.\n\nEl sesgo de publicación, en su forma más simple, significa que los resultados significativos son más propensos a ser publicados que los resultados no significativos.\n\nPero existen varios métodos que las personas han estado utilizando. Los dos métodos más comunes, a menudo utilizados en conjunto, son: 1) gráfico de embudo (funnel plot), que se utiliza para detectar una asimetría en el embudo (un indicio de sesgo de publicación), y 2) la prueba de regresión de Egger, con la cual se prueba estadísticamente la asimetría del embudo.\n¿A qué me refiero con ‘asimetría del embudo’? Si trazamos los tamaños de efecto y su error estándar en orden descendente (\\(se\\); ver figura a continuación), se supone que veremos un embudo invertido donde los tamaños de efecto con un \\(se\\) bajo (o pesos altos) están más estrechamente agrupados que los tamaños de efecto con un \\(se\\) alto (o pesos bajos). Pero esta forma de embudo solo ocurre cuando no hay sesgo de publicación. Si existe, deberíamos ver una asimetría en el embudo. Esto se debe a que los estudios con tamaños de muestra pequeños (es decir, \\(se\\) alto, lo que conduce a no significancia) tienen menos probabilidades de ser publicados. Veremos un ejemplo de esta asimetría del embudo en nuestro conjunto de datos.\nPara crear un gráfico de embudo y ejecutar la prueba de Egger:\n\nfunnel(random_m)\n\n\n\n\n\n# Note that the orignal Egger's test is regtest(random_m, model='lm')\nregtest(random_m)\n\n\nRegression Test for Funnel Plot Asymmetry\n\nModel:     mixed-effects meta-regression model\nPredictor: standard error\n\nTest for Funnel Plot Asymmetry: z = 3.2046, p = 0.0014\nLimit Estimate (as sei -&gt; 0):   b = 0.1584 (CI: 0.0890, 0.2278)\n\n\nEl gráfico de embudo y la prueba de Egger sugieren asimetría en el embudo. Pero debemos tener cuidado. La asimetría del embudo no solo puede ser causada por sesgo de publicación, sino también por heterogeneidad (uno o más moderadores no detectados que distorsionan la forma del embudo). Dado que tenemos mucha varianza inexplicada (es decir, heterogeneidad), no estamos seguros de qué está causando esta asimetría. Pero tenemos algunas evidencias de sesgo de publicación. Un punto relevante es que si la metarregresión explica mucha heterogeneidad, debes usar ese modelo de metarregresión en la función regrest (en nuestro caso, regrest(metareg), pero no lo hice porque nuestro modelo metareg no explicaba nada).\nExiste un método alternativo, que tiene un nombre interesante, el método de recorte y relleno (trim-and-fill). Verás la razón por la que se llama así. Podemos utilizar este método utilizando la función trimfill.\n\n# Note that we are using the defult estimator ('L0'), but there are two others\n# availablere\ntf_m &lt;- trimfill(random_m)\ntf_m\n\n\nEstimated number of missing studies on the left side: 13 (SE = 6.5629)\n\nRandom-Effects Model (k = 115; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0421 (SE = 0.0076)\ntau (square root of estimated tau^2 value):      0.2053\nI^2 (total heterogeneity / total variability):   92.06%\nH^2 (total variability / sampling variability):  12.59\n\nTest for Heterogeneity:\nQ(df = 114) = 872.7669, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2166  0.0227  9.5234  &lt;.0001  0.1721  0.2612  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfunnel(tf_m)\n\n\n\n\nComo puedes ver, este método utiliza la asimetría para agregar más puntos y proporcionar una media general revisada, que es menor que la del modelo de efectos aleatorios original. Aunque este efecto sigue siendo significativo, este método podría convertir una media general significativa en una no significativa. Pero en lugar de considerar esto como una estimación real de la media general, debemos verlo como parte de un análisis de sensibilidad.\nExisten más métodos para realizar pruebas de sesgo de publicación, ninguno de los cuales es perfecto, pero debemos realizar algunas de estas pruebas (ver más en Nakagawa et al. 2017 y las referencias allí citadas).\n\n\nOtro modelo (que no conocía), ¡pero importante!\nRecientemente aprendí que existe otro modelo meta-analítico (ni común ni aleatorio) que es más robusto frente al sesgo de publicación. Recuerda lo que mencioné anteriormente.\n\nEl modelo de efectos aleatorios es un modelo más realista, por lo que deberíamos usar este modelo.\nEl modelo de efectos aleatorios tiene un peso de \\(1/(\\tau^2+v_i)\\).\n\\(I^2\\) es muy grande en conjuntos de datos meta-ecológicos y meta-evolutivos, lo que significa que \\(\\tau^2\\) es muy grande, lo que implica que \\(1/(\\tau^2+v_i) \\approx 1/\\tau^2\\) ya que \\(v_i\\) es insignificante.\nEntonces, todos los puntos de datos tienen el mismo peso \\(1/\\tau^2\\), ¡y básicamente se convierte en un modelo no ponderado!\n\nEsto es problemático porque bajo el sesgo de publicación, como vimos anteriormente, los tamaños de efecto con tamaños de muestra pequeños (valores de \\(v_i\\) muy altos) deberían tener un peso menor en comparación con los tamaños de efecto con tamaños de muestra grandes (valores de \\(v_i\\) bajos). Cuando existe sesgo de publicación, el modelo de efectos aleatorios podría proporcionarnos una estimación sesgada… ¿Qué podríamos hacer? ¿Podríamos usar el peso de \\(1/v_i\\) como en el modelo común, pero utilizar la estructura del modelo de efectos aleatorios? ¡Resulta que sí se puede!\n\n# We make weights, which is 1/vi and stick that into the argument, weights\ndat[, \"wi\"] &lt;- 1/dat$vi\nweight_m &lt;- rma(yi = yi, vi = vi, weights = wi, method = \"REML\", data = dat)\nsummary(weight_m)\n\n\nRandom-Effects Model (k = 102; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n  4.2927   -8.5854   -4.5854    0.6448   -4.4630   \n\ntau^2 (estimated amount of total heterogeneity): 0.0262 (SE = 0.0053)\ntau (square root of estimated tau^2 value):      0.1619\nI^2 (total heterogeneity / total variability):   88.90%\nH^2 (total variability / sampling variability):  9.01\n\nTest for Heterogeneity:\nQ(df = 101) = 769.0185, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2088  0.0473  4.4121  &lt;.0001  0.1161  0.3016  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEn realidad, este estimador puntual es exactamente el mismo que el del modelo de efecto común (esto no es solo una coincidencia, ya que este modelo calcula la media general como el modelo de efecto común). Pero es importante tener en cuenta que este modelo tiene un intervalo de confianza del 97% más amplio que el modelo de efecto común, y este nivel de incertidumbre es más realista (al igual que el modelo de efectos aleatorios). Básicamente, ¡este modelo tiene lo mejor de ambos mundos!\nEste modelo se basa en el artículo de Henmi y Copas (2010). Aunque es más robusto al sesgo de publicación, no conozco ningún metanálisis en el campo de la ecología y la evolución que utilice este modelo. ¿Probablemente deberíamos usar este modelo en lugar de un modelo de efectos aleatorios? Creo que sí, llamemos a este modelo el ‘modelo robusto’. Creo que también deberíamos usar este modelo robusto para la meta-regresión. Agradezco a Wolfgang por hablarme de este modelo.\nSi estás de acuerdo con todo esto, pasa a los modelos más complejos.\n\n\nMás ayuda (referencias)\n¿Alguna pregunta? Envíame un correo electrónico a s(-dot-)nakagawa(-at-)unsw(-dot-)edu(-dot-)au. También visita nuestro sitio web.\nVisita el sitio web del paquete ‘metafor’. Allí encontrarás muchos ejemplos prácticos.\n\nHenmi, M., and J. B. Copas. 2010. Confidence intervals for random effects meta-analysis and robustness to publication bias. Statistics in Medicine 29:2969-2983.\n\n\nHiggins, J. P. T., S. G. Thompson, J. J. Deeks, and D. G. Altman. 2003. Measuring inconsistency in meta-analyses. British Medical Journal 327:557-560.\n\n\nNakagawa, S., D. W. A. Noble, A. M. Senior, and M. Lagisz. 2017. Meta-evaluation of meta-analysis: ten appraisal questions for biologists. BMC Biology 15:18.\n\n\nSenior, A. M., C. E. Grueber, T. Kamiya, M. Lagisz, K. O’Dwyer, E. S. A. Santos, and S. Nakagawa. 2016. Heterogeneity in ecological and evolutionary meta-analyses: its magnitude and implications. Ecology 97:3293-3299.\n\nAutores: Shinichi Nakagawa y Malgorzata (Losia) Lagisz\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/meta-analysis/meta-analysis-3/index.html",
    "href": "statistics/meta-analysis/meta-analysis-3/index.html",
    "title": "Multilevel Models",
    "section": "",
    "text": "Primero lee nuestra introducción al metaanálisis y familiarízate con los modelos estadísticos comúnmente utilizados para el metaanálisis. Ahora, consideremos modelos más complejos."
  },
  {
    "objectID": "statistics/meta-analysis/meta-analysis-3/index.html#más-ayuda-referencias",
    "href": "statistics/meta-analysis/meta-analysis-3/index.html#más-ayuda-referencias",
    "title": "Multilevel Models",
    "section": "Más ayuda (referencias)",
    "text": "Más ayuda (referencias)\n¿Tienes alguna pregunta? O envíame un correo electrónico a s(-dot-)nakagawa(-at-)unsw(-dot-)edu(-dot-)au. También visita nuestro sitio web.\nVisita el sitio web del paquete metafor. Allí encontrarás muchos ejemplos prácticos.\n\nCornwell, W., and S. Nakagawa. 2017. Métodos comparativos filogenéticos. Current Biology 27:R333-R336.\n\n\nNakagawa, S., D. W. A. Noble, A. M. Senior, and M. Lagisz. 2017. Meta-evaluación de meta-análisis: diez preguntas de evaluación para biólogos. BMC Biology 15:18.\n\n\nNakagawa, S., and E. S. A. Santos. 2012. Aspectos metodológicos y avances en meta-análisis biológico. Evolutionary Ecology 26:1253-1274.\n\n\nNoble, D. W. A., M. Lagisz, R. E. O’Dea, and S. Nakagawa. 2017. Análisis de no independencia y sensibilidad en meta-análisis ecológicos y evolutivos. Molecular Ecology 26:2410-2425.\n\nAutores: Shinichi Nakagawa y Malgorzata (Losia) Lagisz\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/mixed-models/index.html",
    "href": "statistics/mixed-models/index.html",
    "title": "Modelos mixtos",
    "section": "",
    "text": "Los modelos mixtos son aquellos que tienen una mezcla de efectos fijos y aleatorios. Los efectos aleatorios son factores categóricos cuyos niveles han sido seleccionados entre muchos posibles niveles, y el investigador desea hacer inferencias más allá de los niveles elegidos. Es un concepto complicado, pero imagina que contrastas dos tipos de hábitat (bosque y pradera) muestreando cinco sitios dentro de cada uno, y cinco medidas replicadas dentro de cada sitio. El tipo de hábitat es un factor fijo en el que el investigador solo está interesado en esos dos niveles de tipo de hábitat. Si los cinco sitios se seleccionaron de una colección más grande de sitios posibles, entonces el sitio se considera un efecto aleatorio con 10 niveles.\n\nModelos mixtos 1: Modelos mixtos lineales con un efecto aleatorio.\nModelos mixtos 2: Modelos mixtos lineales con varios efectos aleatorios.\nModelos mixtos 3: Modelos mixtos lineales generalizados.\n\n Photo: N. Murray"
  },
  {
    "objectID": "statistics/mixed-models/mixed-model-1/index.html",
    "href": "statistics/mixed-models/mixed-model-1/index.html",
    "title": "Introducción",
    "section": "",
    "text": "Modelos lineales mixtos con un efecto aleatorio\nNecesitarás utilizar modelos de efectos mixtos si tienes un factor aleatorio en tu diseño experimental. Un factor aleatorio:\nEste es un concepto difícil de comprender y se explica mejor con un ejemplo. Los datos que analizaremos aquí son recuentos de invertebrados en 3-4 sitios de cada uno de los 7 estuarios (elegidos al azar). Aquí, los estuarios son el efecto aleatorio, ya que hay un gran número de estuarios posibles y solo tomamos una muestra aleatoria de algunos de ellos, pero nos gustaría hacer inferencias sobre los estuarios en general.\nIntroduciremos los modelos mixtos en tres partes:\nLas tres páginas utilizarán los mismos datos para ilustración."
  },
  {
    "objectID": "statistics/mixed-models/mixed-model-1/index.html#propiedades-de-los-modelos-mixtos",
    "href": "statistics/mixed-models/mixed-model-1/index.html#propiedades-de-los-modelos-mixtos",
    "title": "Introducción",
    "section": "Propiedades de los modelos mixtos",
    "text": "Propiedades de los modelos mixtos\nSuposiciones. Los modelos mixtos tienen algunas suposiciones importantes (las revisaremos más adelante para nuestros ejemplos):\n\nLas observaciones \\(y\\) son independientes, condicionales a algunos predictores \\(x\\).\nLa respuesta \\(y\\) sigue una distribución normal condicional a algunos predictores \\(x\\).\nLa respuesta \\(y\\) tiene una varianza constante, condicional a algunos predictores \\(x\\).\nExiste una relación lineal entre \\(y\\) y los predictores \\(x\\) y los efectos aleatorios \\(z\\).\nLos efectos aleatorios \\(z\\) son independientes de \\(y\\).\nLos efectos aleatorios \\(z\\) siguen una distribución normal.\n\n\nEjecutando el análisis\nUtilizaremos el paquete lme4 para todos nuestros modelos de efectos mixtos. Nos permitirá modelar tanto datos continuos como discretos con uno o más efectos aleatorios. Primero, instala y carga este paquete:\n\nlibrary(lme4)\n\nAnalizaremos un conjunto de datos que tuvo como objetivo probar el efecto de la contaminación del agua en la abundancia de algunos invertebrados marinos submareales, comparando muestras de estuarios modificados y prístinos. Como los recuentos totales son grandes, asumiremos que los datos son continuos. Más adelante, en Modelos mixtos 3, modelaremos los recuentos como discretos utilizando modelos lineales mixtos generalizados (GLMM).\nDescarga el conjunto de datos de muestra, Estuaries.csv, y cárgalo en R.\n\nEstuaries &lt;- read.csv(\"Estuaries.csv\", header = T)\n\nAjuste de un modelo con un efecto fijo y aleatorio\nEn este conjunto de datos, tenemos un efecto fijo (Modificación; modificado vs prístino) y un efecto aleatorio (Estuario). Podemos usar la función lmer para ajustar un modelo para cualquier variable dependiente con una distribución continua. Para ajustar un modelo para la abundancia total, usaríamos:\n\nft.estu &lt;- lmer(Total ~ Modification + (1 | Estuary), data = Estuaries, REML = T)\n\ndonde Total es la variable dependiente (a la izquierda de ~), Modificación es el efecto fijo, y Estuario es el efecto aleatorio.\nObserva que la sintaxis para un efecto aleatorio es (1|Estuario) - esto ajusta una intercepción diferente (por lo tanto, 1) para cada Estuario.\nEste modelo se puede ajustar mediante máxima verosimilitud (REML=F) o máxima verosimilitud restringida (&gt;REML=T). Para ajustar modelos, es mejor usar REML, ya que es menos sesgado (imparcial para muestras equilibradas), especialmente en muestras pequeñas. Sin embargo, para usar la función anova a continuación, necesitamos volver a ajustar con máxima verosimilitud.\n\n\nSupuestos a verificar\nAntes de examinar los resultados de nuestro análisis, es importante verificar que nuestros datos cumplan con los supuestos del modelo que utilizamos. Veamos todos los supuestos en orden.\nSupuesto 1: Las observaciones \\(y\\) son independientes, condicionales a algunos efectos fijos \\(x\\) y efectos aleatorios \\(z\\)\nNo podemos verificar este supuesto, pero puedes asegurarte de que sea cierto tomando una muestra aleatoria dentro de cada nivel del efecto aleatorio en tu diseño experimental.\nSupuesto 2: La respuesta \\(y\\) se distribuye normalmente, condicional a algunos predictores \\(x\\) y efectos aleatorios \\(z\\)\nEste supuesto solo es crítico cuando tenemos un tamaño de muestra pequeño o datos muy sesgados. Podemos verificarlo con un gráfico cuantil-normal de los residuos.\n\nqqnorm(residuals(ft.estu))\n\n\n\n\nEstamos buscando una relación lineal. Aquí, la suposición de normalidad parece razonable.\nSuposición 3: La respuesta \\(y\\) tiene varianza constante, condicional a algunos efectos fijos \\(x\\) y efectos aleatorios \\(z\\).\nAl igual que en un modelo lineal, un modelo mixto asume una varianza constante. Podemos verificar esto buscando una forma de abanico en el gráfico de residuos (residuos vs valores ajustados).\n\nscatter.smooth(residuals(ft.estu) ~ fitted(ft.estu))\n\n\n\n\nEste gráfico de residuos parece razonable, hay diferencias en la variabilidad entre estuarios, pero la variabilidad no aumenta con la media. Ten en cuenta que la función scatter.smooth es simplemente un gráfico de dispersión con una curva ajustada y suavizada.\nSuposición 4: Existe una relación lineal entre \\(y\\) y los predictores \\(x\\) y los efectos aleatorios \\(z\\).\nPara verificar esta suposición, revisamos nuevamente el gráfico de residuos en busca de no linealidad o forma de U. En nuestro caso, no hay evidencia de no linealidad. Si los residuos parecen ir hacia abajo y luego hacia arriba, o hacia arriba y luego hacia abajo, es posible que necesitemos agregar una función polinómica de los predictores utilizando la función poly.\nSuposición 5: Los efectos aleatorios \\(z\\) son independientes de \\(y\\).\nNo podemos verificar esta suposición, pero puedes asegurarte de que sea verdadera tomando una muestra aleatoria de estuarios.\nSuposición 6: Los efectos aleatorios \\(z\\) siguen una distribución normal.\nEsta suposición no es crucial (y difícil) de verificar.\n\n\nInterpretación de los resultados\nPrueba de hipótesis para el efecto fijo\nEl paquete lme4 no proporcionará valores p para los efectos fijos como parte de la salida en summary. Esto se debe a que los valores p de las pruebas de Wald (usando summary) y las pruebas de razón de verosimilitud (usando anova) son solo aproximados en modelos mixtos.\nNo obstante, utilizaremos la función anova para probar un efecto de modificación en la abundancia total de invertebrados, teniendo en cuenta el efecto aleatorio del estuario.\nPrimero, ajustamos el modelo completo por máxima verosimilitud y un segundo modelo que carece del efecto fijo de la modificación.\n\nft.estu &lt;- lmer(Total ~ Modification + (1 | Estuary), data = Estuaries, REML = F)\nft.estu.0 &lt;- lmer(Total ~ (1 | Estuary), data = Estuaries, REML = F)\n\nEntonces, comparamos estos dos modelos con una prueba de razón de verosimilitud utilizando la función anova.\n\nanova(ft.estu.0, ft.estu)\n\nData: Estuaries\nModels:\nft.estu.0: Total ~ (1 | Estuary)\nft.estu: Total ~ Modification + (1 | Estuary)\n          npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nft.estu.0    3 415.02 420.99 -204.51   409.02                       \nft.estu      4 411.92 419.87 -201.96   403.92 5.1055  1    0.02385 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEncontramos evidencia de un efecto de Modificación (p = 0.02385).\nTambién podemos calcular intervalos de confianza para cada parámetro del modelo utilizando la función confint.\n\nconfint(ft.estu)\n\nComputing profile confidence intervals ...\n\n\n                          2.5 %    97.5 %\n.sig01                 2.718166 12.538348\n.sigma                 7.676352 11.522837\n(Intercept)           31.918235 49.981321\nModificationPristine -26.360731 -2.538241\n\n\nEsto también proporciona evidencia de un efecto de Modificación, ya que este parámetro (es decir, la diferencia entre los estuarios modificados y prístinos) tiene intervalos de confianza del 95% que no se superponen con cero.\nPrueba de hipótesis para efectos aleatorios\nPuedes utilizar la función anova para probar efectos aleatorios, pero los valores p son muy aproximados y no recomendamos este procedimiento. En su lugar, utilizaremos un bootstrap paramétrico. Este es un método basado en simulación que implica una buena cantidad de código, pero no hay mucho sobre el código que debas cambiar para diferentes modelos, principalmente es solo cuestión de copiar y pegar.\nBootstrap paramétrico\n\nnBoot &lt;- 1000\nlrStat &lt;- rep(NA, nBoot)\nft.null &lt;- lm(Total ~ Modification, data = Estuaries) # modelo nulo\nft.alt &lt;- lmer(Total ~ Modification + (1 | Estuary), data = Estuaries, REML = F) # modelo alternativo\nlrObs &lt;- 2 * logLik(ft.alt) - 2 * logLik(ft.null) # estadístico de prueba observado\nfor (iBoot in 1:nBoot)\n{\n  Estuaries$TotalSim &lt;- unlist(simulate(ft.null)) # datos remuestreados\n  bNull &lt;- lm(TotalSim ~ Modification, data = Estuaries) # modelo nulo\n  bAlt &lt;- lmer(TotalSim ~ Modification + (1 | Estuary), data = Estuaries, REML = F) # modelo alternativo\n  lrStat[iBoot] &lt;- 2 * logLik(bAlt) - 2 * logLik(bNull) # estadístico de prueba remuestreado\n}\nmean(lrStat &gt; lrObs) # Valor p para la prueba del efecto de Estuario\n\n[1] 0.001\n\n\n\n\nPreguntas frecuentes sobre modelos mixtos\n1. ¿Necesito muestras balanceadas para ajustar un modelo mixto?\nNo, los diseños no balanceados están bien. Sin embargo, los diseños balanceados generalmente te darán mejor poder estadístico, por lo que es bueno apuntar a ellos.\n2. ¿Debo muestrear muchos niveles del efecto aleatorio o muchas observaciones dentro de cada nivel?\nEsto depende de lo que te interese. En nuestro ejemplo, nos interesa el efecto de la modificación. En el diseño del estudio, las desembocaduras de los ríos se encuentran directamente debajo de la modificación, por lo que necesitamos muchas desembocaduras de ríos dentro de cada nivel de modificación para obtener una buena inferencia sobre los efectos de la modificación. Esto es cierto en general, necesitas muchas muestras en el nivel inferior al nivel que te interesa principalmente.\n3. ¿Mi factor aleatorio debe ser un efecto aleatorio?\nNo necesariamente. Si tienes un factor aleatorio (es decir, tienes una muestra aleatoria de categorías de una variable categórica) y quieres hacer inferencias sobre esa variable en general, no solo en las categorías que observaste, entonces inclúyelo como un efecto aleatorio. Si estás satisfecho haciendo inferencias solo sobre los niveles que observaste, entonces puedes incluirlo como un efecto fijo. En nuestro ejemplo, queríamos hacer inferencias sobre la modificación en general, es decir, en cada desembocadura de río modificada y no modificada, por lo que incluimos la desembocadura de río como un efecto aleatorio. Si hubiéramos tratado la desembocadura de río como un factor fijo, nos habríamos limitado a hacer conclusiones solo sobre las desembocaduras de ríos que muestreamos.\n4. ¿Qué pasa si los niveles de mi factor no son realmente aleatorios?\nEsto podría ser un problema ya que la suposición 4 podría no cumplirse. Siempre debes muestrear el efecto aleatorio de manera aleatoria para evitar sesgos y conclusiones incorrectas.\n\n\nComunicación de los resultados\nEscrita. Los resultados de los modelos mixtos lineales se comunican de manera similar a los resultados de los modelos lineales. En la sección de resultados, debes mencionar que estás utilizando modelos mixtos con el paquete R lme4, y listar tus efectos aleatorios y fijos. También debes mencionar cómo llevaste a cabo la inferencia, es decir, pruebas de razón de verosimilitud (usando la función anova) o bootstrap paramétrico. En la sección de resultados para un predictor, basta con escribir una línea, por ejemplo: “Hay evidencia sólida (p&lt;0.001) de un efecto negativo de la modificación en la abundancia total”. Para múltiples predictores, es mejor mostrar los resultados en una tabla.\nVisual. La mejor manera de comunicar visualmente los resultados dependerá de tu pregunta. Para un modelo mixto simple con un efecto aleatorio, una opción es un gráfico de los datos en bruto con las medias del modelo superpuestas. Se requiere un poco de código para este tipo de gráfico, y será un poco diferente para tus datos y modelo.\n\nModEst &lt;- unique(Estuaries[c(\"Estuary\", \"Modification\")]) # encontrar qué Estuarios están modificados\n\n# Preparar un vector de colores con colores específicos para los niveles de modificación\n\nmyColors &lt;- ifelse(unique(ModEst$Modification) == \"Modified\", rgb(0.1, 0.1, 0.7, 0.5),\n  ifelse(unique(ModEst$Modification) == \"Pristine\", rgb(0.8, 0.1, 0.3, 0.6),\n    \"grey90\"\n  )\n)\n\nboxplot(Total ~ Estuary, data = Estuaries, col = myColors, xlab = \"Estuary\", ylab = \"Total invertebrates\")\nlegend(\"bottomleft\",\n  inset = .02,\n  c(\" Modified \", \" Pristine \"), fill = unique(myColors), horiz = TRUE, cex = 0.8\n)\n\n# 0 si está Modificado, 1 si está Prístino\nis.mod &lt;- ifelse(unique(ModEst$Modification) == \"Modified\", 0,\n  ifelse(unique(ModEst$Modification) == \"Pristine\", 1, NA)\n)\n\nEst.means &lt;- coef(ft.estu)$Estuary[, 1] + coef(ft.estu)$Estuary[, 2] * is.mod # Medias del modelo\n\nWarning in coef(ft.estu)$Estuary[, 2] * is.mod: longitud de objeto mayor no es\nmúltiplo de la longitud de uno menor\n\nstripchart(Est.means ~ sort(unique(Estuary)), data = Estuaries, pch = 18, col = \"red\", vertical = TRUE, add = TRUE)\n\n\n\n\n\n\nMás ayuda\nPuedes escribir ?lmer en R para obtener ayuda con estas funciones.\nBorrador de capítulo del libro de los autores de lme4.\n\nFaraway, JJ. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press, 2005.\n\n\nZuur, A, EN Ieno and GM Smith. Analysing ecological data. Springer Science & Business Media, 2007.\n\nAutor: Gordana Popovic\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/mixed-models/mixed-model-2/index.html",
    "href": "statistics/mixed-models/mixed-model-2/index.html",
    "title": "Factores Cruzados y Anidados",
    "section": "",
    "text": "Mixed models 1 es una introducción a los modelos mixtos con un factor aleatorio. Después de leer eso, si crees que tienes más de un factor aleatorio, continúa leyendo. Por ejemplo, es posible que tengas factores cruzados o anidados.\nMuchos diseños experimentales en ecología y ciencias ambientales requieren modelos mixtos con varios efectos aleatorios (factores). Es posible que hayas oído hablar de factores anidados y cruzados. A menudo definimos estos como diseños bastante distintos (por ejemplo, de www.theanalysisfactor.com)\nDos factores están cruzados cuando cada categoría (nivel) de un factor coocurre en el diseño con cada categoría del otro factor. En otras palabras, hay al menos una observación en cada combinación de categorías para los dos factores.\nUn factor está anidado dentro de otro factor cuando cada categoría del primer factor coocurre con solo una categoría del otro. En otras palabras, una observación debe estar dentro de una categoría del Factor 2 para tener una categoría específica del Factor 1. No se representan todas las combinaciones de categorías.\nTambién existen diseños intermedios que están parcialmente cruzados, donde algunos niveles de un factor ocurren en varios (pero no todos) los niveles del segundo factor. Estos diseños a menudo se han enseñado como problemas separados con diferentes formas de llevar a cabo análisis de varianza (ANOVA) dependiendo de si tienes factores cruzados o anidados. Usando modelos mixtos con el paquete lme4, podemos pensar en todos estos en un marco de trabajo, donde los diseños anidados y cruzados se modelan de la misma manera. Pensar en los factores como cruzados o anidados se simplifica mediante una identificación cuidadosa de los niveles del factor, pero hablaremos más sobre esto más adelante."
  },
  {
    "objectID": "statistics/mixed-models/mixed-model-2/index.html#comunicación-de-los-resultados",
    "href": "statistics/mixed-models/mixed-model-2/index.html#comunicación-de-los-resultados",
    "title": "Factores Cruzados y Anidados",
    "section": "Comunicación de los resultados",
    "text": "Comunicación de los resultados\nEscrito. Los resultados de los modelos mixtos lineales se comunican de manera similar a los resultados de los modelos lineales. En la sección de resultados, debes mencionar que estás utilizando modelos mixtos con el paquete R lme4, y listar tus efectos aleatorios y fijos. También debes mencionar cómo realizaste la inferencia, es decir, pruebas de razón de verosimilitud (utilizando la función anova) o bootstrap paramétrico. En la sección de resultados para un predictor, basta con escribir una línea, por ejemplo: “Hay una fuerte evidencia (p &lt; 0.001) de un efecto negativo de la modificación en la abundancia total”. Para múltiples predictores, es mejor mostrar los resultados en una tabla.\nVisual. La mejor manera de comunicar visualmente los resultados dependerá de tu pregunta. Para un modelo mixto simple con un efecto aleatorio, una gráfica de los datos crudos con las medias del modelo superpuestas es una posibilidad. Se requiere un poco de código para esta gráfica, y será un poco diferente para tus datos y modelo.\nPuedes hacer gráficas dentro de cada sitio, pero esto es un poco extraño para un diagrama de caja, ya que solo hay dos observaciones por sitio. Puedes hacer esto para tus datos si tienes más observaciones dentro de cada Sitio.\n\nlibrary(Hmisc)\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nModEst &lt;- unique(Estuaries[c(\"SiteWithin\", \"Modification\")]) # find which Estuaries are modified\ncols &lt;- as.numeric(ModEst[order(ModEst[, 1]), 2]) + 3 # Assign colour by modification\nboxplot(log(Total) ~ SiteWithin, data = Estuaries, col = cols, xlab = \"Estuary\", ylab = \"Total invertebrates\")\nlegend(\"bottomleft\",\n  inset = .02,\n  c(\" Modified \", \" Pristine \"), fill = unique(cols), horiz = TRUE, cex = 0.8\n)\nmeans &lt;- fitted(fit.mod) # this will give the estimate at each data point\nEst.means &lt;- summarize(means, Estuaries$SiteWithin, mean)$means # extract means by site\nstripchart(Est.means ~ sort(unique(SiteWithin)), data = Estuaries, pch = 18, col = \"red\", vertical = TRUE, add = TRUE) # plot means by site\n\n\n\n\nAlternativamente, puedes hacer gráficas por Estuario (ver Modelos mixtos 1).\n\nMás ayuda\nPuedes escribir ?lmer en R para obtener ayuda con estas funciones.\nBorrador de capítulo de libro de los autores de lme4.\n\nFaraway, JJ (2005) Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC Press.\n\n\nZuur, A, EN Ieno and GM Smith (2007) Analysing ecological data. Springer Science & Business Media.\n\nAutor: Gordana Popovic\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/mixed-models/mixed-model-3/index.html",
    "href": "statistics/mixed-models/mixed-model-3/index.html",
    "title": "Modelos mixtos generalizados",
    "section": "",
    "text": "Modelos lineales mixtos generalizados\nDebes leer Modelos mixtos 1 y Modelos mixtos 2 como introducción a los modelos mixtos para datos continuos, así como las páginas de ayuda sobre Modelos lineales generalizados como introducción para modelar datos discretos.\nEsta página combinará ambos conceptos para permitirte modelar datos discretos (por ejemplo, presencia/ausencia) con efectos aleatorios utilizando modelos lineales mixtos generalizados (GLMMs).\nPropiedades de los modelos mixtos\nSuposiciones. Las suposiciones de los modelos lineales mixtos generalizados son una combinación de las suposiciones de los GLMs y los modelos mixtos.\n\nLas observaciones \\(y\\) son independientes, condicionales a algunos predictores \\(x\\).\nLa respuesta \\(y\\) proviene de una distribución conocida de la familia exponencial, con una relación conocida entre la media y la varianza.\nExiste una relación lineal entre una función conocida (enlace) de la media de \\(y\\) y los predictores \\(x\\) y los efectos aleatorios \\(z\\).\nLos efectos aleatorios \\(z\\) son independientes de \\(y\\).\nLos efectos aleatorios \\(z\\) siguen una distribución normal.\n\n\nEjecutando el análisis\nAnalizaremos el mismo conjunto de datos que se utilizó en los dos primeros tutoriales sobre modelos mixtos. Este conjunto de datos tenía como objetivo probar el efecto de la contaminación del agua en la abundancia de algunos invertebrados marinos submareales mediante la comparación de muestras de estuarios modificados y prístinos. En los dos primeros tutoriales, analizamos el recuento total de invertebrados, que asumimos que era continuo debido a que los recuentos totales eran grandes. Aquí analizaremos los recuentos y las presencias/ausencias de especies individuales, lo cual requiere modelos lineales mixtos generalizados.\nUtilizaremos el paquete lme4 para todos nuestros modelos de efectos mixtos. Nos permite modelar datos continuos y discretos con uno o más efectos aleatorios. Sin embargo, hay algunas limitaciones para los datos discretos.\nLo que puede hacer lme4 * Modelar datos binarios (por ejemplo, presencia/ausencia). * Modelar recuentos con distribución de Poisson.\nLo que no puede hacer lme4 * Modelar recuentos sobre-dispersos (desafortunadamente, estos son muy comunes en ecología). * Proporcionar buenos gráficos de residuos (los necesitamos para verificar las suposiciones).\nPrimero, carga el paquete:\n\nlibrary(lme4)\n\nDescarga el conjunto de datos de muestra, Estuaries.csv, y cárgalo en R.\n\nEstuaries &lt;- read.csv(\"Estuaries.csv\", header = T)\n\nEn este ejemplo, tenemos un efecto fijo (Modificación; modificado vs prístino) y un efecto aleatorio (Estuario). Para probar si hay un efecto de modificación en los recuentos individuales de especies y en la presencia/ausencia, necesitamos utilizar modelos lineales mixtos generalizados con la función glmer.\n\nConsidera los recuentos de hidroides (la variable Hydroid).\n\n\n [1] 0 0 0 0 1 1 0 0 7 5 2 0 0 0 3 3 0 0 0 0 0 0 0 0 0 0 1 1 2 1 2 2 0 0 0 0 0 0\n[39] 0 0 0 0 1 0 0 0 0 3 1 0 1 2 2 2\n\n\nAl observar los datos, puedes ver que los recuentos son pequeños, con muchos ceros, por lo que no queremos tratarlos como continuos. Los modelaremos como recuentos con una distribución de Poisson, y también como datos de presencia/ausencia.\nPara modelar la presencia/ausencia, primero creamos una variable HydroidPres que es 1 (VERDADERO) cuando los hidroides están presentes y 0 (FALSO) en caso contrario.\n\nEstuaries$HydroidPres &lt;- Estuaries$Hydroid &gt; 0\n\nDatos binarios\nPara ajustar un modelo para la presencia o ausencia de hidroides, usaríamos glmer con family=binomial.\n\nfit.bin &lt;- glmer(HydroidPres ~ Modification + (1 | Estuary), family = binomial, data = Estuaries)\n\nComprobando suposiciones Como de costumbre, podemos examinar los gráficos de residuos para comprobar las suposiciones.\n\npar(mfrow = c(1, 2))\nplot(residuals(fit.bin) ~ fitted(fit.bin), main = \"residuals v.s. Fitted\")\nqqnorm(residuals(fit.bin))\n\n\n\n\nDesafortunadamente, para datos binarios, los gráficos de residuos son bastante difíciles de interpretar. En el gráfico de residuos frente a los valores ajustados, todos los 0 están en una línea (inferior izquierda) y todos los 1 están en una línea (superior derecha) debido a la discreción de los datos. Esto nos impide poder buscar patrones. Tenemos el mismo problema con el gráfico de cuantiles normales.\nAl echar un breve vistazo a nuestras suposiciones, las suposiciones 1 y 4 no las podemos comprobar, pero serán verdaderas si muestreamos de forma aleatoria. Las suposiciones 2 y 3 debemos comprobarlas con los gráficos de residuos, pero dada su limitación no estamos seguros. La suposición 5 es difícil de comprobar en general y no es crucial.\nPrueba de hipótesis para efectos fijos\nPara modelos mixtos lineales generalizados (GLMM), necesitamos utilizar el bootstrap paramétrico incluso para la inferencia de efectos fijos. Esto se debe a que los valores p de la función anova son bastante aproximados para GLMM, incluso para efectos fijos. A veces, la función glmer mostrará advertencias o errores, por lo que he agregado un tryCatch a este código para manejar eso.\n\nnBoot &lt;- 1000\nlrStat &lt;- rep(NA, nBoot)\nft.null &lt;- glmer(HydroidPres ~ 1 + (1 | Estuary), family = binomial, data = Estuaries) # modelo nulo\nft.alt &lt;- glmer(HydroidPres ~ Modification + (1 | Estuary), family = binomial, data = Estuaries) # modelo alternativo\n\nlrObs &lt;- 2 * logLik(ft.alt) - 2 * logLik(ft.null) # estadística de prueba observada\n\nfor (iBoot in 1:nBoot)\n{\n  Estuaries$HydroidPresSim &lt;- unlist(simulate(ft.null)) # datos remuestreados\n  tryCatch(\n    { # a veces el código glmer no converge\n\n      bNull &lt;- glmer(HydroidPresSim ~ 1 + (1 | Estuary), family = binomial, data = Estuaries) # modelo nulo\n      bAlt &lt;- glmer(HydroidPresSim ~ Modification + (1 | Estuary), family = binomial, data = Estuaries) # modelo alternativo\n      lrStat[iBoot] &lt;- 2 * logLik(bAlt) - 2 * logLik(bNull) # estadística de prueba remuestreada\n    },\n    warning = function(war) {\n      lrStat[iBoot] &lt;- NA\n    },\n    error = function(err) {\n      lrStat[iBoot] &lt;- NA\n    }\n  ) # si el código no converge, se omite la simulación\n}\nmean(lrStat &gt; lrObs, na.rm = T) # p-valor para el test del efecto Estuary\n\n[1] 0.03092784\n\n\nTenemos evidencia de un efecto de modificación en la presencia de hidroides.\nDatos de conteo\nlme4 puede modelar datos de conteo que siguen una distribución de Poisson. Si los datos no se ajustan a la relación media/varianza de Poisson, entonces las cosas se vuelven mucho más complicadas, y no abordaremos esa situación aquí.\nPara modelar los conteos de hidroides, usaríamos glmer con family=poisson.\n\nfit.pois &lt;- glmer(Hydroid ~ Modification + (1 | Estuary), family = poisson, data = Estuaries)\n\nPara verificar las suposiciones:\n\npar(mfrow = c(1, 2))\nplot(residuals(fit.pois) ~ fitted(fit.pois), main = \"Residuals vs. Fitted\")\nqqnorm(residuals(fit.pois))\n\n\n\n\nUna vez más, los gráficos de residuos no son muy útiles, pero al menos nos dan una idea de si la suposición de varianza es razonable. No hay una forma de abanico evidente, por lo que un modelo de Poisson parece estar bien.\nPrueba de hipótesis para efectos fijos\nUna vez más, podemos utilizar el remuestreo paramétrico para probar si hay un efecto de Modificación.\n\nnBoot &lt;- 1000\nlrStat &lt;- rep(NA, nBoot)\nft.null &lt;- glmer(Hydroid ~ 1 + (1 | Estuary), family = poisson, data = Estuaries) # modelo nulo\nft.alt &lt;- glmer(Hydroid ~ Modification + (1 | Estuary), family = poisson, data = Estuaries) # modelo alternativo\n\nlrObs &lt;- 2 * logLik(ft.alt) - 2 * logLik(ft.null) # estadística de prueba observada\nfor (iBoot in 1:nBoot)\n{\n  Estuaries$HydroidSim &lt;- unlist(simulate(ft.null)) # datos remuestreados\n  tryCatch(\n    {\n      bNull &lt;- glmer(HydroidSim ~ 1 + (1 | Estuary), family = poisson, data = Estuaries) # modelo nulo\n      bAlt &lt;- glmer(HydroidSim ~ Modification + (1 | Estuary), family = poisson, data = Estuaries) # modelo alternativo\n      lrStat[iBoot] &lt;- 2 * logLik(bAlt) - 2 * logLik(bNull) # estadística de prueba remuestreada\n    },\n    warning = function(war) {\n      lrStat[iBoot] &lt;- NA\n    },\n    error = function(err) {\n      lrStat[iBoot] &lt;- NA\n    }\n  ) # si el código no converge, se omite la simulación\n}\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\nmean(lrStat &gt; lrObs, na.rm = TRUE) # Valor p para la prueba del efecto Estuary\n\n[1] 0.04320988\n\n\nTenemos evidencia de un efecto de la modificación en la abundancia de hidroideos.\nUn ejemplo no Poisson\nA menudo, los datos de conteo no se ajustan a una distribución de Poisson. Observa lo que sucede si intentas modelar los conteos del briozoo Schizoporella errata de ese mismo conjunto de datos.\n\nfit.pois2 &lt;- glmer(Schizoporella.errata ~ Modification + (1 | Estuary), family = poisson, data = Estuaries)\npar(mfrow = c(1, 2))\nplot(residuals(fit.pois) ~ fitted(fit.pois), main = \"residuals vs. Fitted\")\nqqnorm(residuals(fit.pois))\n\n\n\n\nAquí podemos ver una forma de abanico distinta en el gráfico de residuos frente a los valores ajustados. Desafortunadamente, lme4 no puede manejar esta situación (sobredispersión) y no hay una manera fácil de modelar estos datos. Si esto ocurre en tus datos, prueba el paquete glmmADMB.\nPrueba de hipótesis para efectos aleatorios\nComo antes, podrías realizar pruebas de hipótesis sobre los efectos aleatorios utilizando un remuestreo paramétrico. Consulta Modelos mixtos 1 y Modelos mixtos 2 para ver el código que podrías modificar para esta situación.\n\n\nComunicación de los resultados\nEscrita. Los resultados de los modelos lineales mixtos generalizados se comunican de manera similar a los resultados de los modelos lineales. En la sección de resultados, debes mencionar que estás utilizando modelos mixtos con el paquete de R lme4 y enumerar tus efectos aleatorios y fijos. También debes mencionar cómo realizaste la inferencia, es decir, pruebas de razón de verosimilitud (usando anova) o bootstrap paramétrico. En la sección de resultados para un predictor, basta con escribir una línea, por ejemplo: “Hay evidencia sólida (p &lt; 0.001) de un efecto negativo de la modificación en la abundancia total”. Para múltiples predictores, es mejor mostrar los resultados en una tabla.\nVisual. La mejor manera de comunicar visualmente los resultados dependerá de tu pregunta. Para un modelo mixto simple con un efecto aleatorio, una gráfica de los datos brutos con las medias del modelo superpuestas es una posibilidad.\n\nlibrary(Hmisc)\nfit.pois &lt;- glmer(Hydroid ~ Modification + (1 | Estuary), family = poisson, data = Estuaries)\nmeans &lt;- fitted(fit.pois) # esto dará la estimación en cada punto de datos\nModEst &lt;- unique(Estuaries[c(\"Estuary\", \"Modification\")]) # encuentra qué Estuaries están modificados\ncols &lt;- as.numeric(ModEst[order(ModEst[, 1]), 2]) + 3 # asignar color por modificación\n\nWarning: NAs introducidos por coerción\n\nboxplot(Hydroid ~ Estuary, data = Estuaries, col = cols, xlab = \"Estuario\", ylab = \"Recuento de hidroideos\")\nlegend(\"topleft\",\n  inset = .02,\n  c(\"Modificado\", \"Prístino\"), fill = unique(cols), horiz = TRUE, cex = 0.8\n)\n\nEst.means &lt;- summarize(means, Estuaries$Estuary, mean)$means # extraer medias por Estuary\nstripchart(Est.means ~ sort(unique(Estuary)), data = Estuaries, pch = 18, col = \"red\", vertical = TRUE, add = TRUE) # trazar medias por estuario\n\n\n\n\n\n\nMás ayuda\nPuedes escribir ?glmer en R para obtener ayuda con esta función.\nCapítulo de libro en borrador de los autores de lme4.\n\nFaraway, JJ (2005) Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC Press.\n\n\nZuur, A, EN Ieno y GM Smith (2007) Analysing ecological data. Springer Science & Business Media.\n\nAutor: Gordana Popovic\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/mvabund/index.html",
    "href": "statistics/mvabund/index.html",
    "title": "Análisis multivariado con mvabund",
    "section": "",
    "text": "Los datos multivariados son comunes en las ciencias ambientales, ocurren cuando medimos varias variables de respuesta en cada muestra replicada. Preguntas como cómo varía la composición de especies de una comunidad entre sitios, o cómo varía la forma de los árboles (medida por varias características morfológicas) con la altitud, son preguntas multivariadas.\nUtilizaremos el paquete mvabund para especificar y ajustar modelos estadísticos multivariados a este tipo de datos.\n¿En qué se diferencia este método de otros análisis multivariados? Muchos análisis comúnmente utilizados para conjuntos de datos multivariados (por ejemplo, PERMANOVA, ANOSIM, CCA, RDA, etc.) son “análisis basados en distancias”. Esto significa que el primer paso del análisis es calcular una medida de similitud entre cada par de muestras, convirtiendo así un conjunto de datos multivariado en univariado.\nExisten algunos problemas con este tipo de análisis. En primer lugar, su poder estadístico es muy bajo, excepto para variables con alta varianza. Esto significa que para variables que son menos variables, es menos probable que los análisis detecten un efecto de tratamiento. En segundo lugar, no tienen en cuenta una propiedad muy importante de los datos multivariados, que es la relación media-varianza. Típicamente, en conjuntos de datos multivariados como conjuntos de datos de abundancia de especies, las cuentas de especies raras tendrán muchos ceros con poca varianza, y las cuentas más altas para especies más abundantes serán más variables.\nEl enfoque de mvabund mejora el poder estadístico en una variedad de especies con diferentes varianzas e incluye una suposición de una relación media-varianza. Esto se logra ajustando un único modelo lineal generalizado (GLM) a cada variable de respuesta con un conjunto común de variables predictoras. Luego podemos usar remuestreo para probar las respuestas significativas a nivel de comunidad o a nivel de especie ante nuestros predictores.\nAdemás, el marco basado en modelos facilita verificar nuestras suposiciones e interpretar la incertidumbre en torno a nuestros resultados.\nSi estás interesado en este método, mira el video introductorio Introducing mvabund and why multivariate statistics in ecology is a bit like Rick Astley…\n\nEn este ejemplo, utilizamos un conjunto de datos en el que los investigadores querían contrastar la composición de especies de herbívoros marinos en cinco especies de macroalgas. Se recolectaron veinte individuos replicados de cada una de las siete especies de macroalgas de Sydney Harbour, y se registró la abundancia de siete especies de crustáceos herbívoros en cada réplica (datos de Poore et al. 2000). Los datos son multivariados porque se midieron siete variables de respuesta (las especies) en cada una de las muestras.\n\nPodríamos reducir esto a un conjunto de datos univariado calculando las 4950 (100*99/2) diferencias en pares entre las muestras, y utilizar estas diferencias para visualizar patrones en los datos (por ejemplo, como hicimos en nuestro ejemplo de escalamiento multidimensional) o probar hipótesis sobre grupos de muestras mediante remuestreo de estas diferencias.\nAquí, utilizaremos mvabund para contrastar la composición de especies en diferentes hábitats utilizando modelos apropiados para las relaciones media-varianza y permitiéndonos verificar las suposiciones de esos modelos.\n\nEjecución del análisis\nPrimero, instala y carga el paquete mvabund.\n\nlibrary(mvabund)\n\nTus datos deben tener el formato en el que cada muestra es una fila y cada variable es una columna. Descarga el conjunto de datos de especialización de herbívoros y importa en R para ver el formato deseado.\n\nHerbivores &lt;- read.csv(file = \"Herbivore_specialisation.csv\", header = TRUE)\n\nLas dos primeras columnas son variables categóricas que etiquetan las muestras como provenientes de cada uno de los cinco hábitats o como recolectadas durante el día o la noche. La tercera columna es el número de réplica por combinación de hábitat y día/noche. La cuarta columna es la biomasa del hábitat muestreado y el resto de las columnas son los recuentos de cada especie de herbívoro en cada muestra.\nAhora utilizaremos solo los datos de abundancia (en las columnas 5 a 11) y los convertiremos al formato de objeto mvabund utilizado por el paquete mvabund.\n\nHerb_spp &lt;- mvabund(Herbivores[, 5:11])\n\nPodemos echar un vistazo rápido a la dispersión de nuestros datos utilizando la función boxplot.\n\npar(mar = c(2, 10, 2, 2)) # adjusts the margins\nboxplot(Herbivores[, 5:11], horizontal = TRUE, las = 2, main = \"Abundance\")\n\n\n\n\nParece que algunas especies de herbívoros marinos (por ejemplo, Ampithoe ngana) son mucho más abundantes y variables que otras (por ejemplo, Cymadusa munnu). ¡Probablemente sea una buena idea verificar nuestra relación media-varianza! Podemos hacer esto utilizando la función meanvar.plot:\n\nmeanvar.plot(Herb_spp)\n\n\n\n\nClaramente se puede observar que las especies con altas medias (en el eje x) también tienen altas varianzas (eje y).\nPodemos abordar esta relación eligiendo una familia de GLM con una suposición adecuada de media-varianza. La familia predeterminada utilizada por mvabund al ajustar GLMs multivariados es la binomial negativa, que asume una relación cuadrática entre la media y la varianza, y una relación log-lineal entre las variables de respuesta y las variables continuas. En este ejemplo, solo tenemos variables categóricas, por lo que eso no es demasiado importante. Si no estás seguro de estas relaciones, no te preocupes, podemos verificar el ajuste de nuestro modelo más adelante.\nAhora volvamos a nuestras preguntas de investigación. ¿Hay diferencias en la composición de especies de los siete herbívoros marinos muestreados? ¿Algunas de ellas se especializan en tipos particulares de algas mientras que otras son alimentadoras más generalizadas? ¿Cuáles son las especies? Comencemos por observar los datos visualmente.\nExiste una función de trazado incorporada y rápida en el paquete mvabund que nos permite contrastar las abundancias transformadas con las variables predictoras que elijamos. Para contrastar las abundancias con el hábitat, utilizaríamos:\n\nplot(Herb_spp ~ as.factor(Herbivores$Habitat), cex.axis = 0.8, cex = 0.8)\n\n\n PIPING TO 2nd MVFACTOR \n\n\n\n\n\nAlternativamente, podemos incluir el argumento transformation=\"no\" para ver los datos de abundancia en bruto. Como este gráfico se basa en el lenguaje de trazado base de R, puedes agregar argumentos adicionales para personalizar el gráfico. Hemos reducido el tamaño del texto de los ejes (cex.axis=0.8) y de los símbolos (cex=0.8) para poder ver mejor lo que está sucediendo.\nEs un gráfico bastante desordenado, pero hay un par de cosas que llaman nuestra atención. Parece que el herbívoro Ampithoe ngana es muy abundante y comerá prácticamente cualquier cosa. Por otro lado, Cymadusa munnu y Plumithoe quadrimana son bastante raras. Ampithoe ngana, A. caddi, A. kava y Exampithoe kutti son alimentadores generalistas, mientras que Perampithoe parmerong está en gran medida especializada en las dos especies de Sargassum.\nAhora contrastemos la composición de especies entre las especies de algas para ver si los modelos respaldan nuestras observaciones.\nLa sintaxis del modelo a continuación ajusta nuestra variable de respuesta (el objeto mvabund Herb_spp con los 100 recuentos de las 7 especies) a la variable predictora Habitat (tipo de alga).\n\nmod1 &lt;- manyglm(Herb_spp ~ Herbivores$Habitat, family = \"poisson\")\n\n\n\nSupuestos a verificar\nAntes de examinar los resultados, debemos verificar los supuestos de nuestro modelo. Podemos utilizar la función plot para generar un gráfico de los residuos.\n\nplot(mod1)\n\n\n\n\nSi el modelo se ajusta bien, deberíamos ver una dispersión aleatoria de puntos. Lo que no queremos ver es una relación, ya sea lineal o curvilínea, o una forma de abanico. Esto podría significar que uno de nuestros supuestos estaba equivocado: ya sea que hayamos especificado incorrectamente la relación entre la media y la varianza, o que nuestra relación asumida entre nuestra respuesta y los predictores sea incorrecta. O también podríamos haber omitido una variable explicativa clave en nuestro modelo, lo cual deja mucha varianza sin explicar.\nEn este ejemplo, vemos una clara forma de abanico en el gráfico de residuos, lo que significa que hemos especificado incorrectamente nuestra relación entre la media y la varianza. Podemos utilizar el argumento family para elegir una distribución que se adapte mejor a nuestros datos. Para datos de recuento que no se ajustan a la distribución 'poisson', podemos usar la distribución negative_binomial.\n\nmod2 &lt;- manyglm(Herb_spp ~ Herbivores$Habitat, family = \"negative_binomial\")\nplot(mod2)\n\nWarning in default.plot.manyglm(x, res.type = res.type, which = which, caption\n= caption, : Only the first 7 colors will be used for plotting.\n\n\n\n\n\nEste gráfico de residuos es mucho mejor, ahora no hay una forma de abanico discernible y utilizaremos este modelo para todo el análisis posterior.\n\n\nInterpretación de los resultados\nPodemos probar la hipótesis multivariante de si la composición de especies varía entre los hábitats utilizando la función anova. Esto nos da una tabla de análisis de desviación donde utilizamos pruebas de razón de verosimilitud y valores p remuestreados para buscar un efecto significativo del hábitat en los datos de la comunidad.\n\nanova(mod2)\n\nTime elapsed: 0 hr 0 min 14 sec\n\n\nAnalysis of Deviance Table\n\nModel: Herb_spp ~ Herbivores$Habitat\n\nMultivariate test:\n                   Res.Df Df.diff   Dev Pr(&gt;Dev)    \n(Intercept)            99                           \nHerbivores$Habitat     95       4 625.2    0.001 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nArguments:\n Test statistics calculated assuming uncorrelated response (for faster computation) \n P-value calculated using 999 iterations via PIT-trap resampling.\n\n\nPodemos ver en esta tabla que hay un efecto significativo del hábitat (LRT = 625, P = 0.001), lo que significa que la composición de especies de herbívoros difiere claramente entre las especies de algas en las que se encuentran.\nPara examinar esto más a fondo y ver qué especies de herbívoros es más probable que se encuentren en qué especies de algas, podemos realizar pruebas univariadas para cada especie por separado.\nEsto se hace utilizando el argumento p.uni=\"adjusted\" en la función anova. La parte “adjusted” del argumento se refiere al método de remuestreo utilizado para calcular los valores p, teniendo en cuenta la correlación entre las variables de respuesta. Esta correlación se encuentra a menudo en los sistemas ecológicos donde diferentes especies interactuarán entre sí, compitiendo o facilitando el uso de recursos.\n\nanova(mod2, p.uni = \"adjusted\")\n\nTime elapsed: 0 hr 0 min 15 sec\n\n\nAnalysis of Deviance Table\n\nModel: Herb_spp ~ Herbivores$Habitat\n\nMultivariate test:\n                   Res.Df Df.diff   Dev Pr(&gt;Dev)    \n(Intercept)            99                           \nHerbivores$Habitat     95       4 625.2    0.001 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Tests:\n                   Peramphithoe_parmerong          Ampithoe_caddi         \n                                      Dev Pr(&gt;Dev)            Dev Pr(&gt;Dev)\n(Intercept)                                                               \nHerbivores$Habitat                148.716    0.001         91.659    0.001\n                   Ampithoe_kava          Ampithoe_ngana         \n                             Dev Pr(&gt;Dev)            Dev Pr(&gt;Dev)\n(Intercept)                                                      \nHerbivores$Habitat        85.366    0.001         90.221    0.001\n                   Cymadusa_munnu          Exampithoe_kutti         \n                              Dev Pr(&gt;Dev)              Dev Pr(&gt;Dev)\n(Intercept)                                                         \nHerbivores$Habitat         21.452    0.001          107.254    0.001\n                   Plumithoe_quadrimana         \n                                    Dev Pr(&gt;Dev)\n(Intercept)                                     \nHerbivores$Habitat               80.575    0.001\nArguments:\n Test statistics calculated assuming uncorrelated response (for faster computation) \nP-value calculated using 999 iterations via PIT-trap resampling.\n\n\nIncluso después de ajustar por pruebas múltiples, hay un efecto del hábitat en todas las especies.\nHasta ahora, solo hemos considerado una variable predictora del hábitat. Al modificar la fórmula en mvabund, podemos probar modelos más complejos. Por ejemplo, para ajustar un modelo con tanto el hábitat como el día o la noche, utilizaríamos:\n\nmod3 &lt;- manyglm(Herb_spp ~ Herbivores$Habitat * Herbivores$DayNight, family = \"negative_binomial\")\nanova(mod3)\n\nTime elapsed: 0 hr 0 min 46 sec\n\n\nAnalysis of Deviance Table\n\nModel: Herb_spp ~ Herbivores$Habitat * Herbivores$DayNight\n\nMultivariate test:\n                                       Res.Df Df.diff   Dev Pr(&gt;Dev)    \n(Intercept)                                99                           \nHerbivores$Habitat                         95       4 625.2    0.001 ***\nHerbivores$DayNight                        94       1   6.2    0.600    \nHerbivores$Habitat:Herbivores$DayNight     90       4  25.4    0.438    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nArguments:\n Test statistics calculated assuming uncorrelated response (for faster computation) \n P-value calculated using 999 iterations via PIT-trap resampling.\n\n\nPuedes observar que la composición de especies de herbívoros varía según el hábitat, pero no entre el día y la noche.\n\n\nComunicando los resultados\nEscrito. Si estuviéramos escribiendo un artículo sobre las diferencias en el uso de hábitat por parte de los herbívoros marinos, podríamos escribir lo siguiente: Hubo comunidades de herbívoros marinos diferentes en diferentes sustratos de algas (LRT = 625, P &lt; 0.001). Podemos ser más descriptivos sobre las diferencias en la comunidad utilizando una representación gráfica de nuestros resultados.\nVisual. Los datos multivariados se visualizan mejor mediante gráficos de ordenación. Consulta el paquete boral para obtener una ordenación basada en modelos. Para comenzar, mira este video.\n\n\nMás ayuda\nEste método fue creado por el grupo de investigación Ecostats de la UNSW. Puedes estar al tanto de sus últimas investigaciones en su blog. Han estado actualizando el paquete mvabund con muchas características nuevas emocionantes, incluyendo remuestreo por bloques y análisis de la cuarta esquina.\n\nWang, Y, U Naumann, ST Wright & DI Warton (2012) mvabund - un paquete R para el análisis basado en modelos de datos de abundancia multivariados. Methods in Ecology and Evolution 3: 471-474.\n\nAutores: Rachel V. Blakey y Andrew Letten\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/power-analysis/index.html",
    "href": "statistics/power-analysis/index.html",
    "title": "Power Analyses",
    "section": "",
    "text": "¿Qué es el análisis de potencia y por qué es importante?\nRecopilas datos para responder una pregunta, generalmente con el objetivo de detectar un efecto particular (por ejemplo, efecto del tratamiento). La potencia de tu prueba es la probabilidad de detectar un efecto (es decir, obtener un valor p &lt; 0.05) con el diseño experimental que tienes y el tamaño de efecto que esperas o consideras significativo.\nComenzaré simulando un conjunto de datos que tiene un efecto (una diferencia entre el grupo de control y el grupo de tratamiento), y veremos si podemos detectar el efecto. Realizaremos una simple prueba t de dos muestras como ejemplo.\n\nset.seed(1)\n\nN &lt;- 20 # the sample size\n\ntrt.effect &lt;- 0.2 # difference between control and treatment means\nsigma &lt;- 0.5 # standard deviation of control and treatment groups\n\nmean.con &lt;- 0 # mean of control group\n\nmean.trt &lt;- mean.con + trt.effect # mean of treatment group\n\ncontrol &lt;- rnorm(N, mean.con, sigma) # 20 data points for the control group taken from a normal distribution with known sample size, mean and s.d.\n\ntreatment &lt;- rnorm(N, mean.trt, sigma) # data for the treatment group\n\nt.test(control, treatment)\n\n\n    Welch Two Sample t-test\n\ndata:  control and treatment\nt = -0.71923, df = 37.917, p-value = 0.4764\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3872163  0.1842117\nsample estimates:\n mean of x  mean of y \n0.09526194 0.19676424 \n\n\nPodemos ver las diferencias entre los grupos con un diagrama de caja y probar esas diferencias con una prueba t:\n\nboxplot(cbind(control, treatment))\n\n\n\nt.test(control, treatment)\n\n\n    Welch Two Sample t-test\n\ndata:  control and treatment\nt = -0.71923, df = 37.917, p-value = 0.4764\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3872163  0.1842117\nsample estimates:\n mean of x  mean of y \n0.09526194 0.19676424 \n\n\nSabemos que hay un efecto del tratamiento, con un tamaño de efecto de 0.2, pero la prueba no pudo detectarlo. Esto se conoce como error de tipo II. Repitamos el mismo experimento con exactamente la misma configuración.\n\n#\nset.seed(3)\nN &lt;- 20\ntrt.effect &lt;- 0.2\nsigma &lt;- 0.5\nmean.con &lt;- 0\nmean.trt &lt;- mean.con + trt.effect\ncontrol &lt;- rnorm(N, mean.con, sigma)\ntreatment &lt;- rnorm(N, mean.trt, sigma)\nboxplot(cbind(control, treatment))\n\n\n\nt.test(control, treatment)\n\n\n    Welch Two Sample t-test\n\ndata:  control and treatment\nt = -2.3471, df = 37.416, p-value = 0.02432\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.57817515 -0.04253493\nsample estimates:\n  mean of x   mean of y \n-0.08359522  0.22675982 \n\n\nEsta vez detectamos un efecto. Esto siempre ocurre, sin importar cuál sea tu diseño experimental, si hay un efecto presente, tienes alguna posibilidad de detectarlo, esto se llama potencia. Para no perder tiempo y dinero, solo debemos realizar experimentos que tengan alta potencia, es decir, alta probabilidad de detectar un efecto si existe. Podemos calcular la potencia de este experimento.\n\n\nAnálisis de potencia simple con ‘pwr’\nExisten varios paquetes que realizan análisis de potencia en R. pwr realiza cosas simples hasta lm y simR realiza modelos más complejos modelos mixtos y glms.\nPrimero, descarga e instala el paquete.\n\nlibrary(pwr)\n\nPara realizar cálculos de potencia en pwr, debes dejar uno de los valores como NULL y completar los demás. Luego te proporcionará el valor para el que dejaste en blanco.\nAquí usaré la función pwr.t.test y dejaré en blanco el valor de potencia. Calculará la potencia para el diseño experimental dado el tamaño de muestra y el tamaño de efecto especificados. Ten cuidado; la variable d, que en el archivo de ayuda se llama tamaño de efecto, es nuestro efecto de tratamiento (diferencia entre medias) dividido por la desviación estándar.\n\npwr.t.test(n = 20, d = trt.effect / sigma, sig.level = 0.05, power = NULL)\n\n\n     Two-sample t test power calculation \n\n              n = 20\n              d = 0.4\n      sig.level = 0.05\n          power = 0.2343494\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n¡Ups! La potencia es solo del 25%. Esto significa que, dado el efecto que esperamos y con este tamaño de muestra, solo detectaremos ese efecto el 25% del tiempo. Entonces, probablemente no valga la pena hacer este experimento. Podríamos aumentar el tamaño de muestra para aumentar la potencia.\nAhora podemos utilizar nuestros cálculos de potencia para determinar qué tamaño de muestra nos dará una buena potencia, siendo el 80% el umbral habitual. Repetimos el cálculo pero ahora dejando en blanco la variable N y estableciendo la potencia en 0.8.\n\npwr.t.test(n = NULL, d = trt.effect / sigma, sig.level = 0.05, power = 0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 99.08032\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nEsto nos indica que necesitamos un tamaño de muestra (para cada grupo) de 100 para lograr una potencia del 80% para detectar nuestro efecto. Esto obviamente es importante saberlo si las medidas replicadas son costosas o requieren mucho tiempo para recopilar.\nTambién podemos trazar la potencia para varios tamaños de muestra. Aquí tienes un código que calcula y traza la potencia para tamaños de muestra desde 2 hasta 200.\n\nnvals &lt;- seq(2, 200, length.out = 200)\npowvals &lt;- sapply(nvals, function(x) pwr.2p.test(h = trt.effect / sigma, n = x, sig.level = 0.05)$power)\n\nplot(nvals, powvals,\n  xlab = \"sample size\", ylab = \"power\",\n  main = \"Power curve for sample size for difference in proportions\",\n  lwd = 2, col = \"red\", type = \"l\"\n)\n\nabline(h = 0.8)\n\n\n\n\nEl paquete pwr tiene varias funciones, pero todas funcionan de manera similar.\n\n\n\nFunción\nDescripción\n\n\n\n\npwr.2p.test\ndos proporciones (n iguales)\n\n\npwr.2p2n.test\ndos proporciones (n desiguales)\n\n\npwr.anova.test\nANOVA de un factor equilibrado\n\n\npwr.chisq.test\nprueba de chi-cuadrado\n\n\npwr.f2.test\nmodelo lineal general\n\n\npwr.p.test\nproporción (una muestra)\n\n\npwr.r.test\ncorrelación\n\n\npwr.t.test\npruebas t (una muestra, dos muestras, emparejadas)\n\n\npwr.t2n.test\nprueba t (dos muestras con n desiguales)\n\n\n\nGeneralmente es un poco complicado especificar el tamaño de efecto, puedes encontrar una buena guía para este paquete aquí.\nTen en cuenta que especificar el tamaño de efecto no es una pregunta estadística, es una pregunta ecológica sobre qué tamaño de efecto es significativo para tu estudio en particular. Por ejemplo, ¿quieres poder detectar una disminución del 10% en la abundancia de un animal raro o te conformarías con un diseño de muestreo capaz de detectar una disminución del 25%?\n\n\nAnálisis con modelos lineales y mixtos con ‘simr’\nSi estás llevando a cabo un experimento con factores aleatorios y necesitas realizar un modelo mixto y tu análisis de potencia debe reflejar eso, esto es mucho más complicado que los ejemplos anteriores y deberemos utilizar métodos de simulación. El paquete simR está diseñado para esto.\nDescarga e instala el paquete.\n\nlibrary(simr)\nsimrOptions(progress = FALSE)\n\nAnálisis de potencia con un estudio piloto. Hemos realizado un estudio piloto y ahora queremos decidir cómo asignar el muestreo para el estudio completo. El estudio busca un efecto de la modificación del estuario en la abundancia de una especie marina. Supongamos que tenemos los recursos para realizar un máximo de 72 muestras y queremos maximizar la potencia para una prueba de la variable categórica ‘modificación’.\n\nTenemos un conjunto de datos piloto ‘pilot’ que tiene un efecto fijo continuo de la temperatura, un efecto fijo de la modificación y un efecto aleatorio para el sitio (anidado dentro de la modificación). La variable de respuesta es la abundancia de una especie de interés.\nDescarga estos datos piloto, Pilot.csv, impórtalos en R y cambia la variable de modificación a un factor (etiquetado como 1, 2 y 3, pero no como un entero).\n\nPilot &lt;- read.csv(file = \"Pilot.csv\", header = TRUE)\nPilot$modification &lt;- factor(Pilot$modification)\n\nGraficar la abundancia en función del sitio y los niveles del factor de modificación nos muestra que tenemos datos donde los sitios varían bastante dentro de cada modificación.\n\npar(mfrow = c(1, 2))\nboxplot(abundance ~ modification, data = Pilot, main = \"modification\")\nboxplot(abundance ~ site, data = Pilot, main = \"site\")\n\n\n\n\nEstos datos de conteo con un efecto aleatorio de sitio se modelan mejor con un modelo lineal mixto generalizado utilizando glmer del paquete lme4.\n\nlibrary(lme4)\n\nPilot.mod &lt;- glmer(abundance ~ temperature + modification + (1 | site), family = \"poisson\", data = Pilot)\n\nsummary(Pilot.mod)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: abundance ~ temperature + modification + (1 | site)\n   Data: Pilot\n\n     AIC      BIC   logLik deviance df.resid \n   153.3    157.7    -71.6    143.3       13 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.2983 -0.4266  0.1293  0.4240  0.8410 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n site   (Intercept) 0.006274 0.07921 \nNumber of obs: 18, groups:  site, 6\n\nFixed effects:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.69024    0.09851   17.16   &lt;2e-16 ***\ntemperature    4.08409    0.08875   46.02   &lt;2e-16 ***\nmodification2  1.14103    0.09465   12.05   &lt;2e-16 ***\nmodification3  2.39503    0.09538   25.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) tmprtr mdfct2\ntemperature -0.692              \nmodificatn2 -0.558  0.023       \nmodificatn3 -0.696  0.226  0.565\n\n\nHay dos coeficientes que especifican los efectos de la modificación (ya que hay tres categorías de modificación). Para realizar un análisis de potencia, debemos especificar tamaños de efecto que sean ecológicamente significativos. Ten en cuenta que estos están en la escala del modelo, para un modelo poisson están en la escala logarítmica (y multiplicativos).\n\nfixef(Pilot.mod)[\"modification2\"] &lt;- 0.1\nfixef(Pilot.mod)[\"modification3\"] &lt;- 0.2\n\nAhora podemos usar la función powerSim para calcular la probabilidad de que un experimento con este tamaño de muestra pueda detectar un efecto de modificación, esto es la ‘potencia’. Quiero utilizar una prueba de razón de verosimilitudes (es decir, usar la función anova) para probar un efecto de modificación, así que lo especifico con el argumento lr.\n\npowerSim(Pilot.mod, fixed(\"modification\", \"lr\"), nsim = 50)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nPower for predictor 'modification', (95% confidence interval):\n      46.00% (31.81, 60.68)\n\nTest: Likelihood ratio\n\nBased on 50 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 18\n\nTime elapsed: 0 h 0 m 10 s\n\n# this is a small number of sims for demonstration, the default of 1000 is more appropriate\n\nAsí que esperamos tener un poder de alrededor del 45% para una prueba de razón de verosimilitud para ‘modificación’ con el número actual de observaciones. Esto es bastante bajo, nos gustaría ver un poder por encima del 80%, así que veamos cómo afecta el aumento del tamaño de la muestra.\n¿Cómo podemos aumentar el poder? Aumentar las observaciones es deseable para aumentar el poder, pero en este diseño de muestreo, tenemos varias formas de agregar observaciones a tus datos. Podrías muestrear más sitios, pero mantener el número de muestras por sitio igual, o podrías mantener el número de sitios igual y muestrear más dentro de cada sitio.\n\nxtabs(~ modification + site, data = Pilot)\n\n            site\nmodification a b c d e f\n           1 3 3 0 0 0 0\n           2 0 0 3 3 0 0\n           3 0 0 0 0 3 3\n\n\nActualmente hay 2 sitios por categoría de modificación y 3 observaciones por sitio. Intentemos aumentar el número de sitios. Hacemos esto utilizando la función extend para explorar cómo afectará el aumento del tamaño de la muestra al poder. Al utilizar along=site, agregaremos más sitios. Con nuestro presupuesto de 72 observaciones y 3 observaciones por sitio, tendríamos 24 sitios.\n\nfull1 &lt;- extend(Pilot.mod, along = \"site\", n = 24)\n\nxtabs(~site, data = attributes(full1)$newData)\n\nsite\n 1 10 11 12 13 14 15 16 17 18 19  2 20 21 22 23 24  3  4  5  6  7  8  9 \n 3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 \n\npowerSim(full1, fixed(\"modification\", \"lr\"), nsim = 50)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nPower for predictor 'modification', (95% confidence interval):\n      92.00% (80.77, 97.78)\n\nTest: Likelihood ratio\n\nBased on 50 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 72\n\nTime elapsed: 0 h 0 m 10 s\n\n\nAsí que obtenemos alrededor del 90% de poder, suena genial. Para ver si agregar más observaciones por sitio haría que fuera mejor o peor, utilizamos el argumento within.\n\nfull2 &lt;- extend(Pilot.mod, within = \"site\", n = 12)\n\nxtabs(~site, data = attributes(full2)$newData)\n\nsite\n a  b  c  d  e  f \n12 12 12 12 12 12 \n\npowerSim(full2, fixed(\"modification\", \"lr\"), nsim = 50)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nPower for predictor 'modification', (95% confidence interval):\n      64.00% (49.19, 77.08)\n\nTest: Likelihood ratio\n\nBased on 50 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 72\n\nTime elapsed: 0 h 0 m 10 s\n\n\nEso solo nos da alrededor del 60% de poder. Entonces, para estos datos, diseño de muestreo y pregunta, es mejor agregar más sitios. Por supuesto, también puedes hacer combinaciones, tal vez duplicar los sitios y duplicar las observaciones por sitio.\n\nfull3 &lt;- extend(Pilot.mod, within = \"site\", n = 6)\nfull3 &lt;- extend(full3, along = \"site\", n = 12)\n\nxtabs(~site, data = attributes(full3)$newData)\n\nsite\n 1 10 11 12  2  3  4  5  6  7  8  9 \n 6  6  6  6  6  6  6  6  6  6  6  6 \n\npowerSim(full3, fixed(\"modification\", \"lr\"), nsim = 50)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nPower for predictor 'modification', (95% confidence interval):\n      66.00% (51.23, 78.79)\n\nTest: Likelihood ratio\n\nBased on 50 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 72\n\nTime elapsed: 0 h 0 m 10 s\n\n\nEsto nos da un 75% de poder, que está a medio camino entre los otros. Entonces, en este caso, nuevamente es mejor agregar más sitios que más observaciones por sitio. Esto no siempre es cierto y depende en gran medida de la variabilidad dentro y entre sitios, de tu diseño experimental y de tu pregunta.\n\n\nAlgunas notas\nEl análisis de poder proporciona la probabilidad de detectar un efecto particular (de una determinada magnitud) a un nivel alfa particular (generalmente 0.05) si este efecto está presente. Si estás interesado en múltiples preguntas a partir de los mismos datos, para realizar un análisis de poder generalmente debes elegir una pregunta de interés principal.\nDebes especificar el nivel del efecto que deseas poder detectar. Si utilizas el nivel estimado de un estudio piloto, esto se conoce como un cálculo de “poder observado” y no es un uso válido del análisis de poder.\n\n\nMás información\nAlgunos ejemplos adicionales de estudios piloto del paquete simR\nAnálisis de poder desde cero sin estudio piloto\nMás información sobre especificar el tamaño del efecto\nAutor: Gordana Popovic\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/t-tests/index.html",
    "href": "statistics/t-tests/index.html",
    "title": "T-tests",
    "section": "",
    "text": "Algunas pruebas de hipótesis simples con la estadística t:\n\nPruebas t de una muestra para contrastar un parámetro de muestra con un parámetro de población\nPruebas t de muestras independientes para contrastar las medias de dos muestras\nPruebas t emparejadas para contrastar dos grupos de valores donde cada par se mide en el mismo objeto."
  },
  {
    "objectID": "statistics/t-tests/one-sample-t-test/index.html",
    "href": "statistics/t-tests/one-sample-t-test/index.html",
    "title": "Prueba de una muestra con t de Student",
    "section": "",
    "text": "Una de las pruebas de hipótesis más simples en estadística consiste en evaluar si un parámetro único obtenido de una muestra de mediciones difiere de un parámetro poblacional hipotético. La prueba busca determinar la probabilidad de obtener esa muestra a partir de una población con ciertas propiedades.\nPara esto, utilizamos una prueba de una muestra con t de Student. La estadística de prueba, t, tiene la siguiente forma general:\n\\[t = \\frac{St-\\theta}{S_{St}}\\]\nDonde St es el valor de la estadística de tu muestra, \\(\\theta\\) es el valor poblacional con el que estás comparando tu muestra, y \\(S_{St}\\) es el error estándar de tu estadística de muestra.\nEstas pruebas se pueden utilizar para una variedad de parámetros muestreados de poblaciones (por ejemplo, medias, pendientes e interceptos en regresión lineal, etc.). Aquí, veamos un ejemplo sencillo en el que probamos si la media de un conjunto de medidas replicadas difiere de un valor hipotético.\n\nImagina que un científico forense intentaba rastrear el origen de algunas muestras de suelo tomadas de huellas en una escena del crimen. Recogió 10 muestras y analizó la concentración de polen de una especie de pino que se encuentra en un bosque local. Se sabía que el suelo de ese bosque local tenía una concentración promedio de 125 granos por gramo de suelo. La prueba de t de una muestra probará la probabilidad de que las diez muestras provengan de ese bosque al comparar la concentración promedio en las diez nuevas muestras con el valor esperado si provinieran de ese bosque.\nLa estadística de prueba, t, es:\n\\[t = \\frac{\\bar{x}-\\mu}{SE}\\]\ndonde \\(\\bar{x}\\) es la media de la muestra, \\(\\mu\\) es la media poblacional y SE es el error estándar de la muestra.\nTen en cuenta que el tamaño de la estadística de prueba depende de dos cosas: 1) qué tan diferente es la media de la muestra de la media poblacional (el numerador) y 2) cuánta variación está presente dentro de la muestra (el denominador). La hipótesis nula es que la media poblacional de la cual se tomó la muestra es el valor conocido, es decir, \\(H_o: \\mu=125\\).\n\nEjecutando el análisis\nLa estadística de prueba t es relativamente sencilla de calcular manualmente. Luego, puedes comparar la estadística de prueba con una distribución t para determinar la probabilidad de obtener ese valor de la estadística de prueba si la hipótesis nula es verdadera.\nPara calcular la probabilidad asociada a un valor dado de t en R, usa\n\npt(q, df = your.df, lower.tail = FALSE) * 2\n\ndonde q es tu valor de t, your.df son los grados de libertad (n-1). El lower.tail = FALSE asegura que estás calculando la probabilidad de obtener un valor de t más grande que el tuyo (es decir, la cola superior, P[X &gt; x]). Ten en cuenta que el valor crítico para \\(t_{\\alpha = 0.05}\\) varía según el número de grados de libertad: a mayor número de grados de libertad, menor es el valor crítico de t.\nEs mucho más fácil utilizar la función t.test en R para obtener la estadística de prueba y su probabilidad asociada en una sola salida. Para una prueba de t de una muestra, usaríamos:\n\nt.test(y, mu = your.mu)\n\ndonde y es un vector con tus datos de muestra y your.mu es el parámetro de la población con el que estás comparando la muestra.\nEn nuestro ejemplo de la escena del crimen, podríamos asignar nuestras diez mediciones a un objeto llamado “polen” y ejecutar el test de t en ese objeto.\n\npollen &lt;- c(94, 135, 78, 98, 137, 114, 114, 101, 112, 121)\nt.test(pollen, mu = 125)\n\n\n\nInterpretación de los resultados\n\n\n\n    One Sample t-test\n\ndata:  pollen\nt = -3.0691, df = 9, p-value = 0.01337\nalternative hypothesis: true mean is not equal to 125\n95 percent confidence interval:\n  97.20685 120.79315\nsample estimates:\nmean of x \n      109 \n\n\nLa salida de una prueba t de una muestra es fácil de interpretar. En la salida anterior, la estadística de prueba t es -3.0691 con 9 grados de libertad, y un valor de p bajo (p = 0.013). Por lo tanto, podemos rechazar la hipótesis nula y concluir que es poco probable que las muestras de suelo de la escena del crimen provengan del bosque de pinos cercano.\nTambién se obtiene la media de la muestra (109) y el intervalo de confianza del 95% para la media de la población estimada a partir de esa muestra (este no se superpondrá con la media hipotetizada cuando la prueba sea significativa).\n\n\nSupuestos a verificar\nLas pruebas t son pruebas paramétricas, lo que implica que podemos especificar una distribución de probabilidad para la población de la variable de la cual se tomaron las muestras. Las pruebas paramétricas (y no paramétricas) tienen varios supuestos. Si se violan estos supuestos, no podemos estar seguros de que la estadística de prueba siga una distribución t, en cuyo caso los valores de p pueden ser inexactos.\nNormalidad. La distribución t describe parámetros muestreados de una población normal, por lo que asume que los datos tienen una distribución normal. Sin embargo, hay que tener en cuenta que las pruebas t son bastante robustas ante violaciones de la normalidad (aunque hay que tener cuidado con la influencia de los valores atípicos).\nIndependencia. Las observaciones deben haber sido muestreadas al azar de una población definida para que la media de la muestra sea una estimación imparcial de la media de la población. Si los datos individuales están vinculados de alguna manera, se violará el supuesto de independencia.\n\n\nComunicación de los resultados\nEscrito. Como mínimo, se debe informar la estadística de prueba t observada, el valor de p y el número de grados de libertad. Por ejemplo, se podría escribir “La media del recuento de polen de las huellas (109) fue significativamente más baja de lo esperado si proviniera del bosque cercano con un recuento promedio de 125 (t = 3.07, df = 9, P = 0.01)”.\nVisual. Los diagramas de caja o histogramas de frecuencia se pueden utilizar para visualizar la variación en una sola variable. En este ejemplo, se podría utilizar una línea o una flecha para indicar el valor único (125) con el que se estaba comparando la muestra.\n\nhist(pollen, xlab = \"Pollen count\", main = NULL)\nabline(v = 125, col = \"red\")\n\n\n\nboxplot(pollen, xlab = \"Pollen count\", horizontal = TRUE)\nabline(v = 125, col = \"red\")\n\n\n\n\n\n\nMás ayuda\nEscribe ?t.test para obtener la ayuda de R sobre esta función.\n\nQuinn and Keough (2002) **Experimental design and data analysis for biologists*. Cambridge University Press. Capítulo 3: Prueba de hipótesis.\n\n\nMcKillup (2012) Statistics explained. An introductory guide for life scientists. Cambridge University Press. Capítulo 9: Comparación de las medias de una y dos muestras de datos distribuidos normalmente.\n\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/t-tests/paired-t-test/index.html",
    "href": "statistics/t-tests/paired-t-test/index.html",
    "title": "T-test emparejado",
    "section": "",
    "text": "Los t-tests emparejados se utilizan para comparar las medias de dos grupos de medidas cuando los objetos individuales se miden dos veces, una vez para cada tipo de medida. Los datos pueden estar emparejados de varias formas: si se toma la misma medida de un objeto individual en dos tratamientos diferentes o en dos momentos diferentes, o si se contrastan diferentes tipos de medidas del mismo objeto.\n\nPor ejemplo, para contrastar el rendimiento fotosintético de diez plantas en dos entornos de un invernadero (sombreado y soleado), podríamos medir el rendimiento en cada planta individual dos veces, una vez a la sombra y otra vez al sol. Las medidas están emparejadas por pertenecer a la misma planta individual.\nPara este diseño experimental, utilizaríamos un t-test emparejado para comparar las mediciones realizadas en los dos entornos. Las 20 mediciones individuales no son independientes entre sí, porque esperaríamos que el par de mediciones tomadas de la misma planta individual sean más similares entre sí que si se tomaran al azar de todas las plantas disponibles. Por lo tanto, no podemos utilizar un t-test de muestras independientes - ese test sería apropiado si cada planta se utilizara solo una vez, con algunas plantas medidas en el tratamiento de sombra y un conjunto diferente de plantas medidas en el tratamiento soleado.\nEmparejar datos de esta manera se hace generalmente para reducir la variación probable entre las mediciones con el objetivo de detectar mejor las diferencias entre los grupos. En este ejemplo, la diferencia entre las dos medidas de rendimiento fotosintético en una planta determinada debería reflejar principalmente el efecto de la luz solar, mientras que en un diseño de muestras independientes, la diferencia entre una planta a la sombra y otra planta al sol reflejará tanto las diferencias en los efectos de la luz solar como las diferencias entre las plantas individuales.\nPara un t-test emparejado, la estadística de prueba, t, es:\n\\[t = \\frac{\\bar{d}}{SE_{d}}\\]\nDonde \\(\\bar{d}\\) es la media de las diferencias entre los valores de cada par, y SEd es el error estándar de ese conjunto de diferencias.\nCabe destacar que esta ecuación es idéntica a un t-test de muestra única, utilizado para contrastar cualquier media de muestra (\\(\\bar{x}\\)) con una media poblacional conocida (\\(\\mu\\)) o un valor hipotético. Lo que estás haciendo aquí es probar si tu muestra de diferencias probablemente proviene de una población de diferencias que tienen una media de cero (otra forma de decir que tus grupos son iguales).\n\\[t = \\frac{\\bar{x}-\\mu}{SE}\\]\n\nEjecución del análisis\nEl estadístico de prueba t es relativamente sencillo de calcular manualmente. Luego, se puede verificar el estadístico de prueba con una distribución t para determinar la probabilidad de obtener ese valor del estadístico de prueba si la hipótesis nula es cierta. En R, para calcular la probabilidad asociada a un valor dado de t, utiliza:\n\npt(q, df = your.df, lower.tail = FALSE) * 2\n\nwhere q es tu valor de t, your.df es el número de grados de libertad (el número de pares - 1). El argumento lower.tail = FALSE asegura que estás calculando la probabilidad de obtener un valor de t mayor que el tuyo (es decir, la cola superior, P[X &gt; x]). Ten en cuenta que el valor crítico para t (\\(\\alpha = 0.05\\)) varía dependiendo del número de grados de libertad: mayores grados de libertad = menor valor crítico de t.\nLa función t.test te proporciona el estadístico de prueba y su probabilidad asociada en una única salida. Para un t-test emparejado, usaríamos:\n\nt.test(x = my_sample1, y = my_sample2, paired = TRUE)\n\nwhere my_sample1 y my_sample2 son vectores que contienen las mediciones de cada muestra. Debes asegurarte de que los dos vectores tengan el mismo número de valores y de que los datos de cada par estén en las filas correspondientes.\nAlternativamente, si tienes un marco de datos con las variables de respuesta y predictor en columnas separadas, puedes utilizar una fórmula y ~ x en lugar del código anterior. Nuevamente, debes asegurarte de que los pares coincidentes estén en el orden correcto (por ejemplo, la cuarta fila del tratamiento sombreado corresponde a los datos recopilados de la misma planta que la cuarta fila del tratamiento soleado).\nDescarga un conjunto de datos de ejemplo en este formato, Greenhouse.csv, e impórtalo en R.\n\nGreenhouse &lt;- read.csv(file = \"Greenhouse.csv\", header = TRUE)\n\nEl t-test emparejado se realiza con la función t.test, con los argumentos que especifican la variable de respuesta (Performance) a la izquierda de ~, la variable predictora (Treatment) a la derecha de ~, el marco de datos a utilizar y el hecho de que es un t-test emparejado.\n\nt.test(Performance ~ Treatment, data = Greenhouse, paired = TRUE)\n\n\n\nInterpretación de los resultados\n\n\n\n    Paired t-test\n\ndata:  Performance by Treatment\nt = -18.812, df = 9, p-value = 1.557e-08\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2111674 -0.1658326\nsample estimates:\nmean difference \n        -0.1885 \n\n\nEl resultado importante de una prueba t pareada incluye la estadística de prueba t, en este caso 18.8, los grados de libertad (en este caso 9) y la probabilidad asociada con ese valor de t. En este caso, tenemos un valor de p muy bajo (p &lt; 0.001) y podemos rechazar la hipótesis nula de que las plantas puedan realizar la fotosíntesis con el mismo rendimiento en los dos entornos de luz.\nTambién se obtienen la media y los intervalos de confianza del 95% para las diferencias entre las mediciones de cada par (estos no se superponen con cero cuando la prueba es significativa).\n\n\nSupuestos a verificar\nLas pruebas t son pruebas paramétricas, lo que implica que podemos especificar una distribución de probabilidad para la población de la variable de la cual se tomaron las muestras. Las pruebas paramétricas (y no paramétricas) tienen varios supuestos. Si se violan estos supuestos, ya no podemos estar seguros de que la estadística de prueba siga una distribución t, en cuyo caso los valores de p pueden ser inexactos.\nNormalidad. Para una prueba t pareada, se asume que la muestra de diferencias sigue una distribución normal. Si estas diferencias tienen una distribución muy sesgada, se pueden utilizar transformaciones para obtener una distribución más cercana a la normal.\nIndependencia. El diseño pareado tiene en cuenta que las dos medidas de cada par no son independientes. Sin embargo, es importante que cada par de objetos medidos sea independiente de otros pares. Si están relacionados de alguna manera (por ejemplo, grupos de plantas que comparten una bandeja de agua), puede ser necesario un diseño analítico más complejo que tenga en cuenta factores adicionales.\n\n\nComunicar los resultados\nEscrito. Como mínimo, se deben informar la estadística t observada, el valor de p y el número de grados de libertad. Por ejemplo, se podría escribir “El rendimiento fotosintético de las plantas fue significativamente mayor en entornos soleados en contraste con los entornos sombreados (prueba t pareada: t = 18.81, df = 9, P &lt; 0.001)”.\nVisual. Los diagramas de caja o gráficos de columnas con barras de error son formas efectivas de comunicar la variación en una variable de respuesta continua en relación con un único predictor categórico.\n\nboxplot(Performance ~ Treatment, data = Greenhouse, xlab = \"Light environment\", ylab = \"Photosynthetic performance (FvFm)\")\n\n\n\n\n\n\nMás ayuda\nEscribe ?t.test para obtener la ayuda de R sobre esta función.\n\nQuinn y Keough (2002) Experimental design and data analysis for biologists. Cambridge University Press. Capítulo 9: Prueba de hipótesis.\n\n\nMcKillup (2012) Statistics explained. An introductory guide for life scientists. Cambridge University Press. Capítulo 9: Comparación de las medias de una y dos muestras de datos distribuidos normalmente.\n\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/t-tests/two-sample-t-test/index.html",
    "href": "statistics/t-tests/two-sample-t-test/index.html",
    "title": "Two Sample T-test",
    "section": "",
    "text": "t-test de muestras independientes, también conocido como un t-test de dos muestras, es una de las pruebas estadísticas más comúnmente utilizadas. Se utiliza para comparar si las medias de dos muestras son estadísticamente diferentes entre sí (por ejemplo, control vs. tratamiento, sitio A vs. sitio B, etc.). Por ejemplo, consideremos el caso simple de si una muestra de mediciones de pH de un río difiere de una muestra de mediciones de pH de un segundo río.\n\nLa hipótesis nula es que las medias de la población de las cuales se toman las dos muestras son iguales \\[H_o: \\mu_1=\\mu_2\\].\nLa estadística de prueba, t, es:\n\\[t = \\frac{\\bar{x_{1}}-\\bar{x_{2}}}{s_{\\bar{y_{1}}-\\bar{y_{2}}}}\\]\ndonde el denominador es el error estándar de la diferencia entre las dos medias.\n\\[\\sqrt{\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}{(\\frac{1}{n_{1}}}+\\frac{1}{n_{2}})}\\]\nTen en cuenta que el tamaño de la estadística de prueba depende de dos cosas: 1) qué tan diferentes son las dos medias (el numerador) y 2) cuánta variación hay presente dentro de cada muestra (el denominador).\nEsta ecuación es para el t-test de varianza combinada. Para un t-test de varianzas separadas (también conocido como t-test de Welch), que no asume varianzas iguales, el denominador es:\n\\[\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\]\nTen en cuenta que un t-test es un caso especial de un modelo lineal con una variable de respuesta continua única y un predictor categórico único que tiene dos niveles.\n\nEjecución del análisis\nUn t-test de muestras independientes se puede ejecutar con la misma función t.test utilizada para una muestra o t*-tests pareados. Para un t-test de muestras independientes que asume varianzas iguales, usaríamos:\n\nt.test(x &lt;- my_sample1, y = my_sample2, var.equal = TRUE)\n\nDonde my_sample1 y my_sample2 son vectores que contienen las mediciones de cada muestra.\nMás comúnmente, usaríamos un data frame con las variables de respuesta y predictor como columnas separadas. Luego puedes usar una declaración de fórmula, y ~ x, para especificar las variables de respuesta y predictor en lugar del código anterior. Considera el ejemplo simple en el que deseas comparar el pH de dos ríos. Se tomaron diez medidas replicadas de pH de cada río.\nDescarga el conjunto de datos de muestra, River_pH.csv, e impórtalo en R.\n\nRiver_pH &lt;- read.csv(file = \"River_pH.csv\", header = TRUE)\n\nEl test t se realiza con la función t.test, donde los argumentos especifican la variable de respuesta (pH) a la izquierda de ~, la variable predictora (River_name) a la derecha de ~, y el marco de datos a utilizar.\n\nt.test(pH ~ River_name, data = River_pH, var.equal = TRUE)\n\nEl argumento var.equal = TRUE especifica que asumimos varianzas iguales. Ten en cuenta que el argumento predeterminado del test t para la hipótesis alternativa es una prueba bilateral. Si deseas realizar una prueba unilateral, debes agregar un argumento a la función que especifique alternative = \"greater\" o alternative = \"less\".\n\n\nInterpretación de los resultados\n\n\n\n    Two Sample t-test\n\ndata:  River_pH$pH by River_pH$River_name\nt = 6.9788, df = 18, p-value = 1.618e-06\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n 1.574706 2.931168\nsample estimates:\nmean in group A mean in group B \n       8.661497        6.408560 \n\n\nLa salida de un test t es fácil de interpretar. En la salida anterior, el estadístico de prueba t = 6.9788 con 18 grados de libertad, y un valor de p muy bajo (p &lt; 0.001). Por lo tanto, podemos rechazar la hipótesis nula de que los dos ríos tienen el mismo pH.\nTambién se obtienen las medias de las dos muestras (necesario para saber cuál es mayor si el test es significativo) y el intervalo de confianza del 95% para la diferencia entre las dos medias (este no se superpondrá con cero cuando el test sea significativo).\n\n\nSupuestos a verificar\nLos tests t son tests paramétricos, lo que implica que podemos especificar una distribución de probabilidad para la población de la variable de la cual se tomaron las muestras. Los tests paramétricos (y no paramétricos) tienen varios supuestos. Si se violan estos supuestos, ya no podemos estar seguros de que el estadístico de prueba siga una distribución t, y en ese caso, los valores de p pueden ser inexactos.\nNormalidad. Los datos siguen una distribución normal. Sin embargo, los tests t son razonablemente robustos frente a violaciones de la normalidad (aunque hay que tener cuidado con la influencia de los valores atípicos).\nVarianza igual. Se asume que las varianzas de cada muestra son aproximadamente iguales. Los tests t también son razonablemente robustos frente a violaciones de la igualdad de varianzas si los tamaños de las muestras son iguales, pero pueden ser problemáticos cuando los tamaños de las muestras son muy diferentes.\nEn caso de varianzas desiguales, puede ser mejor realizar un test t de Welch, que no asume igualdad de varianzas. Para realizar un test de Welch en R, el argumento var.equal en la función t.test debe cambiarse a var.equal=FALSE. De hecho, este es el argumento predeterminado para t.test si no se especifica.\nIndependencia. Las observaciones deben haber sido muestreadas aleatoriamente de la población, de manera que las medias de las dos muestras sean estimaciones no sesgadas de las medias de la población. Si los individuos replicados están vinculados de alguna manera, se violará el supuesto de independencia.\n\n\nComunicación de los resultados\nEscrita. Como mínimo, se debe informar el estadístico de prueba t observado, el valor de p y el número de grados de libertad. Por ejemplo, podrías escribir “el pH fue significativamente más alto en el Río A que en el Río B (test de muestras independientes t: t = 6.98, df = 18, P &lt; 0.001)”.\nVisual. Los diagramas de caja o los gráficos de columnas con barras de error son formas efectivas de comunicar la variación en una única variable de respuesta continua en comparación con una única variable predictora categórica.\n\nboxplot(pH ~ River_name, data = River_pH, xlab = \"River name\", ylab = \"pH\")\n\n\n\n\n\n\nMás ayuda\nEscribe ?t.test para obtener la ayuda de R sobre esta función.\n\nQuinn y Keough (2002) Experimental design and data analysis for biologists. Cambridge University Press. Capítulo 3: Prueba de hipótesis.\n\n\nMcKillup (2012) Statistics explained. An introductory guide for life scientists. Cambridge University Press. Capítulo 9: Comparación de las medias de una y dos muestras de datos distribuidos normalmente.\n\nAutor: Alistair Poore\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "statistics/time-series/index.html",
    "href": "statistics/time-series/index.html",
    "title": "Predicción de series temporales",
    "section": "",
    "text": "En las ciencias ambientales, no siempre queremos entender los procesos actuales y pasados, a veces queremos mirar hacia el futuro. Por ejemplo, podemos querer proyectar cuánto tiempo tomará que una especie en peligro de extinción se extinga si las amenazas continúan. También podemos querer proyectar cómo responderán las poblaciones o distribuciones al cambio climático antropogénico en curso. Predecir el futuro, o pronosticar, ha sido el foco de una gran cantidad de investigaciones estadísticas en el campo de la economía y podemos aplicar las mismas técnicas a preguntas ecológicas. Aquí nos enfocaremos en la predicción de series temporales, donde utilizaremos datos históricos recolectados a lo largo del tiempo para predecir las condiciones en el futuro. Utilizaremos el paquete R forecast de Rob Hyndman.\n\nlibrary(forecast)\n\n\nModelos de suavización exponencial simple\n\nNuestro primer ejemplo utilizará la pérdida anual de bosque tropical (ha) en la región de Tocantins en la Amazonia brasileña, recolectada mediante imágenes de satélite durante 25 años (datos de Global Forest Watch y analizados en más detalle en Virah-Sawmy et al. 2015).\nIntroduciremos los datos manualmente.\n\nArea &lt;- c(\n  1650, 730, 580, 440, 409, 333, 333, 797, 320, 273, 576, 216, 244, 189, 212,\n  156, 158, 271, 124, 63, 107, 61, 49, 40, 52\n)\nYear &lt;- c(1988:2012)\n\nPrimero, utilizamos la función ts para crear un objeto de series temporales en R, especificando los datos, el tiempo de inicio y finalización, y la frecuencia (en este caso, un año).\n\nArea_loss &lt;- ts(Area, start = 1988, end = 2012, frequency = 1)\nArea_loss\n\nTime Series:\nStart = 1988 \nEnd = 2012 \nFrequency = 1 \n [1] 1650  730  580  440  409  333  333  797  320  273  576  216  244  189  212\n[16]  156  158  271  124   63  107   61   49   40   52\n\n\nLuego, podemos dividir los datos en datos de entrenamiento y datos de prueba utilizando la función window. En este ejemplo, trataremos de pronosticar la pérdida de bosque tropical para los últimos tres años de la serie temporal (2010-2012). Estos tres años serán los datos de prueba y los años anteriores serán los datos de entrenamiento. De esta manera, podremos comparar nuestros valores pronosticados con las observaciones reales.\n\nArea_loss_train &lt;- window(Area_loss, start = 1988, end = 2009, frequency = 1)\nArea_loss_test &lt;- window(Area_loss, start = 2010, end = 2012, frequency = 1)\n\nAhora que hemos formateado nuestros datos de manera adecuada, ajustaremos un modelo simple de suavizado exponencial utilizando la función ets. Este modelo utiliza un promedio ponderado de observaciones pasadas, donde los pesos disminuyen exponencialmente hacia el pasado. Esto significa que las observaciones más recientes tienen el peso más alto y, por lo tanto, influyen más en las predicciones.\nEn la función ets, el tipo de modelo se expresa como un código de tres caracteres que representa el tipo de error (primer carácter), el tipo de tendencia (segundo carácter) y el tipo de estacionalidad (tercer carácter). Utilizaremos el tipo de modelo = ANN, que es un modelo simple de suavizado exponencial con errores aditivos (A), sin tendencia asumida (N) y sin estacionalidad (N).\n\nfit1 &lt;- ets(Area_loss_train, model = \"ANN\")\n\nAhora que hemos ajustado el modelo utilizando nuestros datos de entrenamiento (1988-2009), podemos utilizar el modelo para predecir la pérdida de bosque lluvioso en los últimos 3 años (2010-2012), especificando h como el número de puntos de tiempo que queremos pronosticar (en este caso, 3 años).\n\nfit1_forecast &lt;- forecast(fit1, h = 3)\nfit1_forecast\n\n     Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95\n2010       70.79069 -287.8400 429.4214 -477.6876 619.2690\n2011       70.79069 -379.4406 521.0220 -617.7786 759.3600\n2012       70.79069 -455.3275 596.9089 -733.8377 875.4191\n\n\nLa salida de nuestra predicción nos proporciona los valores pronosticados (Forecast) para cada año (Punto) y luego los intervalos de confianza del 80% y del 95%. ¿Cómo nos fue? Al utilizar la función plot en un objeto forecast, podemos ver un gráfico de la tendencia histórica (línea), junto con nuestras observaciones pronosticadas (puntos azules) y nuestros intervalos de confianza (95% - sombreado gris, 80% - sombreado azul).\n\nplot(fit1_forecast)\n\n\n\n\nEn este caso, ya tenemos datos históricos para nuestros puntos pronosticados, así que los representaremos en el gráfico utilizando la función points (cuadrados negros), seleccionando los últimos tres valores en los vectores de Año y Área (23:25).\n\nplot(fit1_forecast)\npoints(Year[23:25], Area[23:25], lty = 1, col = \"black\", lwd = 3, pch = 0)\nlegend(\"topright\",\n  legend = c(\"observed\", \"forecasted\"), lwd = 3,\n  col = c(\"black\", \"blue\"), lty = c(0, 0), merge = TRUE,\n  bty = \"n\", pch = c(0, 19), cex = 1.3\n)\n\n\n\n\nEl valor de alfa (el parámetro de suavizado) se estimó en 0.7587, lo que significa un decaimiento relativamente rápido de los pesos de las observaciones hacia el pasado. En otras palabras, las observaciones pasadas tienen una influencia relativamente pequeña en las predicciones futuras.\nEn nuestro gráfico, observamos una fuerte tendencia descendente, por lo que podríamos agregar una tendencia aditiva en el modelo. Esto se hace simplemente cambiando el segundo carácter en nuestro código de modelo a “A” (aditiva) y repitiendo el proceso.\n\nfit2 &lt;- ets(Area_loss_train, model = \"AAN\")\nfit2_forecast &lt;- forecast(fit2, h = 3)\n\nPodemos comparar el modelo exponencial simple con el modelo que incorpora la tendencia aditiva al representarlos lado a lado.\n\npar(mfrow = c(1, 2))\n\nplot(fit1_forecast, main = \"Simple exponential model\")\npoints(Year[23:25], Area[23:25], lty = 1, col = \"black\", lwd = 4, pch = 0)\n\nplot(fit2_forecast, main = \"Additive trend model\")\npoints(Year[23:25], Area[23:25], lty = 1, col = \"black\", lwd = 4, pch = 0)\n\nlegend(\"topright\",\n  legend = c(\"observed\", \"forecasted\"), lwd = 4,\n  col = c(\"black\", \"blue\"), lty = c(0, 0), merge = TRUE,\n  bty = \"n\", pch = c(0, 19), cex = 1.3\n)\n\n\n\n\n¿Qué modelo crees que funciona mejor? Otra opción es ajustar todos los posibles modelos y seleccionar el mejor modelo utilizando el AIC. Esto se hace simplemente no especificando ninguna estructura de modelo en el comando ets.\n\nfit3 &lt;- ets(Area_loss_train)\nfit3_forecast &lt;- forecast(fit3, h = 3)\nfit3$method\n\n[1] \"ETS(M,N,N)\"\n\n\nSi estás interesado en qué estructura de modelo se seleccionó, puedes extraerla del objeto del modelo fit_3$method. En este caso, la estructura del modelo fue MNN: un modelo con errores multiplicativos (M), pero sin tendencia general (N) ni estacionalidad (N) asumida. Ahora podemos representar los tres modelos para ver cuál nos brinda la mejor aproximación de nuestras mediciones observadas de pérdida de selva tropical.\n\npar(mfrow = c(1, 3))\n\nplot(fit1_forecast, main = \"Simple exponential model\")\nplot(fit2_forecast, main = \"Additive trend model\")\nplot(fit3_forecast, main = \"Best model (lowest AIC)\")\npoints(Year[23:25], Area[23:25], lty = 1, col = \"black\", lwd = 4, pch = 0)\n\nlegend(\"topright\",\n  legend = c(\"observed\", \"forecasted\"), lwd = 4,\n  col = c(\"black\", \"blue\"), lty = c(0, 0), merge = TRUE,\n  bty = \"n\", pch = c(0, 19), cex = 1.3\n)\n\n\n\n\nTambién podemos predecir la pérdida futura de selva tropical (sin datos de prueba). Mantengamos la predicción en tres años pero incluyamos todas nuestras observaciones históricas en los datos de entrenamiento. Nuevamente utilizaremos el método de selección de modelo AIC.\n\nArea_loss_train_2 &lt;- window(Area_loss, start = 1988, end = 2012, frequency = 1)\n\nfit4 &lt;- ets(Area_loss_train_2)\n\nfit4_forecast &lt;- forecast(fit4, h = 3)\n\npar(mfrow = c(1, 1))\n\nplot(fit4_forecast)\n\n\n\n\n¡Malas noticias! El pronóstico es de una disminución del área de selva tropical en la Amazonia.\n\n\nModelos que incorporan variabilidad estacional\nProbemos otro ejemplo que incorpora no solo tendencias a largo plazo sino también variabilidad estacional. Utilizaremos un ejemplo utilizado en el libro de texto en línea de Rob Hyndman y George Athanasopoulos online textbook e incluido en el paquete forecast: la producción mensual de gas en Australia. Comenzaremos representando los datos históricos desde 1956 hasta 1995. Si representamos los datos, podemos ver dos patrones: una tendencia general positiva y un patrón estacional zigzagueante.\n\ndata(gas)\n\nplot(gas)\n\n\n\n\nSi utilizamos nuestro primer modelo, el modelo exponencial simple que no asume una tendencia ni estacionalidad (ANN), obtenemos lo siguiente al predecir los próximos 3 años. Ten en cuenta que h ahora son 36 puntos de tiempo (= 3 años x 12 meses).\n\nfit_gas1 &lt;- ets(gas, model = \"ANN\")\n\nfit_gas1_forecast &lt;- forecast(fit_gas1, h = 36)\n\nplot(fit_gas1_forecast, main = \"Simple exponential model\")\n\n\n\n\nComo era de esperar, podemos ver que el modelo no logra predecir correctamente la tendencia general ni la variabilidad estacional. De hecho, el modelo predice una producción constante de gas durante los 3 años (o 36 pasos de tiempo mensuales).\nAhora ajustemos el modelo asumiendo una variabilidad estacional aditiva (ANA).\n\nfit_gas2 &lt;- ets(gas, model = \"ANA\")\n\nfit_gas2_forecast &lt;- forecast(fit_gas2, h = 36)\n\npar(mfrow = c(1, 2))\n\nplot(fit_gas1_forecast, main = \"Simple exponential model\")\nplot(fit_gas2_forecast, main = \"Seasonal trend model\")\n\n\n\n\nEste modelo parece mucho más convincente que el ajuste exponencial simple. Por último, permitamos que la función ets elija el mejor modelo utilizando el criterio de información de Akaike (AIC).\n\nfit_gas3 &lt;- ets(gas)\n\nfit_gas3_forecast &lt;- forecast(fit_gas3, h = 36)\n\npar(mfrow = c(1, 3))\nplot(fit_gas1_forecast, main = \"Simple exponential model\")\nplot(fit_gas2_forecast, main = \"Seasonal trend model\")\nplot(fit_gas3_forecast, main = \"Best model (lowest AIC)\")\n\n\n\n\n\n\nMás ayuda\nEscribe ?forecast para obtener ayuda en R sobre el paquete forecast.\nPara obtener más detalles sobre el paquete y pronóstico de series de tiempo en general, consulta el libro en línea de Rob Hyndman y George Athanasopoulos. Este script se basa en la publicación del blog de investigación Ecostats y en el taller del grupo de usuarios de R BEES de Jakub Stoklosa.\nAutor: Jakub Stoklosa y Rachel V. Blakey\nAño: 2016\nÚltima actualización: Nov. 2023"
  },
  {
    "objectID": "graphics/index.html#graficos-basicos-en-r",
    "href": "graphics/index.html#graficos-basicos-en-r",
    "title": "Gráficos",
    "section": "Graficos basicos en R",
    "text": "Graficos basicos en R\n\n\n\n\n\nR cuenta con una amplia gama de funciones y paquetes para visualizar datos. Aquí tienes ayuda para realizar algunos gráficos muy simples utilizando las funciones básicas en R para datos con:\n\nuna variable continua - histogramas y diagramas de caja\ndos variables continuas - gráficos de dispersión\nuna variable continua vs variables categóricas - diagramas de caja y gráficos de barras"
  },
  {
    "objectID": "graphics/index.html#graficos-con-ggplot",
    "href": "graphics/index.html#graficos-con-ggplot",
    "title": "Gráficos",
    "section": "Graficos con ggplot",
    "text": "Graficos con ggplot\nggplot2 es un paquete poderoso para la producción de gráficos en R que se puede utilizar para crear gráficos de aspecto profesional para informes, ensayos o trabajos académicos. Puede crear una variedad de gráficos, incluyendo diagramas de caja, gráficos de dispersión e histogramas, y se pueden personalizar ampliamente para adaptarse a tus datos.\nEl paquete lleva el nombre de un libro llamado The Grammar of Graphics que introduce un enfoque sistemático y estructurado para crear gráficos estadísticos.\nComienza con los Conceptos básicos para aprender la sintaxis básica de cómo crear un gráfico.\nLuego, visita nuestras otras páginas para personalizar aún más la estética del gráfico, incluyendo el color y el formato:\n\nFundamentos de ggplot2\nPersonalización de un gráfico ggplot\nTítulos y etiquetas\nColores y formas\nGráfico de barras con error intervalos de error"
  },
  {
    "objectID": "graphics/index.html#visualización-de-datos-multivariables",
    "href": "graphics/index.html#visualización-de-datos-multivariables",
    "title": "Gráficos",
    "section": "Visualización de datos multivariables",
    "text": "Visualización de datos multivariables\n\n\n\n\n\nR cuenta con una amplia gama de funciones y paquetes para visualizar datos multivariados. Aquí tienes ayuda para algunas de las técnicas más comúnmente utilizadas:\n\nEscalamiento multidimensional\n\nAnálisis de componentes principales\n\nAnálisis de agrupamiento - Cluster"
  },
  {
    "objectID": "graphics/index.html#visualización-de-datos-espaciales-incluyendo-mapas",
    "href": "graphics/index.html#visualización-de-datos-espaciales-incluyendo-mapas",
    "title": "Gráficos",
    "section": "Visualización de datos espaciales (incluyendo mapas)",
    "text": "Visualización de datos espaciales (incluyendo mapas)\nR cuenta con una amplia variedad de funciones y paquetes para visualizar datos espaciales. Esta página enlazará con una serie de tutoriales para manejar datos espaciales y crear mapas.\n\nCreación de mapas simples con ggmap\n[Creación de mapas a partir de shapefiles]"
  },
  {
    "objectID": "graphics/index.html#visualización-de-datos-espaciales",
    "href": "graphics/index.html#visualización-de-datos-espaciales",
    "title": "Gráficos",
    "section": "Visualización de datos espaciales",
    "text": "Visualización de datos espaciales\nR cuenta con una amplia variedad de funciones y paquetes para visualizar datos espaciales. Esta página enlazará con una serie de tutoriales para manejar datos espaciales y crear mapas.\n\nCreación de mapas simples con ggmap\nFundamentos de imagenes raster\nMapas interactivos con R"
  },
  {
    "objectID": "graphics/index.html#datos-multivaridos",
    "href": "graphics/index.html#datos-multivaridos",
    "title": "Gráficos",
    "section": "Datos multivaridos",
    "text": "Datos multivaridos\n\n\n\n\n\nR cuenta con una amplia gama de funciones y paquetes para visualizar datos multivariados. Aquí tienes ayuda para algunas de las técnicas más comúnmente utilizadas:\n\nEscalamiento multidimensional\n\nAnálisis de componentes principales\n\nAnálisis de agrupamiento - Cluster\nHeatmaps - Mapa de Calor"
  },
  {
    "objectID": "statistics/index.html#pruebas-de-hipótesis-simples-con-la-estadística-t",
    "href": "statistics/index.html#pruebas-de-hipótesis-simples-con-la-estadística-t",
    "title": "Statistics",
    "section": "Pruebas de hipótesis simples con la estadística t:",
    "text": "Pruebas de hipótesis simples con la estadística t:\n\nPruebas t de una muestra - contrastando un parámetro de muestra con un parámetro de población.\nPruebas t de muestras independientes - contrastando las medias de dos muestras.\nPruebas t pareadas - contrastando dos grupos cuando los datos están apareados."
  },
  {
    "objectID": "statistics/index.html#modelos-lineales---lm",
    "href": "statistics/index.html#modelos-lineales---lm",
    "title": "Statistics",
    "section": "Modelos lineales - LM:",
    "text": "Modelos lineales - LM:\nEstas páginas contienen algunas introducciones respecto de los modelos lineales comúnmente utilizados que prueban la respuesta de una variable dependiente continua frente a una o más variables predictoras que pueden ser continuas o categóricas. Ten en cuenta que estos se denominan diferentes técnicas (por ejemplo, regresión vs. ANOVA) debido al uso común en la literatura que encontrarás; todos ellos involucran el mismo marco de modelado lineal.\n\n\n\n\n\n\nRegresión lineal\n\nInterpretación de Regresiones Lineales\n\n\n\nEl análisis de varianza (ANOVA)\nEs una de las técnicas más utilizadas en las ciencias biológicas y ambientales. El ANOVA se utiliza para contrastar una variable dependiente continua y en diferentes niveles de una o más variables independientes categóricas x. Las variables independientes se denominan factor o tratamiento, y las diferentes categorías dentro de ese tratamiento se denominan niveles. En este módulo, comenzaremos con el diseño más simple, aquellos con un solo factor.\nDonde se utilizaría una prueba t-test de muestras independientes para comparar las medias de grupos en dos niveles, el ANOVA se utiliza para la comparación de medias de grupos &gt;2, o cuando hay dos o más variables predictoras (ver ANOVA: factorial). La lógica de esta prueba es esencialmente la misma que la prueba t-test: compara la variación entre grupos con la variación dentro de los grupos para determinar si las diferencias observadas se deben al azar o no.\n\nAnálisis de varianza: de un solo factor\nAnálisis de varianza: factorial\n\nANOVA anidado\nComprensión de las interacciones"
  },
  {
    "objectID": "statistics/index.html#modelos-lineales-generalizados---glm",
    "href": "statistics/index.html#modelos-lineales-generalizados---glm",
    "title": "Statistics",
    "section": "Modelos lineales generalizados - GLM:",
    "text": "Modelos lineales generalizados - GLM:\nLos modelos lineales generalizados (GLM) se utilizan cuando la distribución de los datos no se ajusta a las suposiciones de los modelos lineales, específicamente las suposiciones de residuos distribuidos normalmente y ausencia de relación entre la varianza y la media (por ejemplo, datos de presencia/ausencia, recuento o altamente sesgados).\n\nModelos lineales generalizados 1: Introducción y datos binomiales\nModelos lineales generalizados 2: Datos de recuento\nInterpretación de coeficientes en glms"
  },
  {
    "objectID": "statistics/index.html#modelos-mixtos",
    "href": "statistics/index.html#modelos-mixtos",
    "title": "Statistics",
    "section": "Modelos mixtos:",
    "text": "Modelos mixtos:\nLos modelos mixtos son aquellos que tienen una mezcla de efectos fijos y aleatorios. Los efectos aleatorios son factores categóricos cuyos niveles han sido seleccionados entre muchos posibles niveles, y el investigador desea hacer inferencias más allá de los niveles elegidos. Es un concepto complicado, pero imagina que contrastas dos tipos de hábitat (bosque y pradera) muestreando cinco sitios dentro de cada uno, y cinco medidas replicadas dentro de cada sitio. El tipo de hábitat es un factor fijo en el que el investigador solo está interesado en esos dos niveles de tipo de hábitat. Si los cinco sitios se seleccionaron de una colección más grande de sitios posibles, entonces el sitio se considera un efecto aleatorio con 10 niveles.\n\nModelos mixtos 1: Modelos mixtos lineales con un efecto aleatorio.\nModelos mixtos 2: Modelos mixtos lineales con varios efectos aleatorios.\nModelos mixtos 3: Modelos mixtos lineales generalizados."
  },
  {
    "objectID": "statistics/index.html#pruebas-de-hipótesis-simples",
    "href": "statistics/index.html#pruebas-de-hipótesis-simples",
    "title": "Statistics",
    "section": "Pruebas de hipótesis simples:",
    "text": "Pruebas de hipótesis simples:\n\nPruebas t de una muestra - contrastando un parámetro de muestra con un parámetro de población.\nPruebas t de muestras independientes - contrastando las medias de dos muestras.\nPruebas t pareadas - contrastando dos grupos cuando los datos están apareados."
  },
  {
    "objectID": "statistics/index.html#análisis-de-datos-categóricos",
    "href": "statistics/index.html#análisis-de-datos-categóricos",
    "title": "Statistics",
    "section": "Análisis de datos categóricos:",
    "text": "Análisis de datos categóricos:\nAlgunas pruebas comúnmente utilizadas para contrastar las frecuencias de observaciones entre variables categóricas.\n\nPruebas de bondad de ajuste.\nTablas de contingencia.\nPrueba Exacta de Fisher."
  },
  {
    "objectID": "statistics/index.html#meta-análisis",
    "href": "statistics/index.html#meta-análisis",
    "title": "Statistics",
    "section": "Meta-análisis:",
    "text": "Meta-análisis:\nEL meta-análisis se utilizan cada vez más en ecología, evolución y ciencias ambientales para encontrar patrones generales entre muchos estudios, resolver controversias entre estudios conflictivos y generar nuevas hipótesis. Estos tutoriales ofrecen una introducción para realizar meta-análisis con el paquete R metafor.\n\nMeta-análisis 1: Introducción y cálculo de tamaños de efecto.\nMeta-análisis 2: Modelos de efecto fijo y efecto aleatorio.\nMeta-análisis 3: Modelos más complejos.\n\n\n\nOtras pruebas estadisticas:\n\nModelos aditivos generalizados\nAnálisis de potencia: cálculo de potencia y determinación del tamaño de muestra.\nIntroducción a mvabund: análisis basado en modelos de datos de abundancia multivariada.\nSeries temporales"
  }
]